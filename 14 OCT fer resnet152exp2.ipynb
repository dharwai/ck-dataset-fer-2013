{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efdc2d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-07 16:43:02.484427: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-10-07 16:43:02.837840: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-07 16:43:03.624699: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28709 images belonging to 7 classes.\n",
      "Found 7178 images belonging to 7 classes.\n",
      "Train and Validation sets have been created.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-07 16:43:05.268327: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-10-07 16:43:05.473222: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-10-07 16:43:05.473903: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-10-07 16:43:05.475178: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-10-07 16:43:05.475283: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-10-07 16:43:05.475359: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-10-07 16:43:06.141999: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-10-07 16:43:06.142149: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-10-07 16:43:06.142242: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-10-07 16:43:06.142328: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5831 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3070 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"resnet152\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 48, 48, 3)]          0         []                            \n",
      "                                                                                                  \n",
      " conv1_pad (ZeroPadding2D)   (None, 54, 54, 3)            0         ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " conv1_conv (Conv2D)         (None, 24, 24, 64)           9472      ['conv1_pad[0][0]']           \n",
      "                                                                                                  \n",
      " conv1_bn (BatchNormalizati  (None, 24, 24, 64)           256       ['conv1_conv[0][0]']          \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv1_relu (Activation)     (None, 24, 24, 64)           0         ['conv1_bn[0][0]']            \n",
      "                                                                                                  \n",
      " pool1_pad (ZeroPadding2D)   (None, 26, 26, 64)           0         ['conv1_relu[0][0]']          \n",
      "                                                                                                  \n",
      " pool1_pool (MaxPooling2D)   (None, 12, 12, 64)           0         ['pool1_pad[0][0]']           \n",
      "                                                                                                  \n",
      " conv2_block1_1_conv (Conv2  (None, 12, 12, 64)           4160      ['pool1_pool[0][0]']          \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block1_1_bn (BatchNo  (None, 12, 12, 64)           256       ['conv2_block1_1_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv2_block1_1_relu (Activ  (None, 12, 12, 64)           0         ['conv2_block1_1_bn[0][0]']   \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " conv2_block1_2_conv (Conv2  (None, 12, 12, 64)           36928     ['conv2_block1_1_relu[0][0]'] \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block1_2_bn (BatchNo  (None, 12, 12, 64)           256       ['conv2_block1_2_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv2_block1_2_relu (Activ  (None, 12, 12, 64)           0         ['conv2_block1_2_bn[0][0]']   \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " conv2_block1_0_conv (Conv2  (None, 12, 12, 256)          16640     ['pool1_pool[0][0]']          \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block1_3_conv (Conv2  (None, 12, 12, 256)          16640     ['conv2_block1_2_relu[0][0]'] \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block1_0_bn (BatchNo  (None, 12, 12, 256)          1024      ['conv2_block1_0_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv2_block1_3_bn (BatchNo  (None, 12, 12, 256)          1024      ['conv2_block1_3_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv2_block1_add (Add)      (None, 12, 12, 256)          0         ['conv2_block1_0_bn[0][0]',   \n",
      "                                                                     'conv2_block1_3_bn[0][0]']   \n",
      "                                                                                                  \n",
      " conv2_block1_out (Activati  (None, 12, 12, 256)          0         ['conv2_block1_add[0][0]']    \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv2_block2_1_conv (Conv2  (None, 12, 12, 64)           16448     ['conv2_block1_out[0][0]']    \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block2_1_bn (BatchNo  (None, 12, 12, 64)           256       ['conv2_block2_1_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv2_block2_1_relu (Activ  (None, 12, 12, 64)           0         ['conv2_block2_1_bn[0][0]']   \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " conv2_block2_2_conv (Conv2  (None, 12, 12, 64)           36928     ['conv2_block2_1_relu[0][0]'] \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block2_2_bn (BatchNo  (None, 12, 12, 64)           256       ['conv2_block2_2_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv2_block2_2_relu (Activ  (None, 12, 12, 64)           0         ['conv2_block2_2_bn[0][0]']   \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " conv2_block2_3_conv (Conv2  (None, 12, 12, 256)          16640     ['conv2_block2_2_relu[0][0]'] \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block2_3_bn (BatchNo  (None, 12, 12, 256)          1024      ['conv2_block2_3_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv2_block2_add (Add)      (None, 12, 12, 256)          0         ['conv2_block1_out[0][0]',    \n",
      "                                                                     'conv2_block2_3_bn[0][0]']   \n",
      "                                                                                                  \n",
      " conv2_block2_out (Activati  (None, 12, 12, 256)          0         ['conv2_block2_add[0][0]']    \n",
      " on)                                                                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " conv2_block3_1_conv (Conv2  (None, 12, 12, 64)           16448     ['conv2_block2_out[0][0]']    \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block3_1_bn (BatchNo  (None, 12, 12, 64)           256       ['conv2_block3_1_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv2_block3_1_relu (Activ  (None, 12, 12, 64)           0         ['conv2_block3_1_bn[0][0]']   \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " conv2_block3_2_conv (Conv2  (None, 12, 12, 64)           36928     ['conv2_block3_1_relu[0][0]'] \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block3_2_bn (BatchNo  (None, 12, 12, 64)           256       ['conv2_block3_2_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv2_block3_2_relu (Activ  (None, 12, 12, 64)           0         ['conv2_block3_2_bn[0][0]']   \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " conv2_block3_3_conv (Conv2  (None, 12, 12, 256)          16640     ['conv2_block3_2_relu[0][0]'] \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block3_3_bn (BatchNo  (None, 12, 12, 256)          1024      ['conv2_block3_3_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv2_block3_add (Add)      (None, 12, 12, 256)          0         ['conv2_block2_out[0][0]',    \n",
      "                                                                     'conv2_block3_3_bn[0][0]']   \n",
      "                                                                                                  \n",
      " conv2_block3_out (Activati  (None, 12, 12, 256)          0         ['conv2_block3_add[0][0]']    \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv3_block1_1_conv (Conv2  (None, 6, 6, 128)            32896     ['conv2_block3_out[0][0]']    \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block1_1_bn (BatchNo  (None, 6, 6, 128)            512       ['conv3_block1_1_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv3_block1_1_relu (Activ  (None, 6, 6, 128)            0         ['conv3_block1_1_bn[0][0]']   \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " conv3_block1_2_conv (Conv2  (None, 6, 6, 128)            147584    ['conv3_block1_1_relu[0][0]'] \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block1_2_bn (BatchNo  (None, 6, 6, 128)            512       ['conv3_block1_2_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv3_block1_2_relu (Activ  (None, 6, 6, 128)            0         ['conv3_block1_2_bn[0][0]']   \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " conv3_block1_0_conv (Conv2  (None, 6, 6, 512)            131584    ['conv2_block3_out[0][0]']    \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block1_3_conv (Conv2  (None, 6, 6, 512)            66048     ['conv3_block1_2_relu[0][0]'] \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block1_0_bn (BatchNo  (None, 6, 6, 512)            2048      ['conv3_block1_0_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv3_block1_3_bn (BatchNo  (None, 6, 6, 512)            2048      ['conv3_block1_3_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv3_block1_add (Add)      (None, 6, 6, 512)            0         ['conv3_block1_0_bn[0][0]',   \n",
      "                                                                     'conv3_block1_3_bn[0][0]']   \n",
      "                                                                                                  \n",
      " conv3_block1_out (Activati  (None, 6, 6, 512)            0         ['conv3_block1_add[0][0]']    \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv3_block2_1_conv (Conv2  (None, 6, 6, 128)            65664     ['conv3_block1_out[0][0]']    \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block2_1_bn (BatchNo  (None, 6, 6, 128)            512       ['conv3_block2_1_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv3_block2_1_relu (Activ  (None, 6, 6, 128)            0         ['conv3_block2_1_bn[0][0]']   \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " conv3_block2_2_conv (Conv2  (None, 6, 6, 128)            147584    ['conv3_block2_1_relu[0][0]'] \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block2_2_bn (BatchNo  (None, 6, 6, 128)            512       ['conv3_block2_2_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv3_block2_2_relu (Activ  (None, 6, 6, 128)            0         ['conv3_block2_2_bn[0][0]']   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " conv3_block2_3_conv (Conv2  (None, 6, 6, 512)            66048     ['conv3_block2_2_relu[0][0]'] \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block2_3_bn (BatchNo  (None, 6, 6, 512)            2048      ['conv3_block2_3_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv3_block2_add (Add)      (None, 6, 6, 512)            0         ['conv3_block1_out[0][0]',    \n",
      "                                                                     'conv3_block2_3_bn[0][0]']   \n",
      "                                                                                                  \n",
      " conv3_block2_out (Activati  (None, 6, 6, 512)            0         ['conv3_block2_add[0][0]']    \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv3_block3_1_conv (Conv2  (None, 6, 6, 128)            65664     ['conv3_block2_out[0][0]']    \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block3_1_bn (BatchNo  (None, 6, 6, 128)            512       ['conv3_block3_1_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv3_block3_1_relu (Activ  (None, 6, 6, 128)            0         ['conv3_block3_1_bn[0][0]']   \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " conv3_block3_2_conv (Conv2  (None, 6, 6, 128)            147584    ['conv3_block3_1_relu[0][0]'] \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block3_2_bn (BatchNo  (None, 6, 6, 128)            512       ['conv3_block3_2_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv3_block3_2_relu (Activ  (None, 6, 6, 128)            0         ['conv3_block3_2_bn[0][0]']   \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " conv3_block3_3_conv (Conv2  (None, 6, 6, 512)            66048     ['conv3_block3_2_relu[0][0]'] \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block3_3_bn (BatchNo  (None, 6, 6, 512)            2048      ['conv3_block3_3_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv3_block3_add (Add)      (None, 6, 6, 512)            0         ['conv3_block2_out[0][0]',    \n",
      "                                                                     'conv3_block3_3_bn[0][0]']   \n",
      "                                                                                                  \n",
      " conv3_block3_out (Activati  (None, 6, 6, 512)            0         ['conv3_block3_add[0][0]']    \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv3_block4_1_conv (Conv2  (None, 6, 6, 128)            65664     ['conv3_block3_out[0][0]']    \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block4_1_bn (BatchNo  (None, 6, 6, 128)            512       ['conv3_block4_1_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv3_block4_1_relu (Activ  (None, 6, 6, 128)            0         ['conv3_block4_1_bn[0][0]']   \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " conv3_block4_2_conv (Conv2  (None, 6, 6, 128)            147584    ['conv3_block4_1_relu[0][0]'] \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block4_2_bn (BatchNo  (None, 6, 6, 128)            512       ['conv3_block4_2_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv3_block4_2_relu (Activ  (None, 6, 6, 128)            0         ['conv3_block4_2_bn[0][0]']   \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " conv3_block4_3_conv (Conv2  (None, 6, 6, 512)            66048     ['conv3_block4_2_relu[0][0]'] \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block4_3_bn (BatchNo  (None, 6, 6, 512)            2048      ['conv3_block4_3_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv3_block4_add (Add)      (None, 6, 6, 512)            0         ['conv3_block3_out[0][0]',    \n",
      "                                                                     'conv3_block4_3_bn[0][0]']   \n",
      "                                                                                                  \n",
      " conv3_block4_out (Activati  (None, 6, 6, 512)            0         ['conv3_block4_add[0][0]']    \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv3_block5_1_conv (Conv2  (None, 6, 6, 128)            65664     ['conv3_block4_out[0][0]']    \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block5_1_bn (BatchNo  (None, 6, 6, 128)            512       ['conv3_block5_1_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv3_block5_1_relu (Activ  (None, 6, 6, 128)            0         ['conv3_block5_1_bn[0][0]']   \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " conv3_block5_2_conv (Conv2  (None, 6, 6, 128)            147584    ['conv3_block5_1_relu[0][0]'] \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block5_2_bn (BatchNo  (None, 6, 6, 128)            512       ['conv3_block5_2_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv3_block5_2_relu (Activ  (None, 6, 6, 128)            0         ['conv3_block5_2_bn[0][0]']   \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " conv3_block5_3_conv (Conv2  (None, 6, 6, 512)            66048     ['conv3_block5_2_relu[0][0]'] \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block5_3_bn (BatchNo  (None, 6, 6, 512)            2048      ['conv3_block5_3_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv3_block5_add (Add)      (None, 6, 6, 512)            0         ['conv3_block4_out[0][0]',    \n",
      "                                                                     'conv3_block5_3_bn[0][0]']   \n",
      "                                                                                                  \n",
      " conv3_block5_out (Activati  (None, 6, 6, 512)            0         ['conv3_block5_add[0][0]']    \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv3_block6_1_conv (Conv2  (None, 6, 6, 128)            65664     ['conv3_block5_out[0][0]']    \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block6_1_bn (BatchNo  (None, 6, 6, 128)            512       ['conv3_block6_1_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv3_block6_1_relu (Activ  (None, 6, 6, 128)            0         ['conv3_block6_1_bn[0][0]']   \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " conv3_block6_2_conv (Conv2  (None, 6, 6, 128)            147584    ['conv3_block6_1_relu[0][0]'] \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block6_2_bn (BatchNo  (None, 6, 6, 128)            512       ['conv3_block6_2_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv3_block6_2_relu (Activ  (None, 6, 6, 128)            0         ['conv3_block6_2_bn[0][0]']   \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " conv3_block6_3_conv (Conv2  (None, 6, 6, 512)            66048     ['conv3_block6_2_relu[0][0]'] \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block6_3_bn (BatchNo  (None, 6, 6, 512)            2048      ['conv3_block6_3_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv3_block6_add (Add)      (None, 6, 6, 512)            0         ['conv3_block5_out[0][0]',    \n",
      "                                                                     'conv3_block6_3_bn[0][0]']   \n",
      "                                                                                                  \n",
      " conv3_block6_out (Activati  (None, 6, 6, 512)            0         ['conv3_block6_add[0][0]']    \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv3_block7_1_conv (Conv2  (None, 6, 6, 128)            65664     ['conv3_block6_out[0][0]']    \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block7_1_bn (BatchNo  (None, 6, 6, 128)            512       ['conv3_block7_1_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv3_block7_1_relu (Activ  (None, 6, 6, 128)            0         ['conv3_block7_1_bn[0][0]']   \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " conv3_block7_2_conv (Conv2  (None, 6, 6, 128)            147584    ['conv3_block7_1_relu[0][0]'] \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block7_2_bn (BatchNo  (None, 6, 6, 128)            512       ['conv3_block7_2_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv3_block7_2_relu (Activ  (None, 6, 6, 128)            0         ['conv3_block7_2_bn[0][0]']   \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " conv3_block7_3_conv (Conv2  (None, 6, 6, 512)            66048     ['conv3_block7_2_relu[0][0]'] \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block7_3_bn (BatchNo  (None, 6, 6, 512)            2048      ['conv3_block7_3_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv3_block7_add (Add)      (None, 6, 6, 512)            0         ['conv3_block6_out[0][0]',    \n",
      "                                                                     'conv3_block7_3_bn[0][0]']   \n",
      "                                                                                                  \n",
      " conv3_block7_out (Activati  (None, 6, 6, 512)            0         ['conv3_block7_add[0][0]']    \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv3_block8_1_conv (Conv2  (None, 6, 6, 128)            65664     ['conv3_block7_out[0][0]']    \n",
      " D)                                                                                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " conv3_block8_1_bn (BatchNo  (None, 6, 6, 128)            512       ['conv3_block8_1_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv3_block8_1_relu (Activ  (None, 6, 6, 128)            0         ['conv3_block8_1_bn[0][0]']   \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " conv3_block8_2_conv (Conv2  (None, 6, 6, 128)            147584    ['conv3_block8_1_relu[0][0]'] \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block8_2_bn (BatchNo  (None, 6, 6, 128)            512       ['conv3_block8_2_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv3_block8_2_relu (Activ  (None, 6, 6, 128)            0         ['conv3_block8_2_bn[0][0]']   \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " conv3_block8_3_conv (Conv2  (None, 6, 6, 512)            66048     ['conv3_block8_2_relu[0][0]'] \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block8_3_bn (BatchNo  (None, 6, 6, 512)            2048      ['conv3_block8_3_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv3_block8_add (Add)      (None, 6, 6, 512)            0         ['conv3_block7_out[0][0]',    \n",
      "                                                                     'conv3_block8_3_bn[0][0]']   \n",
      "                                                                                                  \n",
      " conv3_block8_out (Activati  (None, 6, 6, 512)            0         ['conv3_block8_add[0][0]']    \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block1_1_conv (Conv2  (None, 3, 3, 256)            131328    ['conv3_block8_out[0][0]']    \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block1_1_bn (BatchNo  (None, 3, 3, 256)            1024      ['conv4_block1_1_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv4_block1_1_relu (Activ  (None, 3, 3, 256)            0         ['conv4_block1_1_bn[0][0]']   \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " conv4_block1_2_conv (Conv2  (None, 3, 3, 256)            590080    ['conv4_block1_1_relu[0][0]'] \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block1_2_bn (BatchNo  (None, 3, 3, 256)            1024      ['conv4_block1_2_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv4_block1_2_relu (Activ  (None, 3, 3, 256)            0         ['conv4_block1_2_bn[0][0]']   \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " conv4_block1_0_conv (Conv2  (None, 3, 3, 1024)           525312    ['conv3_block8_out[0][0]']    \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block1_3_conv (Conv2  (None, 3, 3, 1024)           263168    ['conv4_block1_2_relu[0][0]'] \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block1_0_bn (BatchNo  (None, 3, 3, 1024)           4096      ['conv4_block1_0_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv4_block1_3_bn (BatchNo  (None, 3, 3, 1024)           4096      ['conv4_block1_3_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv4_block1_add (Add)      (None, 3, 3, 1024)           0         ['conv4_block1_0_bn[0][0]',   \n",
      "                                                                     'conv4_block1_3_bn[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block1_out (Activati  (None, 3, 3, 1024)           0         ['conv4_block1_add[0][0]']    \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block2_1_conv (Conv2  (None, 3, 3, 256)            262400    ['conv4_block1_out[0][0]']    \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block2_1_bn (BatchNo  (None, 3, 3, 256)            1024      ['conv4_block2_1_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv4_block2_1_relu (Activ  (None, 3, 3, 256)            0         ['conv4_block2_1_bn[0][0]']   \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " conv4_block2_2_conv (Conv2  (None, 3, 3, 256)            590080    ['conv4_block2_1_relu[0][0]'] \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block2_2_bn (BatchNo  (None, 3, 3, 256)            1024      ['conv4_block2_2_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv4_block2_2_relu (Activ  (None, 3, 3, 256)            0         ['conv4_block2_2_bn[0][0]']   \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " conv4_block2_3_conv (Conv2  (None, 3, 3, 1024)           263168    ['conv4_block2_2_relu[0][0]'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block2_3_bn (BatchNo  (None, 3, 3, 1024)           4096      ['conv4_block2_3_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv4_block2_add (Add)      (None, 3, 3, 1024)           0         ['conv4_block1_out[0][0]',    \n",
      "                                                                     'conv4_block2_3_bn[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block2_out (Activati  (None, 3, 3, 1024)           0         ['conv4_block2_add[0][0]']    \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block3_1_conv (Conv2  (None, 3, 3, 256)            262400    ['conv4_block2_out[0][0]']    \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block3_1_bn (BatchNo  (None, 3, 3, 256)            1024      ['conv4_block3_1_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv4_block3_1_relu (Activ  (None, 3, 3, 256)            0         ['conv4_block3_1_bn[0][0]']   \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " conv4_block3_2_conv (Conv2  (None, 3, 3, 256)            590080    ['conv4_block3_1_relu[0][0]'] \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block3_2_bn (BatchNo  (None, 3, 3, 256)            1024      ['conv4_block3_2_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv4_block3_2_relu (Activ  (None, 3, 3, 256)            0         ['conv4_block3_2_bn[0][0]']   \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " conv4_block3_3_conv (Conv2  (None, 3, 3, 1024)           263168    ['conv4_block3_2_relu[0][0]'] \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block3_3_bn (BatchNo  (None, 3, 3, 1024)           4096      ['conv4_block3_3_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv4_block3_add (Add)      (None, 3, 3, 1024)           0         ['conv4_block2_out[0][0]',    \n",
      "                                                                     'conv4_block3_3_bn[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block3_out (Activati  (None, 3, 3, 1024)           0         ['conv4_block3_add[0][0]']    \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block4_1_conv (Conv2  (None, 3, 3, 256)            262400    ['conv4_block3_out[0][0]']    \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block4_1_bn (BatchNo  (None, 3, 3, 256)            1024      ['conv4_block4_1_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv4_block4_1_relu (Activ  (None, 3, 3, 256)            0         ['conv4_block4_1_bn[0][0]']   \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " conv4_block4_2_conv (Conv2  (None, 3, 3, 256)            590080    ['conv4_block4_1_relu[0][0]'] \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block4_2_bn (BatchNo  (None, 3, 3, 256)            1024      ['conv4_block4_2_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv4_block4_2_relu (Activ  (None, 3, 3, 256)            0         ['conv4_block4_2_bn[0][0]']   \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " conv4_block4_3_conv (Conv2  (None, 3, 3, 1024)           263168    ['conv4_block4_2_relu[0][0]'] \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block4_3_bn (BatchNo  (None, 3, 3, 1024)           4096      ['conv4_block4_3_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv4_block4_add (Add)      (None, 3, 3, 1024)           0         ['conv4_block3_out[0][0]',    \n",
      "                                                                     'conv4_block4_3_bn[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block4_out (Activati  (None, 3, 3, 1024)           0         ['conv4_block4_add[0][0]']    \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block5_1_conv (Conv2  (None, 3, 3, 256)            262400    ['conv4_block4_out[0][0]']    \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block5_1_bn (BatchNo  (None, 3, 3, 256)            1024      ['conv4_block5_1_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv4_block5_1_relu (Activ  (None, 3, 3, 256)            0         ['conv4_block5_1_bn[0][0]']   \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " conv4_block5_2_conv (Conv2  (None, 3, 3, 256)            590080    ['conv4_block5_1_relu[0][0]'] \n",
      " D)                                                                                               \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " conv4_block5_2_bn (BatchNo  (None, 3, 3, 256)            1024      ['conv4_block5_2_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv4_block5_2_relu (Activ  (None, 3, 3, 256)            0         ['conv4_block5_2_bn[0][0]']   \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " conv4_block5_3_conv (Conv2  (None, 3, 3, 1024)           263168    ['conv4_block5_2_relu[0][0]'] \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block5_3_bn (BatchNo  (None, 3, 3, 1024)           4096      ['conv4_block5_3_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv4_block5_add (Add)      (None, 3, 3, 1024)           0         ['conv4_block4_out[0][0]',    \n",
      "                                                                     'conv4_block5_3_bn[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block5_out (Activati  (None, 3, 3, 1024)           0         ['conv4_block5_add[0][0]']    \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block6_1_conv (Conv2  (None, 3, 3, 256)            262400    ['conv4_block5_out[0][0]']    \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block6_1_bn (BatchNo  (None, 3, 3, 256)            1024      ['conv4_block6_1_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv4_block6_1_relu (Activ  (None, 3, 3, 256)            0         ['conv4_block6_1_bn[0][0]']   \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " conv4_block6_2_conv (Conv2  (None, 3, 3, 256)            590080    ['conv4_block6_1_relu[0][0]'] \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block6_2_bn (BatchNo  (None, 3, 3, 256)            1024      ['conv4_block6_2_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv4_block6_2_relu (Activ  (None, 3, 3, 256)            0         ['conv4_block6_2_bn[0][0]']   \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " conv4_block6_3_conv (Conv2  (None, 3, 3, 1024)           263168    ['conv4_block6_2_relu[0][0]'] \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block6_3_bn (BatchNo  (None, 3, 3, 1024)           4096      ['conv4_block6_3_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv4_block6_add (Add)      (None, 3, 3, 1024)           0         ['conv4_block5_out[0][0]',    \n",
      "                                                                     'conv4_block6_3_bn[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block6_out (Activati  (None, 3, 3, 1024)           0         ['conv4_block6_add[0][0]']    \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block7_1_conv (Conv2  (None, 3, 3, 256)            262400    ['conv4_block6_out[0][0]']    \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block7_1_bn (BatchNo  (None, 3, 3, 256)            1024      ['conv4_block7_1_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv4_block7_1_relu (Activ  (None, 3, 3, 256)            0         ['conv4_block7_1_bn[0][0]']   \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " conv4_block7_2_conv (Conv2  (None, 3, 3, 256)            590080    ['conv4_block7_1_relu[0][0]'] \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block7_2_bn (BatchNo  (None, 3, 3, 256)            1024      ['conv4_block7_2_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv4_block7_2_relu (Activ  (None, 3, 3, 256)            0         ['conv4_block7_2_bn[0][0]']   \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " conv4_block7_3_conv (Conv2  (None, 3, 3, 1024)           263168    ['conv4_block7_2_relu[0][0]'] \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block7_3_bn (BatchNo  (None, 3, 3, 1024)           4096      ['conv4_block7_3_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv4_block7_add (Add)      (None, 3, 3, 1024)           0         ['conv4_block6_out[0][0]',    \n",
      "                                                                     'conv4_block7_3_bn[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block7_out (Activati  (None, 3, 3, 1024)           0         ['conv4_block7_add[0][0]']    \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block8_1_conv (Conv2  (None, 3, 3, 256)            262400    ['conv4_block7_out[0][0]']    \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block8_1_bn (BatchNo  (None, 3, 3, 256)            1024      ['conv4_block8_1_conv[0][0]'] \n",
      " rmalization)                                                                                     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " conv4_block8_1_relu (Activ  (None, 3, 3, 256)            0         ['conv4_block8_1_bn[0][0]']   \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " conv4_block8_2_conv (Conv2  (None, 3, 3, 256)            590080    ['conv4_block8_1_relu[0][0]'] \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block8_2_bn (BatchNo  (None, 3, 3, 256)            1024      ['conv4_block8_2_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv4_block8_2_relu (Activ  (None, 3, 3, 256)            0         ['conv4_block8_2_bn[0][0]']   \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " conv4_block8_3_conv (Conv2  (None, 3, 3, 1024)           263168    ['conv4_block8_2_relu[0][0]'] \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block8_3_bn (BatchNo  (None, 3, 3, 1024)           4096      ['conv4_block8_3_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv4_block8_add (Add)      (None, 3, 3, 1024)           0         ['conv4_block7_out[0][0]',    \n",
      "                                                                     'conv4_block8_3_bn[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block8_out (Activati  (None, 3, 3, 1024)           0         ['conv4_block8_add[0][0]']    \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block9_1_conv (Conv2  (None, 3, 3, 256)            262400    ['conv4_block8_out[0][0]']    \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block9_1_bn (BatchNo  (None, 3, 3, 256)            1024      ['conv4_block9_1_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv4_block9_1_relu (Activ  (None, 3, 3, 256)            0         ['conv4_block9_1_bn[0][0]']   \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " conv4_block9_2_conv (Conv2  (None, 3, 3, 256)            590080    ['conv4_block9_1_relu[0][0]'] \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block9_2_bn (BatchNo  (None, 3, 3, 256)            1024      ['conv4_block9_2_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv4_block9_2_relu (Activ  (None, 3, 3, 256)            0         ['conv4_block9_2_bn[0][0]']   \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " conv4_block9_3_conv (Conv2  (None, 3, 3, 1024)           263168    ['conv4_block9_2_relu[0][0]'] \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block9_3_bn (BatchNo  (None, 3, 3, 1024)           4096      ['conv4_block9_3_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv4_block9_add (Add)      (None, 3, 3, 1024)           0         ['conv4_block8_out[0][0]',    \n",
      "                                                                     'conv4_block9_3_bn[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block9_out (Activati  (None, 3, 3, 1024)           0         ['conv4_block9_add[0][0]']    \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block10_1_conv (Conv  (None, 3, 3, 256)            262400    ['conv4_block9_out[0][0]']    \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block10_1_bn (BatchN  (None, 3, 3, 256)            1024      ['conv4_block10_1_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block10_1_relu (Acti  (None, 3, 3, 256)            0         ['conv4_block10_1_bn[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv4_block10_2_conv (Conv  (None, 3, 3, 256)            590080    ['conv4_block10_1_relu[0][0]']\n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block10_2_bn (BatchN  (None, 3, 3, 256)            1024      ['conv4_block10_2_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block10_2_relu (Acti  (None, 3, 3, 256)            0         ['conv4_block10_2_bn[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv4_block10_3_conv (Conv  (None, 3, 3, 1024)           263168    ['conv4_block10_2_relu[0][0]']\n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block10_3_bn (BatchN  (None, 3, 3, 1024)           4096      ['conv4_block10_3_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block10_add (Add)     (None, 3, 3, 1024)           0         ['conv4_block9_out[0][0]',    \n",
      "                                                                     'conv4_block10_3_bn[0][0]']  \n",
      "                                                                                                  \n",
      " conv4_block10_out (Activat  (None, 3, 3, 1024)           0         ['conv4_block10_add[0][0]']   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ion)                                                                                             \n",
      "                                                                                                  \n",
      " conv4_block11_1_conv (Conv  (None, 3, 3, 256)            262400    ['conv4_block10_out[0][0]']   \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block11_1_bn (BatchN  (None, 3, 3, 256)            1024      ['conv4_block11_1_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block11_1_relu (Acti  (None, 3, 3, 256)            0         ['conv4_block11_1_bn[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv4_block11_2_conv (Conv  (None, 3, 3, 256)            590080    ['conv4_block11_1_relu[0][0]']\n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block11_2_bn (BatchN  (None, 3, 3, 256)            1024      ['conv4_block11_2_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block11_2_relu (Acti  (None, 3, 3, 256)            0         ['conv4_block11_2_bn[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv4_block11_3_conv (Conv  (None, 3, 3, 1024)           263168    ['conv4_block11_2_relu[0][0]']\n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block11_3_bn (BatchN  (None, 3, 3, 1024)           4096      ['conv4_block11_3_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block11_add (Add)     (None, 3, 3, 1024)           0         ['conv4_block10_out[0][0]',   \n",
      "                                                                     'conv4_block11_3_bn[0][0]']  \n",
      "                                                                                                  \n",
      " conv4_block11_out (Activat  (None, 3, 3, 1024)           0         ['conv4_block11_add[0][0]']   \n",
      " ion)                                                                                             \n",
      "                                                                                                  \n",
      " conv4_block12_1_conv (Conv  (None, 3, 3, 256)            262400    ['conv4_block11_out[0][0]']   \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block12_1_bn (BatchN  (None, 3, 3, 256)            1024      ['conv4_block12_1_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block12_1_relu (Acti  (None, 3, 3, 256)            0         ['conv4_block12_1_bn[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv4_block12_2_conv (Conv  (None, 3, 3, 256)            590080    ['conv4_block12_1_relu[0][0]']\n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block12_2_bn (BatchN  (None, 3, 3, 256)            1024      ['conv4_block12_2_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block12_2_relu (Acti  (None, 3, 3, 256)            0         ['conv4_block12_2_bn[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv4_block12_3_conv (Conv  (None, 3, 3, 1024)           263168    ['conv4_block12_2_relu[0][0]']\n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block12_3_bn (BatchN  (None, 3, 3, 1024)           4096      ['conv4_block12_3_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block12_add (Add)     (None, 3, 3, 1024)           0         ['conv4_block11_out[0][0]',   \n",
      "                                                                     'conv4_block12_3_bn[0][0]']  \n",
      "                                                                                                  \n",
      " conv4_block12_out (Activat  (None, 3, 3, 1024)           0         ['conv4_block12_add[0][0]']   \n",
      " ion)                                                                                             \n",
      "                                                                                                  \n",
      " conv4_block13_1_conv (Conv  (None, 3, 3, 256)            262400    ['conv4_block12_out[0][0]']   \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block13_1_bn (BatchN  (None, 3, 3, 256)            1024      ['conv4_block13_1_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block13_1_relu (Acti  (None, 3, 3, 256)            0         ['conv4_block13_1_bn[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv4_block13_2_conv (Conv  (None, 3, 3, 256)            590080    ['conv4_block13_1_relu[0][0]']\n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block13_2_bn (BatchN  (None, 3, 3, 256)            1024      ['conv4_block13_2_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block13_2_relu (Acti  (None, 3, 3, 256)            0         ['conv4_block13_2_bn[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv4_block13_3_conv (Conv  (None, 3, 3, 1024)           263168    ['conv4_block13_2_relu[0][0]']\n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " conv4_block13_3_bn (BatchN  (None, 3, 3, 1024)           4096      ['conv4_block13_3_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block13_add (Add)     (None, 3, 3, 1024)           0         ['conv4_block12_out[0][0]',   \n",
      "                                                                     'conv4_block13_3_bn[0][0]']  \n",
      "                                                                                                  \n",
      " conv4_block13_out (Activat  (None, 3, 3, 1024)           0         ['conv4_block13_add[0][0]']   \n",
      " ion)                                                                                             \n",
      "                                                                                                  \n",
      " conv4_block14_1_conv (Conv  (None, 3, 3, 256)            262400    ['conv4_block13_out[0][0]']   \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block14_1_bn (BatchN  (None, 3, 3, 256)            1024      ['conv4_block14_1_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block14_1_relu (Acti  (None, 3, 3, 256)            0         ['conv4_block14_1_bn[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv4_block14_2_conv (Conv  (None, 3, 3, 256)            590080    ['conv4_block14_1_relu[0][0]']\n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block14_2_bn (BatchN  (None, 3, 3, 256)            1024      ['conv4_block14_2_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block14_2_relu (Acti  (None, 3, 3, 256)            0         ['conv4_block14_2_bn[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv4_block14_3_conv (Conv  (None, 3, 3, 1024)           263168    ['conv4_block14_2_relu[0][0]']\n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block14_3_bn (BatchN  (None, 3, 3, 1024)           4096      ['conv4_block14_3_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block14_add (Add)     (None, 3, 3, 1024)           0         ['conv4_block13_out[0][0]',   \n",
      "                                                                     'conv4_block14_3_bn[0][0]']  \n",
      "                                                                                                  \n",
      " conv4_block14_out (Activat  (None, 3, 3, 1024)           0         ['conv4_block14_add[0][0]']   \n",
      " ion)                                                                                             \n",
      "                                                                                                  \n",
      " conv4_block15_1_conv (Conv  (None, 3, 3, 256)            262400    ['conv4_block14_out[0][0]']   \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block15_1_bn (BatchN  (None, 3, 3, 256)            1024      ['conv4_block15_1_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block15_1_relu (Acti  (None, 3, 3, 256)            0         ['conv4_block15_1_bn[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv4_block15_2_conv (Conv  (None, 3, 3, 256)            590080    ['conv4_block15_1_relu[0][0]']\n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block15_2_bn (BatchN  (None, 3, 3, 256)            1024      ['conv4_block15_2_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block15_2_relu (Acti  (None, 3, 3, 256)            0         ['conv4_block15_2_bn[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv4_block15_3_conv (Conv  (None, 3, 3, 1024)           263168    ['conv4_block15_2_relu[0][0]']\n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block15_3_bn (BatchN  (None, 3, 3, 1024)           4096      ['conv4_block15_3_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block15_add (Add)     (None, 3, 3, 1024)           0         ['conv4_block14_out[0][0]',   \n",
      "                                                                     'conv4_block15_3_bn[0][0]']  \n",
      "                                                                                                  \n",
      " conv4_block15_out (Activat  (None, 3, 3, 1024)           0         ['conv4_block15_add[0][0]']   \n",
      " ion)                                                                                             \n",
      "                                                                                                  \n",
      " conv4_block16_1_conv (Conv  (None, 3, 3, 256)            262400    ['conv4_block15_out[0][0]']   \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block16_1_bn (BatchN  (None, 3, 3, 256)            1024      ['conv4_block16_1_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block16_1_relu (Acti  (None, 3, 3, 256)            0         ['conv4_block16_1_bn[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv4_block16_2_conv (Conv  (None, 3, 3, 256)            590080    ['conv4_block16_1_relu[0][0]']\n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block16_2_bn (BatchN  (None, 3, 3, 256)            1024      ['conv4_block16_2_conv[0][0]']\n",
      " ormalization)                                                                                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " conv4_block16_2_relu (Acti  (None, 3, 3, 256)            0         ['conv4_block16_2_bn[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv4_block16_3_conv (Conv  (None, 3, 3, 1024)           263168    ['conv4_block16_2_relu[0][0]']\n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block16_3_bn (BatchN  (None, 3, 3, 1024)           4096      ['conv4_block16_3_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block16_add (Add)     (None, 3, 3, 1024)           0         ['conv4_block15_out[0][0]',   \n",
      "                                                                     'conv4_block16_3_bn[0][0]']  \n",
      "                                                                                                  \n",
      " conv4_block16_out (Activat  (None, 3, 3, 1024)           0         ['conv4_block16_add[0][0]']   \n",
      " ion)                                                                                             \n",
      "                                                                                                  \n",
      " conv4_block17_1_conv (Conv  (None, 3, 3, 256)            262400    ['conv4_block16_out[0][0]']   \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block17_1_bn (BatchN  (None, 3, 3, 256)            1024      ['conv4_block17_1_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block17_1_relu (Acti  (None, 3, 3, 256)            0         ['conv4_block17_1_bn[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv4_block17_2_conv (Conv  (None, 3, 3, 256)            590080    ['conv4_block17_1_relu[0][0]']\n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block17_2_bn (BatchN  (None, 3, 3, 256)            1024      ['conv4_block17_2_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block17_2_relu (Acti  (None, 3, 3, 256)            0         ['conv4_block17_2_bn[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv4_block17_3_conv (Conv  (None, 3, 3, 1024)           263168    ['conv4_block17_2_relu[0][0]']\n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block17_3_bn (BatchN  (None, 3, 3, 1024)           4096      ['conv4_block17_3_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block17_add (Add)     (None, 3, 3, 1024)           0         ['conv4_block16_out[0][0]',   \n",
      "                                                                     'conv4_block17_3_bn[0][0]']  \n",
      "                                                                                                  \n",
      " conv4_block17_out (Activat  (None, 3, 3, 1024)           0         ['conv4_block17_add[0][0]']   \n",
      " ion)                                                                                             \n",
      "                                                                                                  \n",
      " conv4_block18_1_conv (Conv  (None, 3, 3, 256)            262400    ['conv4_block17_out[0][0]']   \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block18_1_bn (BatchN  (None, 3, 3, 256)            1024      ['conv4_block18_1_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block18_1_relu (Acti  (None, 3, 3, 256)            0         ['conv4_block18_1_bn[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv4_block18_2_conv (Conv  (None, 3, 3, 256)            590080    ['conv4_block18_1_relu[0][0]']\n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block18_2_bn (BatchN  (None, 3, 3, 256)            1024      ['conv4_block18_2_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block18_2_relu (Acti  (None, 3, 3, 256)            0         ['conv4_block18_2_bn[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv4_block18_3_conv (Conv  (None, 3, 3, 1024)           263168    ['conv4_block18_2_relu[0][0]']\n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block18_3_bn (BatchN  (None, 3, 3, 1024)           4096      ['conv4_block18_3_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block18_add (Add)     (None, 3, 3, 1024)           0         ['conv4_block17_out[0][0]',   \n",
      "                                                                     'conv4_block18_3_bn[0][0]']  \n",
      "                                                                                                  \n",
      " conv4_block18_out (Activat  (None, 3, 3, 1024)           0         ['conv4_block18_add[0][0]']   \n",
      " ion)                                                                                             \n",
      "                                                                                                  \n",
      " conv4_block19_1_conv (Conv  (None, 3, 3, 256)            262400    ['conv4_block18_out[0][0]']   \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block19_1_bn (BatchN  (None, 3, 3, 256)            1024      ['conv4_block19_1_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block19_1_relu (Acti  (None, 3, 3, 256)            0         ['conv4_block19_1_bn[0][0]']  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv4_block19_2_conv (Conv  (None, 3, 3, 256)            590080    ['conv4_block19_1_relu[0][0]']\n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block19_2_bn (BatchN  (None, 3, 3, 256)            1024      ['conv4_block19_2_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block19_2_relu (Acti  (None, 3, 3, 256)            0         ['conv4_block19_2_bn[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv4_block19_3_conv (Conv  (None, 3, 3, 1024)           263168    ['conv4_block19_2_relu[0][0]']\n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block19_3_bn (BatchN  (None, 3, 3, 1024)           4096      ['conv4_block19_3_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block19_add (Add)     (None, 3, 3, 1024)           0         ['conv4_block18_out[0][0]',   \n",
      "                                                                     'conv4_block19_3_bn[0][0]']  \n",
      "                                                                                                  \n",
      " conv4_block19_out (Activat  (None, 3, 3, 1024)           0         ['conv4_block19_add[0][0]']   \n",
      " ion)                                                                                             \n",
      "                                                                                                  \n",
      " conv4_block20_1_conv (Conv  (None, 3, 3, 256)            262400    ['conv4_block19_out[0][0]']   \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block20_1_bn (BatchN  (None, 3, 3, 256)            1024      ['conv4_block20_1_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block20_1_relu (Acti  (None, 3, 3, 256)            0         ['conv4_block20_1_bn[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv4_block20_2_conv (Conv  (None, 3, 3, 256)            590080    ['conv4_block20_1_relu[0][0]']\n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block20_2_bn (BatchN  (None, 3, 3, 256)            1024      ['conv4_block20_2_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block20_2_relu (Acti  (None, 3, 3, 256)            0         ['conv4_block20_2_bn[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv4_block20_3_conv (Conv  (None, 3, 3, 1024)           263168    ['conv4_block20_2_relu[0][0]']\n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block20_3_bn (BatchN  (None, 3, 3, 1024)           4096      ['conv4_block20_3_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block20_add (Add)     (None, 3, 3, 1024)           0         ['conv4_block19_out[0][0]',   \n",
      "                                                                     'conv4_block20_3_bn[0][0]']  \n",
      "                                                                                                  \n",
      " conv4_block20_out (Activat  (None, 3, 3, 1024)           0         ['conv4_block20_add[0][0]']   \n",
      " ion)                                                                                             \n",
      "                                                                                                  \n",
      " conv4_block21_1_conv (Conv  (None, 3, 3, 256)            262400    ['conv4_block20_out[0][0]']   \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block21_1_bn (BatchN  (None, 3, 3, 256)            1024      ['conv4_block21_1_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block21_1_relu (Acti  (None, 3, 3, 256)            0         ['conv4_block21_1_bn[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv4_block21_2_conv (Conv  (None, 3, 3, 256)            590080    ['conv4_block21_1_relu[0][0]']\n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block21_2_bn (BatchN  (None, 3, 3, 256)            1024      ['conv4_block21_2_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block21_2_relu (Acti  (None, 3, 3, 256)            0         ['conv4_block21_2_bn[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv4_block21_3_conv (Conv  (None, 3, 3, 1024)           263168    ['conv4_block21_2_relu[0][0]']\n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block21_3_bn (BatchN  (None, 3, 3, 1024)           4096      ['conv4_block21_3_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block21_add (Add)     (None, 3, 3, 1024)           0         ['conv4_block20_out[0][0]',   \n",
      "                                                                     'conv4_block21_3_bn[0][0]']  \n",
      "                                                                                                  \n",
      " conv4_block21_out (Activat  (None, 3, 3, 1024)           0         ['conv4_block21_add[0][0]']   \n",
      " ion)                                                                                             \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " conv4_block22_1_conv (Conv  (None, 3, 3, 256)            262400    ['conv4_block21_out[0][0]']   \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block22_1_bn (BatchN  (None, 3, 3, 256)            1024      ['conv4_block22_1_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block22_1_relu (Acti  (None, 3, 3, 256)            0         ['conv4_block22_1_bn[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv4_block22_2_conv (Conv  (None, 3, 3, 256)            590080    ['conv4_block22_1_relu[0][0]']\n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block22_2_bn (BatchN  (None, 3, 3, 256)            1024      ['conv4_block22_2_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block22_2_relu (Acti  (None, 3, 3, 256)            0         ['conv4_block22_2_bn[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv4_block22_3_conv (Conv  (None, 3, 3, 1024)           263168    ['conv4_block22_2_relu[0][0]']\n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block22_3_bn (BatchN  (None, 3, 3, 1024)           4096      ['conv4_block22_3_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block22_add (Add)     (None, 3, 3, 1024)           0         ['conv4_block21_out[0][0]',   \n",
      "                                                                     'conv4_block22_3_bn[0][0]']  \n",
      "                                                                                                  \n",
      " conv4_block22_out (Activat  (None, 3, 3, 1024)           0         ['conv4_block22_add[0][0]']   \n",
      " ion)                                                                                             \n",
      "                                                                                                  \n",
      " conv4_block23_1_conv (Conv  (None, 3, 3, 256)            262400    ['conv4_block22_out[0][0]']   \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block23_1_bn (BatchN  (None, 3, 3, 256)            1024      ['conv4_block23_1_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block23_1_relu (Acti  (None, 3, 3, 256)            0         ['conv4_block23_1_bn[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv4_block23_2_conv (Conv  (None, 3, 3, 256)            590080    ['conv4_block23_1_relu[0][0]']\n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block23_2_bn (BatchN  (None, 3, 3, 256)            1024      ['conv4_block23_2_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block23_2_relu (Acti  (None, 3, 3, 256)            0         ['conv4_block23_2_bn[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv4_block23_3_conv (Conv  (None, 3, 3, 1024)           263168    ['conv4_block23_2_relu[0][0]']\n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block23_3_bn (BatchN  (None, 3, 3, 1024)           4096      ['conv4_block23_3_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block23_add (Add)     (None, 3, 3, 1024)           0         ['conv4_block22_out[0][0]',   \n",
      "                                                                     'conv4_block23_3_bn[0][0]']  \n",
      "                                                                                                  \n",
      " conv4_block23_out (Activat  (None, 3, 3, 1024)           0         ['conv4_block23_add[0][0]']   \n",
      " ion)                                                                                             \n",
      "                                                                                                  \n",
      " conv4_block24_1_conv (Conv  (None, 3, 3, 256)            262400    ['conv4_block23_out[0][0]']   \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block24_1_bn (BatchN  (None, 3, 3, 256)            1024      ['conv4_block24_1_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block24_1_relu (Acti  (None, 3, 3, 256)            0         ['conv4_block24_1_bn[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv4_block24_2_conv (Conv  (None, 3, 3, 256)            590080    ['conv4_block24_1_relu[0][0]']\n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block24_2_bn (BatchN  (None, 3, 3, 256)            1024      ['conv4_block24_2_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block24_2_relu (Acti  (None, 3, 3, 256)            0         ['conv4_block24_2_bn[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv4_block24_3_conv (Conv  (None, 3, 3, 1024)           263168    ['conv4_block24_2_relu[0][0]']\n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block24_3_bn (BatchN  (None, 3, 3, 1024)           4096      ['conv4_block24_3_conv[0][0]']\n",
      " ormalization)                                                                                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " conv4_block24_add (Add)     (None, 3, 3, 1024)           0         ['conv4_block23_out[0][0]',   \n",
      "                                                                     'conv4_block24_3_bn[0][0]']  \n",
      "                                                                                                  \n",
      " conv4_block24_out (Activat  (None, 3, 3, 1024)           0         ['conv4_block24_add[0][0]']   \n",
      " ion)                                                                                             \n",
      "                                                                                                  \n",
      " conv4_block25_1_conv (Conv  (None, 3, 3, 256)            262400    ['conv4_block24_out[0][0]']   \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block25_1_bn (BatchN  (None, 3, 3, 256)            1024      ['conv4_block25_1_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block25_1_relu (Acti  (None, 3, 3, 256)            0         ['conv4_block25_1_bn[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv4_block25_2_conv (Conv  (None, 3, 3, 256)            590080    ['conv4_block25_1_relu[0][0]']\n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block25_2_bn (BatchN  (None, 3, 3, 256)            1024      ['conv4_block25_2_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block25_2_relu (Acti  (None, 3, 3, 256)            0         ['conv4_block25_2_bn[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv4_block25_3_conv (Conv  (None, 3, 3, 1024)           263168    ['conv4_block25_2_relu[0][0]']\n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block25_3_bn (BatchN  (None, 3, 3, 1024)           4096      ['conv4_block25_3_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block25_add (Add)     (None, 3, 3, 1024)           0         ['conv4_block24_out[0][0]',   \n",
      "                                                                     'conv4_block25_3_bn[0][0]']  \n",
      "                                                                                                  \n",
      " conv4_block25_out (Activat  (None, 3, 3, 1024)           0         ['conv4_block25_add[0][0]']   \n",
      " ion)                                                                                             \n",
      "                                                                                                  \n",
      " conv4_block26_1_conv (Conv  (None, 3, 3, 256)            262400    ['conv4_block25_out[0][0]']   \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block26_1_bn (BatchN  (None, 3, 3, 256)            1024      ['conv4_block26_1_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block26_1_relu (Acti  (None, 3, 3, 256)            0         ['conv4_block26_1_bn[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv4_block26_2_conv (Conv  (None, 3, 3, 256)            590080    ['conv4_block26_1_relu[0][0]']\n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block26_2_bn (BatchN  (None, 3, 3, 256)            1024      ['conv4_block26_2_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block26_2_relu (Acti  (None, 3, 3, 256)            0         ['conv4_block26_2_bn[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv4_block26_3_conv (Conv  (None, 3, 3, 1024)           263168    ['conv4_block26_2_relu[0][0]']\n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block26_3_bn (BatchN  (None, 3, 3, 1024)           4096      ['conv4_block26_3_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block26_add (Add)     (None, 3, 3, 1024)           0         ['conv4_block25_out[0][0]',   \n",
      "                                                                     'conv4_block26_3_bn[0][0]']  \n",
      "                                                                                                  \n",
      " conv4_block26_out (Activat  (None, 3, 3, 1024)           0         ['conv4_block26_add[0][0]']   \n",
      " ion)                                                                                             \n",
      "                                                                                                  \n",
      " conv4_block27_1_conv (Conv  (None, 3, 3, 256)            262400    ['conv4_block26_out[0][0]']   \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block27_1_bn (BatchN  (None, 3, 3, 256)            1024      ['conv4_block27_1_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block27_1_relu (Acti  (None, 3, 3, 256)            0         ['conv4_block27_1_bn[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv4_block27_2_conv (Conv  (None, 3, 3, 256)            590080    ['conv4_block27_1_relu[0][0]']\n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block27_2_bn (BatchN  (None, 3, 3, 256)            1024      ['conv4_block27_2_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block27_2_relu (Acti  (None, 3, 3, 256)            0         ['conv4_block27_2_bn[0][0]']  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv4_block27_3_conv (Conv  (None, 3, 3, 1024)           263168    ['conv4_block27_2_relu[0][0]']\n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block27_3_bn (BatchN  (None, 3, 3, 1024)           4096      ['conv4_block27_3_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block27_add (Add)     (None, 3, 3, 1024)           0         ['conv4_block26_out[0][0]',   \n",
      "                                                                     'conv4_block27_3_bn[0][0]']  \n",
      "                                                                                                  \n",
      " conv4_block27_out (Activat  (None, 3, 3, 1024)           0         ['conv4_block27_add[0][0]']   \n",
      " ion)                                                                                             \n",
      "                                                                                                  \n",
      " conv4_block28_1_conv (Conv  (None, 3, 3, 256)            262400    ['conv4_block27_out[0][0]']   \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block28_1_bn (BatchN  (None, 3, 3, 256)            1024      ['conv4_block28_1_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block28_1_relu (Acti  (None, 3, 3, 256)            0         ['conv4_block28_1_bn[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv4_block28_2_conv (Conv  (None, 3, 3, 256)            590080    ['conv4_block28_1_relu[0][0]']\n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block28_2_bn (BatchN  (None, 3, 3, 256)            1024      ['conv4_block28_2_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block28_2_relu (Acti  (None, 3, 3, 256)            0         ['conv4_block28_2_bn[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv4_block28_3_conv (Conv  (None, 3, 3, 1024)           263168    ['conv4_block28_2_relu[0][0]']\n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block28_3_bn (BatchN  (None, 3, 3, 1024)           4096      ['conv4_block28_3_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block28_add (Add)     (None, 3, 3, 1024)           0         ['conv4_block27_out[0][0]',   \n",
      "                                                                     'conv4_block28_3_bn[0][0]']  \n",
      "                                                                                                  \n",
      " conv4_block28_out (Activat  (None, 3, 3, 1024)           0         ['conv4_block28_add[0][0]']   \n",
      " ion)                                                                                             \n",
      "                                                                                                  \n",
      " conv4_block29_1_conv (Conv  (None, 3, 3, 256)            262400    ['conv4_block28_out[0][0]']   \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block29_1_bn (BatchN  (None, 3, 3, 256)            1024      ['conv4_block29_1_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block29_1_relu (Acti  (None, 3, 3, 256)            0         ['conv4_block29_1_bn[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv4_block29_2_conv (Conv  (None, 3, 3, 256)            590080    ['conv4_block29_1_relu[0][0]']\n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block29_2_bn (BatchN  (None, 3, 3, 256)            1024      ['conv4_block29_2_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block29_2_relu (Acti  (None, 3, 3, 256)            0         ['conv4_block29_2_bn[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv4_block29_3_conv (Conv  (None, 3, 3, 1024)           263168    ['conv4_block29_2_relu[0][0]']\n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block29_3_bn (BatchN  (None, 3, 3, 1024)           4096      ['conv4_block29_3_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block29_add (Add)     (None, 3, 3, 1024)           0         ['conv4_block28_out[0][0]',   \n",
      "                                                                     'conv4_block29_3_bn[0][0]']  \n",
      "                                                                                                  \n",
      " conv4_block29_out (Activat  (None, 3, 3, 1024)           0         ['conv4_block29_add[0][0]']   \n",
      " ion)                                                                                             \n",
      "                                                                                                  \n",
      " conv4_block30_1_conv (Conv  (None, 3, 3, 256)            262400    ['conv4_block29_out[0][0]']   \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block30_1_bn (BatchN  (None, 3, 3, 256)            1024      ['conv4_block30_1_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block30_1_relu (Acti  (None, 3, 3, 256)            0         ['conv4_block30_1_bn[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " conv4_block30_2_conv (Conv  (None, 3, 3, 256)            590080    ['conv4_block30_1_relu[0][0]']\n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block30_2_bn (BatchN  (None, 3, 3, 256)            1024      ['conv4_block30_2_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block30_2_relu (Acti  (None, 3, 3, 256)            0         ['conv4_block30_2_bn[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv4_block30_3_conv (Conv  (None, 3, 3, 1024)           263168    ['conv4_block30_2_relu[0][0]']\n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block30_3_bn (BatchN  (None, 3, 3, 1024)           4096      ['conv4_block30_3_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block30_add (Add)     (None, 3, 3, 1024)           0         ['conv4_block29_out[0][0]',   \n",
      "                                                                     'conv4_block30_3_bn[0][0]']  \n",
      "                                                                                                  \n",
      " conv4_block30_out (Activat  (None, 3, 3, 1024)           0         ['conv4_block30_add[0][0]']   \n",
      " ion)                                                                                             \n",
      "                                                                                                  \n",
      " conv4_block31_1_conv (Conv  (None, 3, 3, 256)            262400    ['conv4_block30_out[0][0]']   \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block31_1_bn (BatchN  (None, 3, 3, 256)            1024      ['conv4_block31_1_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block31_1_relu (Acti  (None, 3, 3, 256)            0         ['conv4_block31_1_bn[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv4_block31_2_conv (Conv  (None, 3, 3, 256)            590080    ['conv4_block31_1_relu[0][0]']\n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block31_2_bn (BatchN  (None, 3, 3, 256)            1024      ['conv4_block31_2_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block31_2_relu (Acti  (None, 3, 3, 256)            0         ['conv4_block31_2_bn[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv4_block31_3_conv (Conv  (None, 3, 3, 1024)           263168    ['conv4_block31_2_relu[0][0]']\n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block31_3_bn (BatchN  (None, 3, 3, 1024)           4096      ['conv4_block31_3_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block31_add (Add)     (None, 3, 3, 1024)           0         ['conv4_block30_out[0][0]',   \n",
      "                                                                     'conv4_block31_3_bn[0][0]']  \n",
      "                                                                                                  \n",
      " conv4_block31_out (Activat  (None, 3, 3, 1024)           0         ['conv4_block31_add[0][0]']   \n",
      " ion)                                                                                             \n",
      "                                                                                                  \n",
      " conv4_block32_1_conv (Conv  (None, 3, 3, 256)            262400    ['conv4_block31_out[0][0]']   \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block32_1_bn (BatchN  (None, 3, 3, 256)            1024      ['conv4_block32_1_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block32_1_relu (Acti  (None, 3, 3, 256)            0         ['conv4_block32_1_bn[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv4_block32_2_conv (Conv  (None, 3, 3, 256)            590080    ['conv4_block32_1_relu[0][0]']\n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block32_2_bn (BatchN  (None, 3, 3, 256)            1024      ['conv4_block32_2_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block32_2_relu (Acti  (None, 3, 3, 256)            0         ['conv4_block32_2_bn[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv4_block32_3_conv (Conv  (None, 3, 3, 1024)           263168    ['conv4_block32_2_relu[0][0]']\n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block32_3_bn (BatchN  (None, 3, 3, 1024)           4096      ['conv4_block32_3_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block32_add (Add)     (None, 3, 3, 1024)           0         ['conv4_block31_out[0][0]',   \n",
      "                                                                     'conv4_block32_3_bn[0][0]']  \n",
      "                                                                                                  \n",
      " conv4_block32_out (Activat  (None, 3, 3, 1024)           0         ['conv4_block32_add[0][0]']   \n",
      " ion)                                                                                             \n",
      "                                                                                                  \n",
      " conv4_block33_1_conv (Conv  (None, 3, 3, 256)            262400    ['conv4_block32_out[0][0]']   \n",
      " 2D)                                                                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " conv4_block33_1_bn (BatchN  (None, 3, 3, 256)            1024      ['conv4_block33_1_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block33_1_relu (Acti  (None, 3, 3, 256)            0         ['conv4_block33_1_bn[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv4_block33_2_conv (Conv  (None, 3, 3, 256)            590080    ['conv4_block33_1_relu[0][0]']\n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block33_2_bn (BatchN  (None, 3, 3, 256)            1024      ['conv4_block33_2_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block33_2_relu (Acti  (None, 3, 3, 256)            0         ['conv4_block33_2_bn[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv4_block33_3_conv (Conv  (None, 3, 3, 1024)           263168    ['conv4_block33_2_relu[0][0]']\n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block33_3_bn (BatchN  (None, 3, 3, 1024)           4096      ['conv4_block33_3_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block33_add (Add)     (None, 3, 3, 1024)           0         ['conv4_block32_out[0][0]',   \n",
      "                                                                     'conv4_block33_3_bn[0][0]']  \n",
      "                                                                                                  \n",
      " conv4_block33_out (Activat  (None, 3, 3, 1024)           0         ['conv4_block33_add[0][0]']   \n",
      " ion)                                                                                             \n",
      "                                                                                                  \n",
      " conv4_block34_1_conv (Conv  (None, 3, 3, 256)            262400    ['conv4_block33_out[0][0]']   \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block34_1_bn (BatchN  (None, 3, 3, 256)            1024      ['conv4_block34_1_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block34_1_relu (Acti  (None, 3, 3, 256)            0         ['conv4_block34_1_bn[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv4_block34_2_conv (Conv  (None, 3, 3, 256)            590080    ['conv4_block34_1_relu[0][0]']\n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block34_2_bn (BatchN  (None, 3, 3, 256)            1024      ['conv4_block34_2_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block34_2_relu (Acti  (None, 3, 3, 256)            0         ['conv4_block34_2_bn[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv4_block34_3_conv (Conv  (None, 3, 3, 1024)           263168    ['conv4_block34_2_relu[0][0]']\n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block34_3_bn (BatchN  (None, 3, 3, 1024)           4096      ['conv4_block34_3_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block34_add (Add)     (None, 3, 3, 1024)           0         ['conv4_block33_out[0][0]',   \n",
      "                                                                     'conv4_block34_3_bn[0][0]']  \n",
      "                                                                                                  \n",
      " conv4_block34_out (Activat  (None, 3, 3, 1024)           0         ['conv4_block34_add[0][0]']   \n",
      " ion)                                                                                             \n",
      "                                                                                                  \n",
      " conv4_block35_1_conv (Conv  (None, 3, 3, 256)            262400    ['conv4_block34_out[0][0]']   \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block35_1_bn (BatchN  (None, 3, 3, 256)            1024      ['conv4_block35_1_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block35_1_relu (Acti  (None, 3, 3, 256)            0         ['conv4_block35_1_bn[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv4_block35_2_conv (Conv  (None, 3, 3, 256)            590080    ['conv4_block35_1_relu[0][0]']\n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block35_2_bn (BatchN  (None, 3, 3, 256)            1024      ['conv4_block35_2_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block35_2_relu (Acti  (None, 3, 3, 256)            0         ['conv4_block35_2_bn[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv4_block35_3_conv (Conv  (None, 3, 3, 1024)           263168    ['conv4_block35_2_relu[0][0]']\n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block35_3_bn (BatchN  (None, 3, 3, 1024)           4096      ['conv4_block35_3_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block35_add (Add)     (None, 3, 3, 1024)           0         ['conv4_block34_out[0][0]',   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                     'conv4_block35_3_bn[0][0]']  \n",
      "                                                                                                  \n",
      " conv4_block35_out (Activat  (None, 3, 3, 1024)           0         ['conv4_block35_add[0][0]']   \n",
      " ion)                                                                                             \n",
      "                                                                                                  \n",
      " conv4_block36_1_conv (Conv  (None, 3, 3, 256)            262400    ['conv4_block35_out[0][0]']   \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block36_1_bn (BatchN  (None, 3, 3, 256)            1024      ['conv4_block36_1_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block36_1_relu (Acti  (None, 3, 3, 256)            0         ['conv4_block36_1_bn[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv4_block36_2_conv (Conv  (None, 3, 3, 256)            590080    ['conv4_block36_1_relu[0][0]']\n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block36_2_bn (BatchN  (None, 3, 3, 256)            1024      ['conv4_block36_2_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block36_2_relu (Acti  (None, 3, 3, 256)            0         ['conv4_block36_2_bn[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv4_block36_3_conv (Conv  (None, 3, 3, 1024)           263168    ['conv4_block36_2_relu[0][0]']\n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block36_3_bn (BatchN  (None, 3, 3, 1024)           4096      ['conv4_block36_3_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block36_add (Add)     (None, 3, 3, 1024)           0         ['conv4_block35_out[0][0]',   \n",
      "                                                                     'conv4_block36_3_bn[0][0]']  \n",
      "                                                                                                  \n",
      " conv4_block36_out (Activat  (None, 3, 3, 1024)           0         ['conv4_block36_add[0][0]']   \n",
      " ion)                                                                                             \n",
      "                                                                                                  \n",
      " conv5_block1_1_conv (Conv2  (None, 2, 2, 512)            524800    ['conv4_block36_out[0][0]']   \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block1_1_bn (BatchNo  (None, 2, 2, 512)            2048      ['conv5_block1_1_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv5_block1_1_relu (Activ  (None, 2, 2, 512)            0         ['conv5_block1_1_bn[0][0]']   \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " conv5_block1_2_conv (Conv2  (None, 2, 2, 512)            2359808   ['conv5_block1_1_relu[0][0]'] \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block1_2_bn (BatchNo  (None, 2, 2, 512)            2048      ['conv5_block1_2_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv5_block1_2_relu (Activ  (None, 2, 2, 512)            0         ['conv5_block1_2_bn[0][0]']   \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " conv5_block1_0_conv (Conv2  (None, 2, 2, 2048)           2099200   ['conv4_block36_out[0][0]']   \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block1_3_conv (Conv2  (None, 2, 2, 2048)           1050624   ['conv5_block1_2_relu[0][0]'] \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block1_0_bn (BatchNo  (None, 2, 2, 2048)           8192      ['conv5_block1_0_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv5_block1_3_bn (BatchNo  (None, 2, 2, 2048)           8192      ['conv5_block1_3_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv5_block1_add (Add)      (None, 2, 2, 2048)           0         ['conv5_block1_0_bn[0][0]',   \n",
      "                                                                     'conv5_block1_3_bn[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block1_out (Activati  (None, 2, 2, 2048)           0         ['conv5_block1_add[0][0]']    \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block2_1_conv (Conv2  (None, 2, 2, 512)            1049088   ['conv5_block1_out[0][0]']    \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block2_1_bn (BatchNo  (None, 2, 2, 512)            2048      ['conv5_block2_1_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv5_block2_1_relu (Activ  (None, 2, 2, 512)            0         ['conv5_block2_1_bn[0][0]']   \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " conv5_block2_2_conv (Conv2  (None, 2, 2, 512)            2359808   ['conv5_block2_1_relu[0][0]'] \n",
      " D)                                                                                               \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " conv5_block2_2_bn (BatchNo  (None, 2, 2, 512)            2048      ['conv5_block2_2_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv5_block2_2_relu (Activ  (None, 2, 2, 512)            0         ['conv5_block2_2_bn[0][0]']   \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " conv5_block2_3_conv (Conv2  (None, 2, 2, 2048)           1050624   ['conv5_block2_2_relu[0][0]'] \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block2_3_bn (BatchNo  (None, 2, 2, 2048)           8192      ['conv5_block2_3_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv5_block2_add (Add)      (None, 2, 2, 2048)           0         ['conv5_block1_out[0][0]',    \n",
      "                                                                     'conv5_block2_3_bn[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block2_out (Activati  (None, 2, 2, 2048)           0         ['conv5_block2_add[0][0]']    \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block3_1_conv (Conv2  (None, 2, 2, 512)            1049088   ['conv5_block2_out[0][0]']    \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block3_1_bn (BatchNo  (None, 2, 2, 512)            2048      ['conv5_block3_1_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv5_block3_1_relu (Activ  (None, 2, 2, 512)            0         ['conv5_block3_1_bn[0][0]']   \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " conv5_block3_2_conv (Conv2  (None, 2, 2, 512)            2359808   ['conv5_block3_1_relu[0][0]'] \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block3_2_bn (BatchNo  (None, 2, 2, 512)            2048      ['conv5_block3_2_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv5_block3_2_relu (Activ  (None, 2, 2, 512)            0         ['conv5_block3_2_bn[0][0]']   \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " conv5_block3_3_conv (Conv2  (None, 2, 2, 2048)           1050624   ['conv5_block3_2_relu[0][0]'] \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block3_3_bn (BatchNo  (None, 2, 2, 2048)           8192      ['conv5_block3_3_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv5_block3_add (Add)      (None, 2, 2, 2048)           0         ['conv5_block2_out[0][0]',    \n",
      "                                                                     'conv5_block3_3_bn[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block3_out (Activati  (None, 2, 2, 2048)           0         ['conv5_block3_add[0][0]']    \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 58370944 (222.67 MB)\n",
      "Trainable params: 58219520 (222.09 MB)\n",
      "Non-trainable params: 151424 (591.50 KB)\n",
      "__________________________________________________________________________________________________\n",
      "CNN model has been created, you can proceed to train your data with this model.\n",
      "Epoch 1/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-07 16:43:15.713228: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8905\n",
      "2024-10-07 16:43:16.017973: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:606] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2024-10-07 16:43:16.587462: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x32d660b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-10-07 16:43:16.587480: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3070 Laptop GPU, Compute Capability 8.6\n",
      "2024-10-07 16:43:16.602828: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-10-07 16:43:16.749393: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225/225 [==============================] - 28s 90ms/step - loss: 2.4214 - accuracy: 0.1721 - val_loss: 1.9072 - val_accuracy: 0.2143\n",
      "Epoch 2/500\n",
      "225/225 [==============================] - 15s 68ms/step - loss: 2.2151 - accuracy: 0.2019 - val_loss: 1.9485 - val_accuracy: 0.2533\n",
      "Epoch 3/500\n",
      "225/225 [==============================] - 15s 68ms/step - loss: 2.1204 - accuracy: 0.2208 - val_loss: 1.8729 - val_accuracy: 0.2697\n",
      "Epoch 4/500\n",
      "225/225 [==============================] - 15s 69ms/step - loss: 2.0605 - accuracy: 0.2311 - val_loss: 1.9867 - val_accuracy: 0.2611\n",
      "Epoch 5/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 2.0249 - accuracy: 0.2372 - val_loss: 1.8820 - val_accuracy: 0.2647\n",
      "Epoch 6/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 2.0021 - accuracy: 0.2389 - val_loss: 1.7626 - val_accuracy: 0.2905\n",
      "Epoch 7/500\n",
      "225/225 [==============================] - 16s 71ms/step - loss: 1.9653 - accuracy: 0.2454 - val_loss: 1.7373 - val_accuracy: 0.3015\n",
      "Epoch 8/500\n",
      "225/225 [==============================] - 16s 72ms/step - loss: 1.9420 - accuracy: 0.2527 - val_loss: 1.7542 - val_accuracy: 0.2937\n",
      "Epoch 9/500\n",
      "225/225 [==============================] - 16s 71ms/step - loss: 1.9212 - accuracy: 0.2577 - val_loss: 1.7727 - val_accuracy: 0.3045\n",
      "Epoch 10/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.9049 - accuracy: 0.2642 - val_loss: 1.7333 - val_accuracy: 0.3011\n",
      "Epoch 11/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.8850 - accuracy: 0.2649 - val_loss: 1.9469 - val_accuracy: 0.2708\n",
      "Epoch 12/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.8789 - accuracy: 0.2650 - val_loss: 1.7065 - val_accuracy: 0.3171\n",
      "Epoch 13/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.8584 - accuracy: 0.2698 - val_loss: 1.7386 - val_accuracy: 0.2859\n",
      "Epoch 14/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.8491 - accuracy: 0.2759 - val_loss: 1.7784 - val_accuracy: 0.2938\n",
      "Epoch 15/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.8436 - accuracy: 0.2752 - val_loss: 1.7694 - val_accuracy: 0.2464\n",
      "Epoch 16/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.8277 - accuracy: 0.2806 - val_loss: 1.7246 - val_accuracy: 0.3186\n",
      "Epoch 17/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.8200 - accuracy: 0.2796 - val_loss: 1.7022 - val_accuracy: 0.3217\n",
      "Epoch 18/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.8103 - accuracy: 0.2837 - val_loss: 1.7417 - val_accuracy: 0.2966\n",
      "Epoch 19/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.8004 - accuracy: 0.2899 - val_loss: 1.7237 - val_accuracy: 0.2984\n",
      "Epoch 20/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.7914 - accuracy: 0.2894 - val_loss: 1.7213 - val_accuracy: 0.3069\n",
      "Epoch 21/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.7852 - accuracy: 0.2941 - val_loss: 1.6889 - val_accuracy: 0.3178\n",
      "Epoch 22/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.7792 - accuracy: 0.2933 - val_loss: 1.7541 - val_accuracy: 0.3062\n",
      "Epoch 23/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.7717 - accuracy: 0.2962 - val_loss: 1.7190 - val_accuracy: 0.3126\n",
      "Epoch 24/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.7615 - accuracy: 0.2992 - val_loss: 1.6780 - val_accuracy: 0.3420\n",
      "Epoch 25/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.7563 - accuracy: 0.3011 - val_loss: 1.6713 - val_accuracy: 0.3337\n",
      "Epoch 26/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.7516 - accuracy: 0.3026 - val_loss: 1.7201 - val_accuracy: 0.3279\n",
      "Epoch 27/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.7499 - accuracy: 0.3023 - val_loss: 1.7881 - val_accuracy: 0.2867\n",
      "Epoch 28/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.7507 - accuracy: 0.3022 - val_loss: 1.7388 - val_accuracy: 0.3194\n",
      "Epoch 29/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.7457 - accuracy: 0.3038 - val_loss: 1.7354 - val_accuracy: 0.2874\n",
      "Epoch 30/500\n",
      "225/225 [==============================] - 16s 71ms/step - loss: 1.7323 - accuracy: 0.3115 - val_loss: 1.7053 - val_accuracy: 0.3151\n",
      "Epoch 31/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.7320 - accuracy: 0.3141 - val_loss: 1.6547 - val_accuracy: 0.3519\n",
      "Epoch 32/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.7289 - accuracy: 0.3136 - val_loss: 2.0066 - val_accuracy: 0.2813\n",
      "Epoch 33/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.7267 - accuracy: 0.3137 - val_loss: 1.7316 - val_accuracy: 0.3157\n",
      "Epoch 34/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.7215 - accuracy: 0.3122 - val_loss: 1.6807 - val_accuracy: 0.3228\n",
      "Epoch 35/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.7176 - accuracy: 0.3166 - val_loss: 1.7105 - val_accuracy: 0.3059\n",
      "Epoch 36/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.7207 - accuracy: 0.3186 - val_loss: 1.8064 - val_accuracy: 0.3221\n",
      "Epoch 37/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.7091 - accuracy: 0.3206 - val_loss: 1.6493 - val_accuracy: 0.3631\n",
      "Epoch 38/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.7094 - accuracy: 0.3199 - val_loss: 1.6677 - val_accuracy: 0.3426\n",
      "Epoch 39/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.7052 - accuracy: 0.3184 - val_loss: 1.6884 - val_accuracy: 0.3318\n",
      "Epoch 40/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.7060 - accuracy: 0.3224 - val_loss: 1.6537 - val_accuracy: 0.3494\n",
      "Epoch 41/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.7046 - accuracy: 0.3232 - val_loss: 1.6460 - val_accuracy: 0.3557\n",
      "Epoch 42/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.7030 - accuracy: 0.3230 - val_loss: 1.7051 - val_accuracy: 0.3210\n",
      "Epoch 43/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.7006 - accuracy: 0.3221 - val_loss: 1.7988 - val_accuracy: 0.2930\n",
      "Epoch 44/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.6944 - accuracy: 0.3252 - val_loss: 1.8079 - val_accuracy: 0.2095\n",
      "Epoch 45/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.6960 - accuracy: 0.3251 - val_loss: 1.7513 - val_accuracy: 0.2945\n",
      "Epoch 46/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.6924 - accuracy: 0.3276 - val_loss: 1.6651 - val_accuracy: 0.3426\n",
      "Epoch 47/500\n",
      "225/225 [==============================] - 16s 71ms/step - loss: 1.6940 - accuracy: 0.3280 - val_loss: 1.6337 - val_accuracy: 0.3500\n",
      "Epoch 48/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.6910 - accuracy: 0.3272 - val_loss: 1.6590 - val_accuracy: 0.3377\n",
      "Epoch 49/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.6902 - accuracy: 0.3310 - val_loss: 1.6551 - val_accuracy: 0.3423\n",
      "Epoch 50/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.6884 - accuracy: 0.3300 - val_loss: 1.6759 - val_accuracy: 0.3399\n",
      "Epoch 51/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.6866 - accuracy: 0.3299 - val_loss: 1.6189 - val_accuracy: 0.3667\n",
      "Epoch 52/500\n",
      "225/225 [==============================] - 16s 71ms/step - loss: 1.6845 - accuracy: 0.3332 - val_loss: 1.8734 - val_accuracy: 0.2767\n",
      "Epoch 53/500\n",
      "225/225 [==============================] - 16s 71ms/step - loss: 1.6827 - accuracy: 0.3317 - val_loss: 1.6860 - val_accuracy: 0.3391\n",
      "Epoch 54/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6828 - accuracy: 0.3343 - val_loss: 1.6151 - val_accuracy: 0.3631\n",
      "Epoch 55/500\n",
      "225/225 [==============================] - 16s 71ms/step - loss: 1.6846 - accuracy: 0.3286 - val_loss: 1.7510 - val_accuracy: 0.3057\n",
      "Epoch 56/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.6796 - accuracy: 0.3341 - val_loss: 1.6251 - val_accuracy: 0.3571\n",
      "Epoch 57/500\n",
      "225/225 [==============================] - 16s 71ms/step - loss: 1.6792 - accuracy: 0.3312 - val_loss: 1.7856 - val_accuracy: 0.3310\n",
      "Epoch 58/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.6738 - accuracy: 0.3378 - val_loss: 1.6140 - val_accuracy: 0.3615\n",
      "Epoch 59/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.6800 - accuracy: 0.3334 - val_loss: 1.6444 - val_accuracy: 0.3504\n",
      "Epoch 60/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.6788 - accuracy: 0.3337 - val_loss: 1.6439 - val_accuracy: 0.3610\n",
      "Epoch 61/500\n",
      "225/225 [==============================] - 16s 72ms/step - loss: 1.6746 - accuracy: 0.3347 - val_loss: 1.6246 - val_accuracy: 0.3764\n",
      "Epoch 62/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.6746 - accuracy: 0.3360 - val_loss: 1.7517 - val_accuracy: 0.3086\n",
      "Epoch 63/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.6744 - accuracy: 0.3363 - val_loss: 1.6678 - val_accuracy: 0.3448\n",
      "Epoch 64/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.6775 - accuracy: 0.3338 - val_loss: 1.8540 - val_accuracy: 0.2870\n",
      "Epoch 65/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.6703 - accuracy: 0.3394 - val_loss: 1.8372 - val_accuracy: 0.2470\n",
      "Epoch 66/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.6699 - accuracy: 0.3382 - val_loss: 1.8565 - val_accuracy: 0.2421\n",
      "Epoch 67/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.6719 - accuracy: 0.3369 - val_loss: 1.7115 - val_accuracy: 0.3136\n",
      "Epoch 68/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.6743 - accuracy: 0.3372 - val_loss: 1.6271 - val_accuracy: 0.3551\n",
      "Epoch 69/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.6723 - accuracy: 0.3373 - val_loss: 1.7493 - val_accuracy: 0.3235\n",
      "Epoch 70/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.6686 - accuracy: 0.3367 - val_loss: 1.6860 - val_accuracy: 0.3313\n",
      "Epoch 71/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.6632 - accuracy: 0.3410 - val_loss: 1.6530 - val_accuracy: 0.3537\n",
      "Epoch 72/500\n",
      "225/225 [==============================] - 16s 71ms/step - loss: 1.6641 - accuracy: 0.3375 - val_loss: 1.6178 - val_accuracy: 0.3731\n",
      "Epoch 73/500\n",
      "225/225 [==============================] - 16s 73ms/step - loss: 1.6679 - accuracy: 0.3373 - val_loss: 1.6588 - val_accuracy: 0.3427\n",
      "Epoch 74/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.6635 - accuracy: 0.3425 - val_loss: 1.7116 - val_accuracy: 0.3403\n",
      "Epoch 75/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.6603 - accuracy: 0.3438 - val_loss: 1.6135 - val_accuracy: 0.3702\n",
      "Epoch 76/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.6671 - accuracy: 0.3361 - val_loss: 1.9959 - val_accuracy: 0.2750\n",
      "Epoch 77/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.6637 - accuracy: 0.3377 - val_loss: 1.6404 - val_accuracy: 0.3593\n",
      "Epoch 78/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.6629 - accuracy: 0.3433 - val_loss: 1.6811 - val_accuracy: 0.3192\n",
      "Epoch 79/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6609 - accuracy: 0.3427 - val_loss: 1.6488 - val_accuracy: 0.3592\n",
      "Epoch 80/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6600 - accuracy: 0.3444 - val_loss: 1.8176 - val_accuracy: 0.3208\n",
      "Epoch 81/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.6624 - accuracy: 0.3431 - val_loss: 1.6136 - val_accuracy: 0.3693\n",
      "Epoch 82/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.6627 - accuracy: 0.3424 - val_loss: 1.6654 - val_accuracy: 0.3454\n",
      "Epoch 83/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.6601 - accuracy: 0.3419 - val_loss: 1.6717 - val_accuracy: 0.3183\n",
      "Epoch 84/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6593 - accuracy: 0.3420 - val_loss: 1.6887 - val_accuracy: 0.3426\n",
      "Epoch 85/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.6593 - accuracy: 0.3447 - val_loss: 1.6770 - val_accuracy: 0.3491\n",
      "Epoch 86/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.6555 - accuracy: 0.3456 - val_loss: 1.6352 - val_accuracy: 0.3530\n",
      "Epoch 87/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.6568 - accuracy: 0.3437 - val_loss: 1.6012 - val_accuracy: 0.3695\n",
      "Epoch 88/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.6626 - accuracy: 0.3398 - val_loss: 1.6118 - val_accuracy: 0.3561\n",
      "Epoch 89/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.6589 - accuracy: 0.3433 - val_loss: 1.6011 - val_accuracy: 0.3704\n",
      "Epoch 90/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.6552 - accuracy: 0.3474 - val_loss: 1.6404 - val_accuracy: 0.3572\n",
      "Epoch 91/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.6552 - accuracy: 0.3429 - val_loss: 1.6605 - val_accuracy: 0.3394\n",
      "Epoch 92/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6563 - accuracy: 0.3455 - val_loss: 2.1006 - val_accuracy: 0.2345\n",
      "Epoch 93/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.6553 - accuracy: 0.3463 - val_loss: 1.7322 - val_accuracy: 0.3359\n",
      "Epoch 94/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.6581 - accuracy: 0.3417 - val_loss: 1.6078 - val_accuracy: 0.3626\n",
      "Epoch 95/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.6579 - accuracy: 0.3443 - val_loss: 1.8412 - val_accuracy: 0.2999\n",
      "Epoch 96/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.6570 - accuracy: 0.3431 - val_loss: 1.9156 - val_accuracy: 0.3140\n",
      "Epoch 97/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6561 - accuracy: 0.3428 - val_loss: 1.6363 - val_accuracy: 0.3509\n",
      "Epoch 98/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.6567 - accuracy: 0.3426 - val_loss: 1.7227 - val_accuracy: 0.3144\n",
      "Epoch 99/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.6558 - accuracy: 0.3466 - val_loss: 1.7445 - val_accuracy: 0.3057\n",
      "Epoch 100/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.6518 - accuracy: 0.3446 - val_loss: 1.5918 - val_accuracy: 0.3722\n",
      "Epoch 101/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.6519 - accuracy: 0.3460 - val_loss: 1.7087 - val_accuracy: 0.3390\n",
      "Epoch 102/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.6529 - accuracy: 0.3447 - val_loss: 1.6143 - val_accuracy: 0.3596\n",
      "Epoch 103/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6514 - accuracy: 0.3471 - val_loss: 1.6303 - val_accuracy: 0.3543\n",
      "Epoch 104/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.6499 - accuracy: 0.3450 - val_loss: 1.6166 - val_accuracy: 0.3750\n",
      "Epoch 105/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6517 - accuracy: 0.3465 - val_loss: 1.6875 - val_accuracy: 0.3440\n",
      "Epoch 106/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.6486 - accuracy: 0.3466 - val_loss: 1.6984 - val_accuracy: 0.3344\n",
      "Epoch 107/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.6514 - accuracy: 0.3461 - val_loss: 1.6448 - val_accuracy: 0.3554\n",
      "Epoch 108/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6495 - accuracy: 0.3509 - val_loss: 1.8446 - val_accuracy: 0.2559\n",
      "Epoch 109/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.6535 - accuracy: 0.3452 - val_loss: 1.6437 - val_accuracy: 0.3527\n",
      "Epoch 110/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.6446 - accuracy: 0.3512 - val_loss: 1.7368 - val_accuracy: 0.3408\n",
      "Epoch 111/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.6496 - accuracy: 0.3510 - val_loss: 1.7010 - val_accuracy: 0.3130\n",
      "Epoch 112/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.6493 - accuracy: 0.3460 - val_loss: 1.5588 - val_accuracy: 0.3852\n",
      "Epoch 113/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225/225 [==============================] - 16s 70ms/step - loss: 1.6496 - accuracy: 0.3483 - val_loss: 1.6906 - val_accuracy: 0.3259\n",
      "Epoch 114/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.6464 - accuracy: 0.3493 - val_loss: 1.8042 - val_accuracy: 0.3072\n",
      "Epoch 115/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.6497 - accuracy: 0.3464 - val_loss: 1.8583 - val_accuracy: 0.3172\n",
      "Epoch 116/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.6471 - accuracy: 0.3477 - val_loss: 1.6771 - val_accuracy: 0.3151\n",
      "Epoch 117/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.6456 - accuracy: 0.3476 - val_loss: 1.6866 - val_accuracy: 0.3253\n",
      "Epoch 118/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.6486 - accuracy: 0.3476 - val_loss: 1.6058 - val_accuracy: 0.3798\n",
      "Epoch 119/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.6466 - accuracy: 0.3463 - val_loss: 1.6851 - val_accuracy: 0.3256\n",
      "Epoch 120/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.6455 - accuracy: 0.3509 - val_loss: 1.6379 - val_accuracy: 0.3387\n",
      "Epoch 121/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.6415 - accuracy: 0.3505 - val_loss: 1.6015 - val_accuracy: 0.3749\n",
      "Epoch 122/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.6435 - accuracy: 0.3508 - val_loss: 1.6489 - val_accuracy: 0.3402\n",
      "Epoch 123/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.6462 - accuracy: 0.3499 - val_loss: 1.6637 - val_accuracy: 0.3505\n",
      "Epoch 124/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6459 - accuracy: 0.3520 - val_loss: 1.6138 - val_accuracy: 0.3579\n",
      "Epoch 125/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.6425 - accuracy: 0.3503 - val_loss: 1.7768 - val_accuracy: 0.2969\n",
      "Epoch 126/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.6442 - accuracy: 0.3531 - val_loss: 1.7293 - val_accuracy: 0.2896\n",
      "Epoch 127/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.6407 - accuracy: 0.3494 - val_loss: 1.6090 - val_accuracy: 0.3734\n",
      "Epoch 128/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.6415 - accuracy: 0.3508 - val_loss: 1.6732 - val_accuracy: 0.3412\n",
      "Epoch 129/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.6461 - accuracy: 0.3509 - val_loss: 1.6929 - val_accuracy: 0.3229\n",
      "Epoch 130/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.6412 - accuracy: 0.3525 - val_loss: 1.5997 - val_accuracy: 0.3780\n",
      "Epoch 131/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6468 - accuracy: 0.3473 - val_loss: 2.0068 - val_accuracy: 0.2177\n",
      "Epoch 132/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.6419 - accuracy: 0.3520 - val_loss: 1.6856 - val_accuracy: 0.3247\n",
      "Epoch 133/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.6443 - accuracy: 0.3529 - val_loss: 1.6532 - val_accuracy: 0.3395\n",
      "Epoch 134/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.6413 - accuracy: 0.3537 - val_loss: 1.8879 - val_accuracy: 0.2349\n",
      "Epoch 135/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.6408 - accuracy: 0.3502 - val_loss: 1.6101 - val_accuracy: 0.3700\n",
      "Epoch 136/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.6345 - accuracy: 0.3543 - val_loss: 1.7484 - val_accuracy: 0.3093\n",
      "Epoch 137/500\n",
      "225/225 [==============================] - 16s 71ms/step - loss: 1.6407 - accuracy: 0.3511 - val_loss: 1.5976 - val_accuracy: 0.3837\n",
      "Epoch 138/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.6389 - accuracy: 0.3518 - val_loss: 1.6714 - val_accuracy: 0.3417\n",
      "Epoch 139/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.6407 - accuracy: 0.3510 - val_loss: 1.6266 - val_accuracy: 0.3626\n",
      "Epoch 140/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.6362 - accuracy: 0.3560 - val_loss: 1.6476 - val_accuracy: 0.3736\n",
      "Epoch 141/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.6381 - accuracy: 0.3519 - val_loss: 1.7297 - val_accuracy: 0.3091\n",
      "Epoch 142/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.6406 - accuracy: 0.3522 - val_loss: 1.7670 - val_accuracy: 0.2959\n",
      "Epoch 143/500\n",
      "225/225 [==============================] - 16s 71ms/step - loss: 1.6411 - accuracy: 0.3518 - val_loss: 1.8403 - val_accuracy: 0.3129\n",
      "Epoch 144/500\n",
      "225/225 [==============================] - 16s 71ms/step - loss: 1.6344 - accuracy: 0.3586 - val_loss: 1.6872 - val_accuracy: 0.3573\n",
      "Epoch 145/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.6382 - accuracy: 0.3537 - val_loss: 1.6961 - val_accuracy: 0.3378\n",
      "Epoch 146/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6353 - accuracy: 0.3536 - val_loss: 1.6141 - val_accuracy: 0.3633\n",
      "Epoch 147/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6388 - accuracy: 0.3481 - val_loss: 1.6668 - val_accuracy: 0.3480\n",
      "Epoch 148/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.6331 - accuracy: 0.3553 - val_loss: 1.7980 - val_accuracy: 0.2689\n",
      "Epoch 149/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6337 - accuracy: 0.3545 - val_loss: 1.6109 - val_accuracy: 0.3569\n",
      "Epoch 150/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.6362 - accuracy: 0.3535 - val_loss: 1.6475 - val_accuracy: 0.3494\n",
      "Epoch 151/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.6411 - accuracy: 0.3520 - val_loss: 1.5848 - val_accuracy: 0.3781\n",
      "Epoch 152/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.6374 - accuracy: 0.3544 - val_loss: 1.7717 - val_accuracy: 0.3217\n",
      "Epoch 153/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.6324 - accuracy: 0.3564 - val_loss: 1.6173 - val_accuracy: 0.3585\n",
      "Epoch 154/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.6308 - accuracy: 0.3598 - val_loss: 1.6909 - val_accuracy: 0.3398\n",
      "Epoch 155/500\n",
      "225/225 [==============================] - 16s 71ms/step - loss: 1.6373 - accuracy: 0.3549 - val_loss: 1.6961 - val_accuracy: 0.3370\n",
      "Epoch 156/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.6327 - accuracy: 0.3554 - val_loss: 1.6014 - val_accuracy: 0.3756\n",
      "Epoch 157/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.6354 - accuracy: 0.3532 - val_loss: 1.6159 - val_accuracy: 0.3646\n",
      "Epoch 158/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.6333 - accuracy: 0.3587 - val_loss: 2.3458 - val_accuracy: 0.1684\n",
      "Epoch 159/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.6379 - accuracy: 0.3540 - val_loss: 1.7715 - val_accuracy: 0.3218\n",
      "Epoch 160/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.6310 - accuracy: 0.3581 - val_loss: 1.6066 - val_accuracy: 0.3690\n",
      "Epoch 161/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6360 - accuracy: 0.3543 - val_loss: 1.6025 - val_accuracy: 0.3717\n",
      "Epoch 162/500\n",
      "225/225 [==============================] - 16s 71ms/step - loss: 1.6314 - accuracy: 0.3565 - val_loss: 1.6428 - val_accuracy: 0.3600\n",
      "Epoch 163/500\n",
      "225/225 [==============================] - 16s 71ms/step - loss: 1.6322 - accuracy: 0.3544 - val_loss: 1.5579 - val_accuracy: 0.3838\n",
      "Epoch 164/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.6316 - accuracy: 0.3551 - val_loss: 1.9057 - val_accuracy: 0.3193\n",
      "Epoch 165/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.6383 - accuracy: 0.3528 - val_loss: 1.6675 - val_accuracy: 0.3197\n",
      "Epoch 166/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6301 - accuracy: 0.3563 - val_loss: 1.7550 - val_accuracy: 0.3381\n",
      "Epoch 167/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.6295 - accuracy: 0.3538 - val_loss: 1.6067 - val_accuracy: 0.3745\n",
      "Epoch 168/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.6313 - accuracy: 0.3585 - val_loss: 1.8200 - val_accuracy: 0.3034\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 169/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6305 - accuracy: 0.3582 - val_loss: 1.8288 - val_accuracy: 0.2984\n",
      "Epoch 170/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6308 - accuracy: 0.3587 - val_loss: 1.5929 - val_accuracy: 0.3757\n",
      "Epoch 171/500\n",
      "225/225 [==============================] - 15s 69ms/step - loss: 1.6318 - accuracy: 0.3560 - val_loss: 2.0131 - val_accuracy: 0.2207\n",
      "Epoch 172/500\n",
      "225/225 [==============================] - 15s 69ms/step - loss: 1.6288 - accuracy: 0.3563 - val_loss: 1.6502 - val_accuracy: 0.3405\n",
      "Epoch 173/500\n",
      "225/225 [==============================] - 15s 69ms/step - loss: 1.6294 - accuracy: 0.3569 - val_loss: 1.5889 - val_accuracy: 0.3791\n",
      "Epoch 174/500\n",
      "225/225 [==============================] - 15s 69ms/step - loss: 1.6316 - accuracy: 0.3565 - val_loss: 1.7046 - val_accuracy: 0.3275\n",
      "Epoch 175/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6291 - accuracy: 0.3571 - val_loss: 1.7187 - val_accuracy: 0.3207\n",
      "Epoch 176/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6273 - accuracy: 0.3604 - val_loss: 1.6943 - val_accuracy: 0.3416\n",
      "Epoch 177/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6291 - accuracy: 0.3562 - val_loss: 1.7638 - val_accuracy: 0.3186\n",
      "Epoch 178/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6250 - accuracy: 0.3630 - val_loss: 1.7465 - val_accuracy: 0.3257\n",
      "Epoch 179/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6331 - accuracy: 0.3567 - val_loss: 1.6460 - val_accuracy: 0.3579\n",
      "Epoch 180/500\n",
      "225/225 [==============================] - 15s 69ms/step - loss: 1.6291 - accuracy: 0.3588 - val_loss: 1.8245 - val_accuracy: 0.3083\n",
      "Epoch 181/500\n",
      "225/225 [==============================] - 15s 69ms/step - loss: 1.6240 - accuracy: 0.3643 - val_loss: 1.7274 - val_accuracy: 0.3270\n",
      "Epoch 182/500\n",
      "225/225 [==============================] - 15s 69ms/step - loss: 1.6255 - accuracy: 0.3584 - val_loss: 1.5686 - val_accuracy: 0.3884\n",
      "Epoch 183/500\n",
      "225/225 [==============================] - 15s 69ms/step - loss: 1.6310 - accuracy: 0.3552 - val_loss: 1.7663 - val_accuracy: 0.3077\n",
      "Epoch 184/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6276 - accuracy: 0.3581 - val_loss: 1.7262 - val_accuracy: 0.3231\n",
      "Epoch 185/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6264 - accuracy: 0.3585 - val_loss: 1.5635 - val_accuracy: 0.3809\n",
      "Epoch 186/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.6312 - accuracy: 0.3593 - val_loss: 1.6670 - val_accuracy: 0.3334\n",
      "Epoch 187/500\n",
      "225/225 [==============================] - 16s 72ms/step - loss: 1.6279 - accuracy: 0.3615 - val_loss: 1.7000 - val_accuracy: 0.3442\n",
      "Epoch 188/500\n",
      "225/225 [==============================] - 30s 135ms/step - loss: 1.6252 - accuracy: 0.3606 - val_loss: 1.6754 - val_accuracy: 0.3501\n",
      "Epoch 189/500\n",
      "225/225 [==============================] - 26s 116ms/step - loss: 1.6235 - accuracy: 0.3595 - val_loss: 1.5810 - val_accuracy: 0.3795\n",
      "Epoch 190/500\n",
      "225/225 [==============================] - 28s 125ms/step - loss: 1.6241 - accuracy: 0.3616 - val_loss: 1.7305 - val_accuracy: 0.3254\n",
      "Epoch 191/500\n",
      "225/225 [==============================] - 26s 115ms/step - loss: 1.6237 - accuracy: 0.3593 - val_loss: 1.7234 - val_accuracy: 0.3139\n",
      "Epoch 192/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6254 - accuracy: 0.3611 - val_loss: 1.6884 - val_accuracy: 0.3476\n",
      "Epoch 193/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6251 - accuracy: 0.3589 - val_loss: 1.7817 - val_accuracy: 0.3172\n",
      "Epoch 194/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6254 - accuracy: 0.3607 - val_loss: 1.6219 - val_accuracy: 0.3759\n",
      "Epoch 195/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6279 - accuracy: 0.3571 - val_loss: 1.5887 - val_accuracy: 0.3738\n",
      "Epoch 196/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6255 - accuracy: 0.3595 - val_loss: 1.6133 - val_accuracy: 0.3582\n",
      "Epoch 197/500\n",
      "225/225 [==============================] - 15s 69ms/step - loss: 1.6240 - accuracy: 0.3570 - val_loss: 1.7506 - val_accuracy: 0.3572\n",
      "Epoch 198/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6203 - accuracy: 0.3592 - val_loss: 2.2018 - val_accuracy: 0.2933\n",
      "Epoch 199/500\n",
      "225/225 [==============================] - 15s 69ms/step - loss: 1.6255 - accuracy: 0.3564 - val_loss: 1.6622 - val_accuracy: 0.3348\n",
      "Epoch 200/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6214 - accuracy: 0.3598 - val_loss: 1.5761 - val_accuracy: 0.3866\n",
      "Epoch 201/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6224 - accuracy: 0.3650 - val_loss: 1.6987 - val_accuracy: 0.3228\n",
      "Epoch 202/500\n",
      "225/225 [==============================] - 15s 69ms/step - loss: 1.6208 - accuracy: 0.3607 - val_loss: 1.6984 - val_accuracy: 0.3362\n",
      "Epoch 203/500\n",
      "225/225 [==============================] - 15s 69ms/step - loss: 1.6308 - accuracy: 0.3555 - val_loss: 1.6342 - val_accuracy: 0.3487\n",
      "Epoch 204/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6259 - accuracy: 0.3585 - val_loss: 1.7906 - val_accuracy: 0.2751\n",
      "Epoch 205/500\n",
      "225/225 [==============================] - 15s 69ms/step - loss: 1.6292 - accuracy: 0.3594 - val_loss: 1.7053 - val_accuracy: 0.3367\n",
      "Epoch 206/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6273 - accuracy: 0.3585 - val_loss: 1.6036 - val_accuracy: 0.3810\n",
      "Epoch 207/500\n",
      "225/225 [==============================] - 15s 69ms/step - loss: 1.6217 - accuracy: 0.3613 - val_loss: 1.5964 - val_accuracy: 0.3668\n",
      "Epoch 208/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.6271 - accuracy: 0.3578 - val_loss: 1.6193 - val_accuracy: 0.3650\n",
      "Epoch 209/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6265 - accuracy: 0.3576 - val_loss: 1.7148 - val_accuracy: 0.3137\n",
      "Epoch 210/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6230 - accuracy: 0.3611 - val_loss: 1.6421 - val_accuracy: 0.3551\n",
      "Epoch 211/500\n",
      "225/225 [==============================] - 15s 68ms/step - loss: 1.6196 - accuracy: 0.3636 - val_loss: 1.6121 - val_accuracy: 0.3686\n",
      "Epoch 212/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6181 - accuracy: 0.3642 - val_loss: 1.7302 - val_accuracy: 0.2913\n",
      "Epoch 213/500\n",
      "225/225 [==============================] - 15s 69ms/step - loss: 1.6187 - accuracy: 0.3626 - val_loss: 1.5631 - val_accuracy: 0.3926\n",
      "Epoch 214/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6238 - accuracy: 0.3607 - val_loss: 1.6979 - val_accuracy: 0.3243\n",
      "Epoch 215/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6251 - accuracy: 0.3560 - val_loss: 1.9208 - val_accuracy: 0.3079\n",
      "Epoch 216/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6273 - accuracy: 0.3596 - val_loss: 1.5985 - val_accuracy: 0.3753\n",
      "Epoch 217/500\n",
      "225/225 [==============================] - 15s 69ms/step - loss: 1.6195 - accuracy: 0.3601 - val_loss: 1.6525 - val_accuracy: 0.3629\n",
      "Epoch 218/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6150 - accuracy: 0.3631 - val_loss: 1.7262 - val_accuracy: 0.3022\n",
      "Epoch 219/500\n",
      "225/225 [==============================] - 15s 69ms/step - loss: 1.6158 - accuracy: 0.3645 - val_loss: 1.5829 - val_accuracy: 0.3845\n",
      "Epoch 220/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6194 - accuracy: 0.3594 - val_loss: 1.6226 - val_accuracy: 0.3586\n",
      "Epoch 221/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6214 - accuracy: 0.3634 - val_loss: 1.8095 - val_accuracy: 0.3481\n",
      "Epoch 222/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.6237 - accuracy: 0.3589 - val_loss: 1.6698 - val_accuracy: 0.3412\n",
      "Epoch 223/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6206 - accuracy: 0.3627 - val_loss: 1.6327 - val_accuracy: 0.3564\n",
      "Epoch 224/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6179 - accuracy: 0.3631 - val_loss: 1.6093 - val_accuracy: 0.3632\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 225/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6172 - accuracy: 0.3612 - val_loss: 1.5463 - val_accuracy: 0.4004\n",
      "Epoch 226/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6207 - accuracy: 0.3589 - val_loss: 1.6442 - val_accuracy: 0.3447\n",
      "Epoch 227/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6152 - accuracy: 0.3648 - val_loss: 1.6224 - val_accuracy: 0.3661\n",
      "Epoch 228/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6211 - accuracy: 0.3612 - val_loss: 1.6703 - val_accuracy: 0.3392\n",
      "Epoch 229/500\n",
      "225/225 [==============================] - 15s 69ms/step - loss: 1.6159 - accuracy: 0.3632 - val_loss: 1.5586 - val_accuracy: 0.3891\n",
      "Epoch 230/500\n",
      "225/225 [==============================] - 15s 69ms/step - loss: 1.6144 - accuracy: 0.3690 - val_loss: 1.5974 - val_accuracy: 0.3618\n",
      "Epoch 231/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6202 - accuracy: 0.3606 - val_loss: 1.6551 - val_accuracy: 0.3771\n",
      "Epoch 232/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6173 - accuracy: 0.3639 - val_loss: 1.5977 - val_accuracy: 0.3683\n",
      "Epoch 233/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6100 - accuracy: 0.3669 - val_loss: 1.6207 - val_accuracy: 0.3514\n",
      "Epoch 234/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6194 - accuracy: 0.3655 - val_loss: 1.6368 - val_accuracy: 0.3728\n",
      "Epoch 235/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6122 - accuracy: 0.3675 - val_loss: 1.6331 - val_accuracy: 0.3573\n",
      "Epoch 236/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6130 - accuracy: 0.3645 - val_loss: 1.6327 - val_accuracy: 0.3548\n",
      "Epoch 237/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6166 - accuracy: 0.3642 - val_loss: 1.5859 - val_accuracy: 0.3826\n",
      "Epoch 238/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6179 - accuracy: 0.3655 - val_loss: 1.5722 - val_accuracy: 0.3819\n",
      "Epoch 239/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6210 - accuracy: 0.3596 - val_loss: 1.6061 - val_accuracy: 0.3587\n",
      "Epoch 240/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6112 - accuracy: 0.3663 - val_loss: 1.5921 - val_accuracy: 0.3704\n",
      "Epoch 241/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6125 - accuracy: 0.3667 - val_loss: 1.6955 - val_accuracy: 0.3452\n",
      "Epoch 242/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6146 - accuracy: 0.3636 - val_loss: 1.6001 - val_accuracy: 0.3670\n",
      "Epoch 243/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6171 - accuracy: 0.3646 - val_loss: 1.6368 - val_accuracy: 0.3600\n",
      "Epoch 244/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6178 - accuracy: 0.3633 - val_loss: 1.6649 - val_accuracy: 0.3422\n",
      "Epoch 245/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6175 - accuracy: 0.3610 - val_loss: 1.6258 - val_accuracy: 0.3533\n",
      "Epoch 246/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6168 - accuracy: 0.3667 - val_loss: 1.6384 - val_accuracy: 0.3639\n",
      "Epoch 247/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6120 - accuracy: 0.3676 - val_loss: 1.8283 - val_accuracy: 0.3015\n",
      "Epoch 248/500\n",
      "225/225 [==============================] - 15s 69ms/step - loss: 1.6183 - accuracy: 0.3615 - val_loss: 1.5765 - val_accuracy: 0.3859\n",
      "Epoch 249/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6171 - accuracy: 0.3634 - val_loss: 1.6027 - val_accuracy: 0.3631\n",
      "Epoch 250/500\n",
      "225/225 [==============================] - 15s 68ms/step - loss: 1.6147 - accuracy: 0.3637 - val_loss: 1.6181 - val_accuracy: 0.3569\n",
      "Epoch 251/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6185 - accuracy: 0.3635 - val_loss: 1.7276 - val_accuracy: 0.3068\n",
      "Epoch 252/500\n",
      "225/225 [==============================] - 15s 69ms/step - loss: 1.6142 - accuracy: 0.3654 - val_loss: 1.8277 - val_accuracy: 0.2842\n",
      "Epoch 253/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6161 - accuracy: 0.3638 - val_loss: 1.6528 - val_accuracy: 0.3420\n",
      "Epoch 254/500\n",
      "225/225 [==============================] - 16s 71ms/step - loss: 1.6115 - accuracy: 0.3672 - val_loss: 1.6323 - val_accuracy: 0.3628\n",
      "Epoch 255/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6195 - accuracy: 0.3646 - val_loss: 1.7847 - val_accuracy: 0.3174\n",
      "Epoch 256/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6133 - accuracy: 0.3620 - val_loss: 1.6565 - val_accuracy: 0.3456\n",
      "Epoch 257/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6120 - accuracy: 0.3642 - val_loss: 1.6990 - val_accuracy: 0.3403\n",
      "Epoch 258/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6136 - accuracy: 0.3640 - val_loss: 1.5543 - val_accuracy: 0.3884\n",
      "Epoch 259/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6114 - accuracy: 0.3644 - val_loss: 1.6623 - val_accuracy: 0.3419\n",
      "Epoch 260/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6119 - accuracy: 0.3638 - val_loss: 1.5839 - val_accuracy: 0.3792\n",
      "Epoch 261/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.6128 - accuracy: 0.3667 - val_loss: 1.5410 - val_accuracy: 0.3866\n",
      "Epoch 262/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6184 - accuracy: 0.3621 - val_loss: 1.5723 - val_accuracy: 0.3801\n",
      "Epoch 263/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.6115 - accuracy: 0.3670 - val_loss: 1.6288 - val_accuracy: 0.3568\n",
      "Epoch 264/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6119 - accuracy: 0.3705 - val_loss: 1.6692 - val_accuracy: 0.3247\n",
      "Epoch 265/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6172 - accuracy: 0.3653 - val_loss: 1.7549 - val_accuracy: 0.3130\n",
      "Epoch 266/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6081 - accuracy: 0.3691 - val_loss: 1.6058 - val_accuracy: 0.3727\n",
      "Epoch 267/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6122 - accuracy: 0.3694 - val_loss: 1.6758 - val_accuracy: 0.3580\n",
      "Epoch 268/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6130 - accuracy: 0.3679 - val_loss: 1.6733 - val_accuracy: 0.3526\n",
      "Epoch 269/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6123 - accuracy: 0.3662 - val_loss: 1.5773 - val_accuracy: 0.3876\n",
      "Epoch 270/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6086 - accuracy: 0.3670 - val_loss: 2.0098 - val_accuracy: 0.2600\n",
      "Epoch 271/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6116 - accuracy: 0.3663 - val_loss: 1.5645 - val_accuracy: 0.3911\n",
      "Epoch 272/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6115 - accuracy: 0.3640 - val_loss: 1.6113 - val_accuracy: 0.3625\n",
      "Epoch 273/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6154 - accuracy: 0.3656 - val_loss: 1.6455 - val_accuracy: 0.3569\n",
      "Epoch 274/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6160 - accuracy: 0.3689 - val_loss: 1.6656 - val_accuracy: 0.3679\n",
      "Epoch 275/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6093 - accuracy: 0.3665 - val_loss: 1.6302 - val_accuracy: 0.3635\n",
      "Epoch 276/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6128 - accuracy: 0.3684 - val_loss: 1.5766 - val_accuracy: 0.3675\n",
      "Epoch 277/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6088 - accuracy: 0.3678 - val_loss: 1.6567 - val_accuracy: 0.3519\n",
      "Epoch 278/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6105 - accuracy: 0.3649 - val_loss: 1.7093 - val_accuracy: 0.3126\n",
      "Epoch 279/500\n",
      "225/225 [==============================] - 15s 69ms/step - loss: 1.6090 - accuracy: 0.3671 - val_loss: 1.6090 - val_accuracy: 0.3718\n",
      "Epoch 280/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6092 - accuracy: 0.3649 - val_loss: 1.5956 - val_accuracy: 0.3844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 281/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6092 - accuracy: 0.3676 - val_loss: 1.7467 - val_accuracy: 0.3013\n",
      "Epoch 282/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6051 - accuracy: 0.3711 - val_loss: 1.8325 - val_accuracy: 0.3122\n",
      "Epoch 283/500\n",
      "225/225 [==============================] - 15s 69ms/step - loss: 1.6089 - accuracy: 0.3688 - val_loss: 1.5579 - val_accuracy: 0.3819\n",
      "Epoch 284/500\n",
      "225/225 [==============================] - 15s 69ms/step - loss: 1.6135 - accuracy: 0.3627 - val_loss: 1.5926 - val_accuracy: 0.3767\n",
      "Epoch 285/500\n",
      "225/225 [==============================] - 15s 69ms/step - loss: 1.6138 - accuracy: 0.3676 - val_loss: 1.7158 - val_accuracy: 0.3214\n",
      "Epoch 286/500\n",
      "225/225 [==============================] - 15s 68ms/step - loss: 1.6112 - accuracy: 0.3679 - val_loss: 1.8422 - val_accuracy: 0.2942\n",
      "Epoch 287/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6065 - accuracy: 0.3674 - val_loss: 1.5863 - val_accuracy: 0.3714\n",
      "Epoch 288/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6049 - accuracy: 0.3672 - val_loss: 1.6490 - val_accuracy: 0.3555\n",
      "Epoch 289/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6058 - accuracy: 0.3682 - val_loss: 1.5380 - val_accuracy: 0.3984\n",
      "Epoch 290/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6068 - accuracy: 0.3683 - val_loss: 1.7506 - val_accuracy: 0.3240\n",
      "Epoch 291/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6085 - accuracy: 0.3661 - val_loss: 1.8192 - val_accuracy: 0.3087\n",
      "Epoch 292/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6093 - accuracy: 0.3671 - val_loss: 1.7539 - val_accuracy: 0.3169\n",
      "Epoch 293/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6091 - accuracy: 0.3662 - val_loss: 1.6396 - val_accuracy: 0.3646\n",
      "Epoch 294/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6063 - accuracy: 0.3742 - val_loss: 1.5752 - val_accuracy: 0.3773\n",
      "Epoch 295/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6114 - accuracy: 0.3693 - val_loss: 1.5798 - val_accuracy: 0.3732\n",
      "Epoch 296/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6081 - accuracy: 0.3658 - val_loss: 1.6276 - val_accuracy: 0.3713\n",
      "Epoch 297/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6084 - accuracy: 0.3664 - val_loss: 1.6644 - val_accuracy: 0.3558\n",
      "Epoch 298/500\n",
      "225/225 [==============================] - 15s 69ms/step - loss: 1.6074 - accuracy: 0.3682 - val_loss: 1.7354 - val_accuracy: 0.2811\n",
      "Epoch 299/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6024 - accuracy: 0.3732 - val_loss: 1.7250 - val_accuracy: 0.3621\n",
      "Epoch 300/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6076 - accuracy: 0.3675 - val_loss: 1.6027 - val_accuracy: 0.3638\n",
      "Epoch 301/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.6119 - accuracy: 0.3653 - val_loss: 1.7261 - val_accuracy: 0.3451\n",
      "Epoch 302/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6043 - accuracy: 0.3725 - val_loss: 1.5360 - val_accuracy: 0.4023\n",
      "Epoch 303/500\n",
      "225/225 [==============================] - 15s 69ms/step - loss: 1.6082 - accuracy: 0.3694 - val_loss: 1.5727 - val_accuracy: 0.3753\n",
      "Epoch 304/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6060 - accuracy: 0.3692 - val_loss: 1.8962 - val_accuracy: 0.3016\n",
      "Epoch 305/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6097 - accuracy: 0.3625 - val_loss: 1.7133 - val_accuracy: 0.3732\n",
      "Epoch 306/500\n",
      "225/225 [==============================] - 15s 68ms/step - loss: 1.6073 - accuracy: 0.3665 - val_loss: 1.8789 - val_accuracy: 0.3239\n",
      "Epoch 307/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6057 - accuracy: 0.3710 - val_loss: 1.7100 - val_accuracy: 0.3409\n",
      "Epoch 308/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6091 - accuracy: 0.3646 - val_loss: 1.6401 - val_accuracy: 0.3571\n",
      "Epoch 309/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6058 - accuracy: 0.3672 - val_loss: 1.6194 - val_accuracy: 0.3601\n",
      "Epoch 310/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6071 - accuracy: 0.3678 - val_loss: 1.7185 - val_accuracy: 0.3238\n",
      "Epoch 311/500\n",
      "225/225 [==============================] - 15s 69ms/step - loss: 1.6056 - accuracy: 0.3700 - val_loss: 1.6850 - val_accuracy: 0.3218\n",
      "Epoch 312/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6071 - accuracy: 0.3713 - val_loss: 1.6662 - val_accuracy: 0.3631\n",
      "Epoch 313/500\n",
      "225/225 [==============================] - 15s 69ms/step - loss: 1.6059 - accuracy: 0.3680 - val_loss: 1.5285 - val_accuracy: 0.4069\n",
      "Epoch 314/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6079 - accuracy: 0.3657 - val_loss: 1.6157 - val_accuracy: 0.3670\n",
      "Epoch 315/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6067 - accuracy: 0.3692 - val_loss: 1.8578 - val_accuracy: 0.3073\n",
      "Epoch 316/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6082 - accuracy: 0.3662 - val_loss: 1.7033 - val_accuracy: 0.3438\n",
      "Epoch 317/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.6062 - accuracy: 0.3664 - val_loss: 1.7074 - val_accuracy: 0.3330\n",
      "Epoch 318/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6080 - accuracy: 0.3681 - val_loss: 1.6232 - val_accuracy: 0.3635\n",
      "Epoch 319/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6077 - accuracy: 0.3671 - val_loss: 1.6035 - val_accuracy: 0.3685\n",
      "Epoch 320/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6012 - accuracy: 0.3718 - val_loss: 1.8398 - val_accuracy: 0.3275\n",
      "Epoch 321/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6082 - accuracy: 0.3712 - val_loss: 1.8069 - val_accuracy: 0.2765\n",
      "Epoch 322/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6062 - accuracy: 0.3673 - val_loss: 1.9561 - val_accuracy: 0.2824\n",
      "Epoch 323/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6058 - accuracy: 0.3655 - val_loss: 1.6156 - val_accuracy: 0.3670\n",
      "Epoch 324/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6111 - accuracy: 0.3693 - val_loss: 1.7177 - val_accuracy: 0.3437\n",
      "Epoch 325/500\n",
      "225/225 [==============================] - 15s 69ms/step - loss: 1.6113 - accuracy: 0.3629 - val_loss: 1.5973 - val_accuracy: 0.3752\n",
      "Epoch 326/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6069 - accuracy: 0.3661 - val_loss: 1.5726 - val_accuracy: 0.3777\n",
      "Epoch 327/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6065 - accuracy: 0.3669 - val_loss: 1.5852 - val_accuracy: 0.3773\n",
      "Epoch 328/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6016 - accuracy: 0.3710 - val_loss: 1.5746 - val_accuracy: 0.3735\n",
      "Epoch 329/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.5991 - accuracy: 0.3680 - val_loss: 1.5829 - val_accuracy: 0.3872\n",
      "Epoch 330/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6018 - accuracy: 0.3707 - val_loss: 1.6053 - val_accuracy: 0.3716\n",
      "Epoch 331/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6000 - accuracy: 0.3690 - val_loss: 1.5853 - val_accuracy: 0.3738\n",
      "Epoch 332/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.6043 - accuracy: 0.3671 - val_loss: 1.7440 - val_accuracy: 0.3429\n",
      "Epoch 333/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6061 - accuracy: 0.3667 - val_loss: 1.7144 - val_accuracy: 0.3801\n",
      "Epoch 334/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6036 - accuracy: 0.3714 - val_loss: 1.6676 - val_accuracy: 0.3607\n",
      "Epoch 335/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6060 - accuracy: 0.3709 - val_loss: 1.8343 - val_accuracy: 0.3020\n",
      "Epoch 336/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6018 - accuracy: 0.3714 - val_loss: 1.6388 - val_accuracy: 0.3615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 337/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6044 - accuracy: 0.3698 - val_loss: 1.6492 - val_accuracy: 0.3709\n",
      "Epoch 338/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6046 - accuracy: 0.3700 - val_loss: 1.8897 - val_accuracy: 0.2760\n",
      "Epoch 339/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.5988 - accuracy: 0.3774 - val_loss: 1.7007 - val_accuracy: 0.3344\n",
      "Epoch 340/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6054 - accuracy: 0.3688 - val_loss: 1.6040 - val_accuracy: 0.3575\n",
      "Epoch 341/500\n",
      "225/225 [==============================] - 15s 69ms/step - loss: 1.6031 - accuracy: 0.3678 - val_loss: 1.5368 - val_accuracy: 0.4012\n",
      "Epoch 342/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.5991 - accuracy: 0.3725 - val_loss: 1.7447 - val_accuracy: 0.3142\n",
      "Epoch 343/500\n",
      "225/225 [==============================] - 15s 69ms/step - loss: 1.6045 - accuracy: 0.3717 - val_loss: 1.6341 - val_accuracy: 0.3660\n",
      "Epoch 344/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6064 - accuracy: 0.3657 - val_loss: 1.7730 - val_accuracy: 0.3433\n",
      "Epoch 345/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6023 - accuracy: 0.3713 - val_loss: 1.5376 - val_accuracy: 0.3979\n",
      "Epoch 346/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6036 - accuracy: 0.3730 - val_loss: 1.6520 - val_accuracy: 0.3583\n",
      "Epoch 347/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6020 - accuracy: 0.3718 - val_loss: 1.5867 - val_accuracy: 0.3596\n",
      "Epoch 348/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6004 - accuracy: 0.3670 - val_loss: 1.7600 - val_accuracy: 0.3356\n",
      "Epoch 349/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6067 - accuracy: 0.3677 - val_loss: 1.6337 - val_accuracy: 0.3525\n",
      "Epoch 350/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.5986 - accuracy: 0.3714 - val_loss: 1.6885 - val_accuracy: 0.3391\n",
      "Epoch 351/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6033 - accuracy: 0.3713 - val_loss: 1.6190 - val_accuracy: 0.3593\n",
      "Epoch 352/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.5974 - accuracy: 0.3750 - val_loss: 1.6295 - val_accuracy: 0.3644\n",
      "Epoch 353/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6041 - accuracy: 0.3693 - val_loss: 1.6464 - val_accuracy: 0.3522\n",
      "Epoch 354/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6040 - accuracy: 0.3715 - val_loss: 1.6506 - val_accuracy: 0.3633\n",
      "Epoch 355/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6013 - accuracy: 0.3710 - val_loss: 1.7854 - val_accuracy: 0.3342\n",
      "Epoch 356/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.5989 - accuracy: 0.3705 - val_loss: 1.5938 - val_accuracy: 0.3729\n",
      "Epoch 357/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6005 - accuracy: 0.3725 - val_loss: 1.7707 - val_accuracy: 0.2608\n",
      "Epoch 358/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6026 - accuracy: 0.3696 - val_loss: 1.6934 - val_accuracy: 0.3785\n",
      "Epoch 359/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.6032 - accuracy: 0.3713 - val_loss: 1.6455 - val_accuracy: 0.3335\n",
      "Epoch 360/500\n",
      "225/225 [==============================] - 15s 69ms/step - loss: 1.5988 - accuracy: 0.3751 - val_loss: 1.8241 - val_accuracy: 0.3224\n",
      "Epoch 361/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.5959 - accuracy: 0.3743 - val_loss: 1.6189 - val_accuracy: 0.3670\n",
      "Epoch 362/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6007 - accuracy: 0.3738 - val_loss: 1.6311 - val_accuracy: 0.3711\n",
      "Epoch 363/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6058 - accuracy: 0.3671 - val_loss: 1.5724 - val_accuracy: 0.3888\n",
      "Epoch 364/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6010 - accuracy: 0.3695 - val_loss: 1.5437 - val_accuracy: 0.3933\n",
      "Epoch 365/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.5964 - accuracy: 0.3728 - val_loss: 1.5846 - val_accuracy: 0.3764\n",
      "Epoch 366/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.5975 - accuracy: 0.3723 - val_loss: 1.5656 - val_accuracy: 0.3915\n",
      "Epoch 367/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.5988 - accuracy: 0.3738 - val_loss: 1.6367 - val_accuracy: 0.3622\n",
      "Epoch 368/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.5985 - accuracy: 0.3713 - val_loss: 1.6005 - val_accuracy: 0.3610\n",
      "Epoch 369/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6004 - accuracy: 0.3703 - val_loss: 2.0826 - val_accuracy: 0.2637\n",
      "Epoch 370/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6005 - accuracy: 0.3739 - val_loss: 1.5937 - val_accuracy: 0.3615\n",
      "Epoch 371/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.5952 - accuracy: 0.3740 - val_loss: 1.6643 - val_accuracy: 0.3477\n",
      "Epoch 372/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6047 - accuracy: 0.3704 - val_loss: 1.6940 - val_accuracy: 0.3582\n",
      "Epoch 373/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6066 - accuracy: 0.3677 - val_loss: 1.6241 - val_accuracy: 0.3873\n",
      "Epoch 374/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.5964 - accuracy: 0.3751 - val_loss: 1.5208 - val_accuracy: 0.3996\n",
      "Epoch 375/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.5979 - accuracy: 0.3754 - val_loss: 1.6130 - val_accuracy: 0.3586\n",
      "Epoch 376/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.5982 - accuracy: 0.3705 - val_loss: 1.6053 - val_accuracy: 0.3858\n",
      "Epoch 377/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.5928 - accuracy: 0.3741 - val_loss: 1.7355 - val_accuracy: 0.3390\n",
      "Epoch 378/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6020 - accuracy: 0.3716 - val_loss: 1.6749 - val_accuracy: 0.3780\n",
      "Epoch 379/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.5988 - accuracy: 0.3705 - val_loss: 1.5514 - val_accuracy: 0.3845\n",
      "Epoch 380/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.6036 - accuracy: 0.3687 - val_loss: 1.6844 - val_accuracy: 0.2921\n",
      "Epoch 381/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6001 - accuracy: 0.3733 - val_loss: 1.6459 - val_accuracy: 0.3417\n",
      "Epoch 382/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6000 - accuracy: 0.3724 - val_loss: 1.6018 - val_accuracy: 0.3756\n",
      "Epoch 383/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6005 - accuracy: 0.3718 - val_loss: 1.8782 - val_accuracy: 0.3125\n",
      "Epoch 384/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6038 - accuracy: 0.3712 - val_loss: 1.5352 - val_accuracy: 0.3873\n",
      "Epoch 385/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.5971 - accuracy: 0.3752 - val_loss: 1.8324 - val_accuracy: 0.3356\n",
      "Epoch 386/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6007 - accuracy: 0.3708 - val_loss: 1.5589 - val_accuracy: 0.3948\n",
      "Epoch 387/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.5971 - accuracy: 0.3733 - val_loss: 1.6434 - val_accuracy: 0.3668\n",
      "Epoch 388/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6001 - accuracy: 0.3718 - val_loss: 1.6465 - val_accuracy: 0.3587\n",
      "Epoch 389/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.5919 - accuracy: 0.3759 - val_loss: 1.6191 - val_accuracy: 0.3749\n",
      "Epoch 390/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.5997 - accuracy: 0.3705 - val_loss: 1.5768 - val_accuracy: 0.3872\n",
      "Epoch 391/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.5999 - accuracy: 0.3725 - val_loss: 1.8104 - val_accuracy: 0.3065\n",
      "Epoch 392/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.5994 - accuracy: 0.3715 - val_loss: 1.7498 - val_accuracy: 0.3374\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 393/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.5941 - accuracy: 0.3725 - val_loss: 1.5445 - val_accuracy: 0.3929\n",
      "Epoch 394/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.5977 - accuracy: 0.3727 - val_loss: 1.5508 - val_accuracy: 0.3961\n",
      "Epoch 395/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.5944 - accuracy: 0.3724 - val_loss: 1.6730 - val_accuracy: 0.3520\n",
      "Epoch 396/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.5971 - accuracy: 0.3771 - val_loss: 1.6245 - val_accuracy: 0.3564\n",
      "Epoch 397/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6003 - accuracy: 0.3690 - val_loss: 1.9197 - val_accuracy: 0.3188\n",
      "Epoch 398/500\n",
      "225/225 [==============================] - 15s 69ms/step - loss: 1.5971 - accuracy: 0.3729 - val_loss: 1.7945 - val_accuracy: 0.2990\n",
      "Epoch 399/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.5995 - accuracy: 0.3743 - val_loss: 1.7823 - val_accuracy: 0.2839\n",
      "Epoch 400/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.5955 - accuracy: 0.3750 - val_loss: 1.5879 - val_accuracy: 0.3672\n",
      "Epoch 401/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.5968 - accuracy: 0.3708 - val_loss: 1.6310 - val_accuracy: 0.3612\n",
      "Epoch 402/500\n",
      "225/225 [==============================] - 15s 69ms/step - loss: 1.5970 - accuracy: 0.3727 - val_loss: 1.7631 - val_accuracy: 0.3158\n",
      "Epoch 403/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6007 - accuracy: 0.3718 - val_loss: 1.5901 - val_accuracy: 0.3851\n",
      "Epoch 404/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6014 - accuracy: 0.3696 - val_loss: 1.5571 - val_accuracy: 0.3876\n",
      "Epoch 405/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.5955 - accuracy: 0.3769 - val_loss: 1.5289 - val_accuracy: 0.4008\n",
      "Epoch 406/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.5967 - accuracy: 0.3723 - val_loss: 1.5470 - val_accuracy: 0.3976\n",
      "Epoch 407/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6013 - accuracy: 0.3732 - val_loss: 1.6226 - val_accuracy: 0.3576\n",
      "Epoch 408/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.5916 - accuracy: 0.3779 - val_loss: 1.5748 - val_accuracy: 0.3789\n",
      "Epoch 409/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.5994 - accuracy: 0.3742 - val_loss: 1.7129 - val_accuracy: 0.3026\n",
      "Epoch 410/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6018 - accuracy: 0.3713 - val_loss: 1.6961 - val_accuracy: 0.3466\n",
      "Epoch 411/500\n",
      "225/225 [==============================] - 15s 69ms/step - loss: 1.6013 - accuracy: 0.3734 - val_loss: 1.5417 - val_accuracy: 0.3957\n",
      "Epoch 412/500\n",
      "225/225 [==============================] - 15s 69ms/step - loss: 1.6006 - accuracy: 0.3724 - val_loss: 1.6089 - val_accuracy: 0.3593\n",
      "Epoch 413/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.5973 - accuracy: 0.3723 - val_loss: 1.5929 - val_accuracy: 0.3780\n",
      "Epoch 414/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.5969 - accuracy: 0.3746 - val_loss: 1.5507 - val_accuracy: 0.3937\n",
      "Epoch 415/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.5959 - accuracy: 0.3724 - val_loss: 1.5524 - val_accuracy: 0.3877\n",
      "Epoch 416/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.5978 - accuracy: 0.3738 - val_loss: 1.5925 - val_accuracy: 0.3735\n",
      "Epoch 417/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.5975 - accuracy: 0.3724 - val_loss: 1.7191 - val_accuracy: 0.3189\n",
      "Epoch 418/500\n",
      "225/225 [==============================] - 15s 69ms/step - loss: 1.5977 - accuracy: 0.3701 - val_loss: 1.6450 - val_accuracy: 0.3604\n",
      "Epoch 419/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.5987 - accuracy: 0.3716 - val_loss: 1.6180 - val_accuracy: 0.3573\n",
      "Epoch 420/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6005 - accuracy: 0.3709 - val_loss: 1.5610 - val_accuracy: 0.3845\n",
      "Epoch 421/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.5980 - accuracy: 0.3752 - val_loss: 1.6865 - val_accuracy: 0.3583\n",
      "Epoch 422/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.5972 - accuracy: 0.3728 - val_loss: 1.8979 - val_accuracy: 0.2761\n",
      "Epoch 423/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.5977 - accuracy: 0.3754 - val_loss: 1.5958 - val_accuracy: 0.3767\n",
      "Epoch 424/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.5962 - accuracy: 0.3743 - val_loss: 1.5382 - val_accuracy: 0.4037\n",
      "Epoch 425/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6034 - accuracy: 0.3684 - val_loss: 1.6416 - val_accuracy: 0.3551\n",
      "Epoch 426/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.5979 - accuracy: 0.3777 - val_loss: 1.8678 - val_accuracy: 0.3005\n",
      "Epoch 427/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.5948 - accuracy: 0.3767 - val_loss: 1.5742 - val_accuracy: 0.3837\n",
      "Epoch 428/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.5957 - accuracy: 0.3760 - val_loss: 1.7151 - val_accuracy: 0.3164\n",
      "Epoch 429/500\n",
      "225/225 [==============================] - 15s 69ms/step - loss: 1.5984 - accuracy: 0.3744 - val_loss: 1.5636 - val_accuracy: 0.3912\n",
      "Epoch 430/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.5945 - accuracy: 0.3727 - val_loss: 1.5638 - val_accuracy: 0.3866\n",
      "Epoch 431/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.5991 - accuracy: 0.3729 - val_loss: 1.6150 - val_accuracy: 0.3596\n",
      "Epoch 432/500\n",
      "225/225 [==============================] - 15s 69ms/step - loss: 1.5972 - accuracy: 0.3723 - val_loss: 1.5855 - val_accuracy: 0.3785\n",
      "Epoch 433/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.5966 - accuracy: 0.3759 - val_loss: 1.7073 - val_accuracy: 0.3183\n",
      "Epoch 434/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.5970 - accuracy: 0.3757 - val_loss: 1.5534 - val_accuracy: 0.4040\n",
      "Epoch 435/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.5983 - accuracy: 0.3733 - val_loss: 1.7999 - val_accuracy: 0.2804\n",
      "Epoch 436/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.5974 - accuracy: 0.3707 - val_loss: 2.0959 - val_accuracy: 0.2891\n",
      "Epoch 437/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6045 - accuracy: 0.3719 - val_loss: 1.6175 - val_accuracy: 0.3657\n",
      "Epoch 438/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.5974 - accuracy: 0.3716 - val_loss: 1.5731 - val_accuracy: 0.3799\n",
      "Epoch 439/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.5963 - accuracy: 0.3742 - val_loss: 1.5879 - val_accuracy: 0.3801\n",
      "Epoch 440/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.5993 - accuracy: 0.3722 - val_loss: 1.6780 - val_accuracy: 0.3621\n",
      "Epoch 441/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.6001 - accuracy: 0.3701 - val_loss: 1.6014 - val_accuracy: 0.3696\n",
      "Epoch 442/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.5976 - accuracy: 0.3737 - val_loss: 1.6156 - val_accuracy: 0.3564\n",
      "Epoch 443/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.5965 - accuracy: 0.3734 - val_loss: 1.8394 - val_accuracy: 0.2935\n",
      "Epoch 444/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.5938 - accuracy: 0.3716 - val_loss: 1.6027 - val_accuracy: 0.3750\n",
      "Epoch 445/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.5973 - accuracy: 0.3749 - val_loss: 1.5664 - val_accuracy: 0.3775\n",
      "Epoch 446/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.5981 - accuracy: 0.3729 - val_loss: 1.5816 - val_accuracy: 0.3817\n",
      "Epoch 447/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.5951 - accuracy: 0.3760 - val_loss: 1.5480 - val_accuracy: 0.3940\n",
      "Epoch 448/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.5971 - accuracy: 0.3749 - val_loss: 1.6029 - val_accuracy: 0.3697\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 449/500\n",
      "225/225 [==============================] - 15s 69ms/step - loss: 1.5955 - accuracy: 0.3751 - val_loss: 1.5609 - val_accuracy: 0.3824\n",
      "Epoch 450/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.5998 - accuracy: 0.3716 - val_loss: 1.5544 - val_accuracy: 0.4009\n",
      "Epoch 451/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.5957 - accuracy: 0.3765 - val_loss: 1.5474 - val_accuracy: 0.3919\n",
      "Epoch 452/500\n",
      "225/225 [==============================] - 15s 69ms/step - loss: 1.5970 - accuracy: 0.3716 - val_loss: 1.6195 - val_accuracy: 0.3473\n",
      "Epoch 453/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.5914 - accuracy: 0.3743 - val_loss: 1.7110 - val_accuracy: 0.3268\n",
      "Epoch 454/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.5966 - accuracy: 0.3721 - val_loss: 1.6226 - val_accuracy: 0.3543\n",
      "Epoch 455/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.5932 - accuracy: 0.3765 - val_loss: 1.5589 - val_accuracy: 0.4009\n",
      "Epoch 456/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.5938 - accuracy: 0.3713 - val_loss: 1.6468 - val_accuracy: 0.3561\n",
      "Epoch 457/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.5932 - accuracy: 0.3746 - val_loss: 1.6629 - val_accuracy: 0.3629\n",
      "Epoch 458/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.5954 - accuracy: 0.3714 - val_loss: 1.5628 - val_accuracy: 0.3899\n",
      "Epoch 459/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.5906 - accuracy: 0.3760 - val_loss: 1.6247 - val_accuracy: 0.3699\n",
      "Epoch 460/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.5979 - accuracy: 0.3745 - val_loss: 1.5642 - val_accuracy: 0.3833\n",
      "Epoch 461/500\n",
      "225/225 [==============================] - 15s 69ms/step - loss: 1.5936 - accuracy: 0.3730 - val_loss: 1.5673 - val_accuracy: 0.3834\n",
      "Epoch 462/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.5978 - accuracy: 0.3756 - val_loss: 1.5503 - val_accuracy: 0.3876\n",
      "Epoch 463/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.5938 - accuracy: 0.3758 - val_loss: 1.6543 - val_accuracy: 0.3792\n",
      "Epoch 464/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.5960 - accuracy: 0.3736 - val_loss: 1.5951 - val_accuracy: 0.3828\n",
      "Epoch 465/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.5983 - accuracy: 0.3717 - val_loss: 1.8003 - val_accuracy: 0.3115\n",
      "Epoch 466/500\n",
      "225/225 [==============================] - 15s 69ms/step - loss: 1.5944 - accuracy: 0.3732 - val_loss: 1.5673 - val_accuracy: 0.3858\n",
      "Epoch 467/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.5931 - accuracy: 0.3715 - val_loss: 1.5657 - val_accuracy: 0.3837\n",
      "Epoch 468/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.5958 - accuracy: 0.3745 - val_loss: 1.6314 - val_accuracy: 0.3576\n",
      "Epoch 469/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.5958 - accuracy: 0.3700 - val_loss: 1.5548 - val_accuracy: 0.3913\n",
      "Epoch 470/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.5898 - accuracy: 0.3762 - val_loss: 1.5991 - val_accuracy: 0.3799\n",
      "Epoch 471/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.5966 - accuracy: 0.3748 - val_loss: 1.5714 - val_accuracy: 0.3801\n",
      "Epoch 472/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.5979 - accuracy: 0.3734 - val_loss: 1.5654 - val_accuracy: 0.3727\n",
      "Epoch 473/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.5879 - accuracy: 0.3808 - val_loss: 1.5544 - val_accuracy: 0.3888\n",
      "Epoch 474/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.5921 - accuracy: 0.3776 - val_loss: 1.5617 - val_accuracy: 0.3941\n",
      "Epoch 475/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.5906 - accuracy: 0.3801 - val_loss: 1.6205 - val_accuracy: 0.3690\n",
      "Epoch 476/500\n",
      "225/225 [==============================] - 15s 69ms/step - loss: 1.5875 - accuracy: 0.3763 - val_loss: 1.6067 - val_accuracy: 0.3736\n",
      "Epoch 477/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.5914 - accuracy: 0.3781 - val_loss: 1.6740 - val_accuracy: 0.3512\n",
      "Epoch 478/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.5928 - accuracy: 0.3746 - val_loss: 1.5560 - val_accuracy: 0.3853\n",
      "Epoch 479/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.5957 - accuracy: 0.3750 - val_loss: 1.5484 - val_accuracy: 0.3941\n",
      "Epoch 480/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.5925 - accuracy: 0.3769 - val_loss: 1.6541 - val_accuracy: 0.3522\n",
      "Epoch 481/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.5919 - accuracy: 0.3787 - val_loss: 1.6771 - val_accuracy: 0.3530\n",
      "Epoch 482/500\n",
      "225/225 [==============================] - 16s 70ms/step - loss: 1.5876 - accuracy: 0.3778 - val_loss: 1.6779 - val_accuracy: 0.3466\n",
      "Epoch 483/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.5941 - accuracy: 0.3740 - val_loss: 1.6652 - val_accuracy: 0.3631\n",
      "Epoch 484/500\n",
      "225/225 [==============================] - 16s 71ms/step - loss: 1.5987 - accuracy: 0.3683 - val_loss: 1.6526 - val_accuracy: 0.3447\n",
      "Epoch 485/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.5938 - accuracy: 0.3743 - val_loss: 1.6106 - val_accuracy: 0.3724\n",
      "Epoch 486/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.5927 - accuracy: 0.3788 - val_loss: 1.5802 - val_accuracy: 0.3881\n",
      "Epoch 487/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.5928 - accuracy: 0.3738 - val_loss: 1.5274 - val_accuracy: 0.4039\n",
      "Epoch 488/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.5974 - accuracy: 0.3743 - val_loss: 1.7667 - val_accuracy: 0.3306\n",
      "Epoch 489/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.5905 - accuracy: 0.3772 - val_loss: 1.5289 - val_accuracy: 0.3991\n",
      "Epoch 490/500\n",
      "225/225 [==============================] - 16s 69ms/step - loss: 1.5914 - accuracy: 0.3787 - val_loss: 1.5913 - val_accuracy: 0.3706\n",
      "Epoch 491/500\n",
      "225/225 [==============================] - 16s 73ms/step - loss: 1.5951 - accuracy: 0.3750 - val_loss: 1.6160 - val_accuracy: 0.3601\n",
      "Epoch 492/500\n",
      "225/225 [==============================] - 16s 72ms/step - loss: 1.5964 - accuracy: 0.3772 - val_loss: 1.6343 - val_accuracy: 0.3417\n",
      "Epoch 493/500\n",
      "225/225 [==============================] - 17s 74ms/step - loss: 1.5980 - accuracy: 0.3709 - val_loss: 1.5378 - val_accuracy: 0.4050\n",
      "Epoch 494/500\n",
      "225/225 [==============================] - 17s 75ms/step - loss: 1.5915 - accuracy: 0.3795 - val_loss: 1.5751 - val_accuracy: 0.3924\n",
      "Epoch 495/500\n",
      "225/225 [==============================] - 16s 72ms/step - loss: 1.5948 - accuracy: 0.3769 - val_loss: 1.5599 - val_accuracy: 0.3918\n",
      "Epoch 496/500\n",
      "225/225 [==============================] - 16s 72ms/step - loss: 1.5893 - accuracy: 0.3760 - val_loss: 1.7149 - val_accuracy: 0.3305\n",
      "Epoch 497/500\n",
      "225/225 [==============================] - 16s 73ms/step - loss: 1.5884 - accuracy: 0.3751 - val_loss: 1.6209 - val_accuracy: 0.3713\n",
      "Epoch 498/500\n",
      "225/225 [==============================] - 32s 143ms/step - loss: 1.5919 - accuracy: 0.3763 - val_loss: 1.6822 - val_accuracy: 0.3366\n",
      "Epoch 499/500\n",
      "225/225 [==============================] - 23s 102ms/step - loss: 1.5938 - accuracy: 0.3737 - val_loss: 1.7136 - val_accuracy: 0.3376\n",
      "Epoch 500/500\n",
      "225/225 [==============================] - 16s 73ms/step - loss: 1.5903 - accuracy: 0.3749 - val_loss: 1.7298 - val_accuracy: 0.3256\n",
      "{'loss': [2.421419382095337, 2.2151408195495605, 2.120431900024414, 2.060549020767212, 2.024853467941284, 2.0021262168884277, 1.9653072357177734, 1.9419585466384888, 1.921177864074707, 1.9049183130264282, 1.8850189447402954, 1.878862738609314, 1.8584290742874146, 1.84909987449646, 1.8435665369033813, 1.8276909589767456, 1.8200469017028809, 1.8102606534957886, 1.8004363775253296, 1.7914152145385742, 1.7851812839508057, 1.7791625261306763, 1.771736741065979, 1.7614765167236328, 1.7563045024871826, 1.7515740394592285, 1.7499173879623413, 1.750669240951538, 1.7456694841384888, 1.7323026657104492, 1.7319693565368652, 1.7289232015609741, 1.7266751527786255, 1.7215125560760498, 1.7176203727722168, 1.720686674118042, 1.7091236114501953, 1.7094112634658813, 1.7051979303359985, 1.7059917449951172, 1.7045849561691284, 1.7029727697372437, 1.7005985975265503, 1.6943789720535278, 1.6959565877914429, 1.6923704147338867, 1.6939692497253418, 1.6910395622253418, 1.6901965141296387, 1.6884478330612183, 1.6865720748901367, 1.6845207214355469, 1.6827255487442017, 1.6827843189239502, 1.684604287147522, 1.6796150207519531, 1.6792141199111938, 1.67377507686615, 1.6800389289855957, 1.6788192987442017, 1.6745576858520508, 1.6746487617492676, 1.6744240522384644, 1.6775376796722412, 1.6703118085861206, 1.6698790788650513, 1.6718722581863403, 1.6743004322052002, 1.67233145236969, 1.668586015701294, 1.6631580591201782, 1.6640934944152832, 1.6679446697235107, 1.6634857654571533, 1.6602790355682373, 1.6671212911605835, 1.6636719703674316, 1.6629269123077393, 1.6608799695968628, 1.6599717140197754, 1.6623923778533936, 1.6627353429794312, 1.6600836515426636, 1.6592897176742554, 1.6592891216278076, 1.6555252075195312, 1.6568199396133423, 1.6625556945800781, 1.658856749534607, 1.6552281379699707, 1.6551990509033203, 1.6563355922698975, 1.655308723449707, 1.6580958366394043, 1.657893419265747, 1.6569931507110596, 1.656143069267273, 1.6567002534866333, 1.6557930707931519, 1.6517972946166992, 1.6519181728363037, 1.652928113937378, 1.6514216661453247, 1.6499263048171997, 1.6516528129577637, 1.6486278772354126, 1.651414155960083, 1.6495141983032227, 1.6535422801971436, 1.6445940732955933, 1.649586796760559, 1.6492985486984253, 1.6495739221572876, 1.6464440822601318, 1.6496509313583374, 1.6471294164657593, 1.6455950736999512, 1.6485635042190552, 1.6465801000595093, 1.6454859972000122, 1.641471028327942, 1.6435182094573975, 1.6462329626083374, 1.645877718925476, 1.6424720287322998, 1.6442197561264038, 1.6407217979431152, 1.6415473222732544, 1.64613938331604, 1.6411783695220947, 1.6467974185943604, 1.6419081687927246, 1.6442644596099854, 1.6412527561187744, 1.6407817602157593, 1.6345422267913818, 1.640709638595581, 1.6389027833938599, 1.6407272815704346, 1.636236310005188, 1.6380890607833862, 1.640577793121338, 1.6410871744155884, 1.634395718574524, 1.638214349746704, 1.635338544845581, 1.63882577419281, 1.633084774017334, 1.6336864233016968, 1.63621187210083, 1.6411142349243164, 1.6373764276504517, 1.6323738098144531, 1.6307731866836548, 1.6373223066329956, 1.6326524019241333, 1.6354272365570068, 1.6333118677139282, 1.6378569602966309, 1.6309947967529297, 1.6359882354736328, 1.6313667297363281, 1.6322181224822998, 1.6316248178482056, 1.6383063793182373, 1.6300641298294067, 1.629520058631897, 1.6313267946243286, 1.630494475364685, 1.6308046579360962, 1.631765604019165, 1.6287819147109985, 1.6294342279434204, 1.6315871477127075, 1.6290956735610962, 1.6272937059402466, 1.6291495561599731, 1.6249632835388184, 1.6331466436386108, 1.629065752029419, 1.6239612102508545, 1.6255486011505127, 1.6309638023376465, 1.6275725364685059, 1.6264193058013916, 1.6312249898910522, 1.6279218196868896, 1.6251606941223145, 1.6234607696533203, 1.624055027961731, 1.6236783266067505, 1.625412106513977, 1.625083565711975, 1.6253855228424072, 1.627939224243164, 1.6254721879959106, 1.623952031135559, 1.6202709674835205, 1.6254788637161255, 1.621419906616211, 1.6223938465118408, 1.620835781097412, 1.6308021545410156, 1.625908374786377, 1.6291782855987549, 1.6272884607315063, 1.6216615438461304, 1.6271162033081055, 1.6265240907669067, 1.6229616403579712, 1.6195701360702515, 1.6181180477142334, 1.618692398071289, 1.623806118965149, 1.6250615119934082, 1.6272960901260376, 1.6194565296173096, 1.6149791479110718, 1.6158058643341064, 1.619352102279663, 1.6214340925216675, 1.6237168312072754, 1.6205517053604126, 1.6178503036499023, 1.6171566247940063, 1.6206783056259155, 1.615188717842102, 1.6211094856262207, 1.615875482559204, 1.6143853664398193, 1.6202400922775269, 1.6172823905944824, 1.6100441217422485, 1.6194261312484741, 1.6122303009033203, 1.6129647493362427, 1.6165554523468018, 1.6179089546203613, 1.6209512948989868, 1.6112290620803833, 1.6125162839889526, 1.614633321762085, 1.6170966625213623, 1.617795467376709, 1.6174980401992798, 1.6168092489242554, 1.6119639873504639, 1.6182622909545898, 1.6170886754989624, 1.6146601438522339, 1.6184579133987427, 1.6141945123672485, 1.616120457649231, 1.6114814281463623, 1.6194918155670166, 1.6132926940917969, 1.6119993925094604, 1.6136308908462524, 1.6113835573196411, 1.6119022369384766, 1.6127994060516357, 1.6183913946151733, 1.6114773750305176, 1.611883282661438, 1.6171950101852417, 1.608078956604004, 1.6121739149093628, 1.6129992008209229, 1.6122686862945557, 1.6086478233337402, 1.6116433143615723, 1.61149001121521, 1.6153538227081299, 1.6160163879394531, 1.6093329191207886, 1.6127663850784302, 1.6088097095489502, 1.6104549169540405, 1.609017252922058, 1.6092325448989868, 1.6092482805252075, 1.605066180229187, 1.6089198589324951, 1.613489031791687, 1.613803744316101, 1.6112003326416016, 1.6064683198928833, 1.604943871498108, 1.6057758331298828, 1.6067907810211182, 1.6085340976715088, 1.609311819076538, 1.6090904474258423, 1.6063330173492432, 1.61143958568573, 1.6081291437149048, 1.6084001064300537, 1.6073875427246094, 1.602387547492981, 1.6075865030288696, 1.6118606328964233, 1.6042943000793457, 1.6081658601760864, 1.606020212173462, 1.6096673011779785, 1.6073094606399536, 1.6057385206222534, 1.6091437339782715, 1.6058168411254883, 1.6071386337280273, 1.605603575706482, 1.6070990562438965, 1.605902075767517, 1.6079100370407104, 1.606744647026062, 1.6082314252853394, 1.606184959411621, 1.6080189943313599, 1.607655644416809, 1.6011874675750732, 1.6081761121749878, 1.6061965227127075, 1.6057934761047363, 1.6110990047454834, 1.6112627983093262, 1.6069406270980835, 1.6065207719802856, 1.601645827293396, 1.5990818738937378, 1.601760983467102, 1.5999531745910645, 1.6043261289596558, 1.6060969829559326, 1.6035634279251099, 1.6059719324111938, 1.601837396621704, 1.6044228076934814, 1.6045995950698853, 1.598772644996643, 1.6054342985153198, 1.6030689477920532, 1.5991449356079102, 1.604541301727295, 1.6063810586929321, 1.6023005247116089, 1.6036230325698853, 1.6020125150680542, 1.6003639698028564, 1.6066585779190063, 1.5986132621765137, 1.6033146381378174, 1.5973501205444336, 1.6040546894073486, 1.603965163230896, 1.6013150215148926, 1.5988974571228027, 1.6005233526229858, 1.6026322841644287, 1.6032061576843262, 1.5988037586212158, 1.5958679914474487, 1.6006684303283691, 1.605835199356079, 1.6009695529937744, 1.596369981765747, 1.597505807876587, 1.5987728834152222, 1.5984547138214111, 1.6004054546356201, 1.6004713773727417, 1.5952200889587402, 1.604726791381836, 1.6066036224365234, 1.5964186191558838, 1.597915530204773, 1.5981886386871338, 1.5928311347961426, 1.6019617319107056, 1.5988422632217407, 1.6035652160644531, 1.6001023054122925, 1.6000266075134277, 1.6005300283432007, 1.6037944555282593, 1.5971195697784424, 1.6006543636322021, 1.5970863103866577, 1.6000605821609497, 1.5918978452682495, 1.5997475385665894, 1.5999155044555664, 1.5993670225143433, 1.5940594673156738, 1.5976802110671997, 1.5944427251815796, 1.5970580577850342, 1.600327730178833, 1.597112774848938, 1.5994584560394287, 1.5955384969711304, 1.5968071222305298, 1.5970313549041748, 1.6006754636764526, 1.6013867855072021, 1.59549880027771, 1.5967118740081787, 1.601325511932373, 1.5915569067001343, 1.5993701219558716, 1.6017842292785645, 1.6012576818466187, 1.600572109222412, 1.5973459482192993, 1.5968701839447021, 1.595943808555603, 1.5978466272354126, 1.5975061655044556, 1.5976725816726685, 1.5987474918365479, 1.6005314588546753, 1.5979632139205933, 1.5971637964248657, 1.5977479219436646, 1.59621262550354, 1.6033610105514526, 1.5978933572769165, 1.5948210954666138, 1.5957399606704712, 1.5984351634979248, 1.5944669246673584, 1.599087119102478, 1.597188949584961, 1.5965607166290283, 1.597022294998169, 1.5982664823532104, 1.597448468208313, 1.604477882385254, 1.5973703861236572, 1.5963339805603027, 1.5992757081985474, 1.6001183986663818, 1.5975621938705444, 1.5964713096618652, 1.5938445329666138, 1.5972652435302734, 1.5981007814407349, 1.5951288938522339, 1.5970633029937744, 1.5955054759979248, 1.5998339653015137, 1.595697045326233, 1.5970139503479004, 1.5914306640625, 1.5966253280639648, 1.593249797821045, 1.5938167572021484, 1.5932140350341797, 1.5953773260116577, 1.5906193256378174, 1.597937822341919, 1.593565821647644, 1.5977513790130615, 1.5938135385513306, 1.5960137844085693, 1.5982532501220703, 1.594400405883789, 1.5930594205856323, 1.5958456993103027, 1.5957770347595215, 1.5898431539535522, 1.596573829650879, 1.597855806350708, 1.5878980159759521, 1.5920872688293457, 1.5905661582946777, 1.5874834060668945, 1.5913587808609009, 1.5928094387054443, 1.5956560373306274, 1.5924913883209229, 1.591895341873169, 1.5875908136367798, 1.5940715074539185, 1.5986659526824951, 1.5937954187393188, 1.5927138328552246, 1.592846155166626, 1.5973880290985107, 1.5904566049575806, 1.5914435386657715, 1.5951403379440308, 1.596405029296875, 1.5980321168899536, 1.5914579629898071, 1.594794511795044, 1.5892614126205444, 1.5884344577789307, 1.5918784141540527, 1.5938071012496948, 1.590332269668579], 'accuracy': [0.17214113473892212, 0.20185308158397675, 0.22083666920661926, 0.23114702105522156, 0.23720784485340118, 0.2389494627714157, 0.24535860121250153, 0.25274303555488586, 0.2577240467071533, 0.2642377018928528, 0.26493433117866516, 0.2649691700935364, 0.26977601647377014, 0.275906503200531, 0.2751750349998474, 0.2806088626384735, 0.27956390380859375, 0.28370893001556396, 0.28990909457206726, 0.2894214391708374, 0.2940889596939087, 0.2933226525783539, 0.29621371626853943, 0.29924413561820984, 0.3010902404785156, 0.3025532066822052, 0.3023442029953003, 0.3021700382232666, 0.3038420081138611, 0.31150510907173157, 0.31411752104759216, 0.3136298656463623, 0.31369954347610474, 0.31223657727241516, 0.3165906071662903, 0.31861087679862976, 0.32063114643096924, 0.31986483931541443, 0.31843674182891846, 0.322407603263855, 0.323208749294281, 0.3229997456073761, 0.3220592737197876, 0.32515937089920044, 0.3251245319843292, 0.32756277918815613, 0.32798078656196594, 0.3271796405315399, 0.3310460150241852, 0.3299662172794342, 0.329931378364563, 0.33324044942855835, 0.33170783519744873, 0.3343202471733093, 0.32857292890548706, 0.3340764343738556, 0.33115050196647644, 0.33776864409446716, 0.33341461420059204, 0.3337281048297882, 0.3347034156322479, 0.3359921872615814, 0.3363405168056488, 0.33383259177207947, 0.33944058418273926, 0.3381866216659546, 0.3368978500366211, 0.33721134066581726, 0.3373158276081085, 0.33665400743484497, 0.3410428762435913, 0.3374899923801422, 0.3372809886932373, 0.34247100353240967, 0.3438294529914856, 0.33606186509132385, 0.33769896626472473, 0.3432721495628357, 0.3427496552467346, 0.34435194730758667, 0.343097984790802, 0.34243616461753845, 0.3419485092163086, 0.342018187046051, 0.3447350859642029, 0.3455710709095001, 0.34372496604919434, 0.3397540748119354, 0.3433069884777069, 0.3474171757698059, 0.3428889811038971, 0.3455014228820801, 0.3463025391101837, 0.34166985750198364, 0.34431710839271545, 0.3431328237056732, 0.34278449416160583, 0.34264516830444336, 0.34658122062683105, 0.3446305990219116, 0.34598904848098755, 0.3447350859642029, 0.34713852405548096, 0.34504857659339905, 0.3464767038822174, 0.3466160297393799, 0.3460935652256012, 0.35090041160583496, 0.3451879322528839, 0.35124874114990234, 0.3509700894355774, 0.34598904848098755, 0.34825316071510315, 0.3492633104324341, 0.34640705585479736, 0.3477306663990021, 0.3476261794567108, 0.3476261794567108, 0.3462677299976349, 0.35086557269096375, 0.35051727294921875, 0.3507959246635437, 0.34992510080337524, 0.35204988718032837, 0.3502734303474426, 0.35312968492507935, 0.3494374454021454, 0.3507959246635437, 0.3509352505207062, 0.352502703666687, 0.34731268882751465, 0.35204988718032837, 0.3528858423233032, 0.3536521792411804, 0.35016894340515137, 0.3543139696121216, 0.35110941529273987, 0.3517712354660034, 0.3510048985481262, 0.3560207486152649, 0.3518757224082947, 0.35215437412261963, 0.35180604457855225, 0.35859835147857666, 0.35372182726860046, 0.3536173403263092, 0.3481138348579407, 0.3553241193294525, 0.3545229732990265, 0.3535476624965668, 0.35198020935058594, 0.354383647441864, 0.3564039170742035, 0.3598174750804901, 0.3549061417579651, 0.35535895824432373, 0.3532341718673706, 0.3586679995059967, 0.35403531789779663, 0.3581106960773468, 0.35427916049957275, 0.35650840401649475, 0.354383647441864, 0.3551499545574188, 0.35278135538101196, 0.35633423924446106, 0.3538263142108917, 0.3584938645362854, 0.35821518301963806, 0.3586679995059967, 0.3560207486152649, 0.35629943013191223, 0.35685673356056213, 0.35654324293136597, 0.35706573724746704, 0.36044445633888245, 0.35616007447242737, 0.362987220287323, 0.35671740770339966, 0.35877251625061035, 0.3642760217189789, 0.35842418670654297, 0.35518479347229004, 0.358145534992218, 0.3584938645362854, 0.35929498076438904, 0.3614546060562134, 0.3605837821960449, 0.35953882336616516, 0.36162877082824707, 0.35929498076438904, 0.3611411154270172, 0.3589118421077728, 0.36065346002578735, 0.35706573724746704, 0.35953882336616516, 0.3570308983325958, 0.359225332736969, 0.3564039170742035, 0.3597826361656189, 0.36497265100479126, 0.3607231080532074, 0.3554982841014862, 0.3584938645362854, 0.3594343364238739, 0.3584938645362854, 0.3613152801990509, 0.35779720544815063, 0.35762304067611694, 0.3611411154270172, 0.36361420154571533, 0.36417150497436523, 0.3626040518283844, 0.3607231080532074, 0.35598593950271606, 0.3596084713935852, 0.36009612679481506, 0.36305686831474304, 0.3644501864910126, 0.35936465859413147, 0.3633703589439392, 0.3589118421077728, 0.36270856857299805, 0.3631265461444855, 0.36121076345443726, 0.35894668102264404, 0.3648333251476288, 0.36124560236930847, 0.3631962239742279, 0.3690131902694702, 0.3605837821960449, 0.3639276921749115, 0.36685359477996826, 0.36549514532089233, 0.3674805760383606, 0.3644501864910126, 0.36417150497436523, 0.36552998423576355, 0.3596433103084564, 0.3663311302661896, 0.36667943000793457, 0.36364904046058655, 0.36458951234817505, 0.3633355498313904, 0.3609669506549835, 0.36667943000793457, 0.36758506298065186, 0.3615242540836334, 0.3634051978588104, 0.3637186884880066, 0.3635445237159729, 0.3653906583786011, 0.363788366317749, 0.36723676323890686, 0.36455467343330383, 0.36197707056999207, 0.36420634388923645, 0.36399734020233154, 0.36438050866127014, 0.363788366317749, 0.3667142689228058, 0.36211639642715454, 0.36699292063713074, 0.3704761564731598, 0.3652861416339874, 0.36911770701408386, 0.3693615198135376, 0.36793339252471924, 0.3661569654941559, 0.36702775955200195, 0.36629629135131836, 0.36403217911720276, 0.3656344711780548, 0.36887386441230774, 0.3665052652359009, 0.36835139989852905, 0.36779406666755676, 0.3649030029773712, 0.3671322464942932, 0.3648681640625, 0.367550253868103, 0.3711031377315521, 0.3688390254974365, 0.36267372965812683, 0.367550253868103, 0.3678637444972992, 0.36737608909606934, 0.36716708540916443, 0.36817723512649536, 0.36831656098365784, 0.36608728766441345, 0.3670974373817444, 0.3661917746067047, 0.37416836619377136, 0.36925703287124634, 0.3657737970352173, 0.3664007782936096, 0.3682468831539154, 0.37322789430618286, 0.3675154149532318, 0.3652513027191162, 0.37249642610549927, 0.3693963587284088, 0.3691873550415039, 0.36249956488609314, 0.3665401041507721, 0.3710334599018097, 0.3646243214607239, 0.36716708540916443, 0.367828905582428, 0.37002333998680115, 0.3712773025035858, 0.36800307035446167, 0.36573895812034607, 0.3691525161266327, 0.3661569654941559, 0.36643561720848083, 0.36814239621162415, 0.36706259846687317, 0.3717649579048157, 0.37117281556129456, 0.3673064112663269, 0.36549514532089233, 0.36925703287124634, 0.36291754245758057, 0.36608728766441345, 0.3668884336948395, 0.37096381187438965, 0.36800307035446167, 0.3706851601600647, 0.3689783811569214, 0.36706259846687317, 0.366749107837677, 0.3714166283607483, 0.37092897295951843, 0.3714166283607483, 0.36981433629989624, 0.37002333998680115, 0.37737295031547546, 0.3687693774700165, 0.36775922775268555, 0.3725312650203705, 0.37173011898994446, 0.36573895812034607, 0.37131214141845703, 0.37301892042160034, 0.3718346059322357, 0.3669581115245819, 0.3676895797252655, 0.3714166283607483, 0.3712773025035858, 0.3750043511390686, 0.36925703287124634, 0.3714514672756195, 0.37099865078926086, 0.3704761564731598, 0.3725312650203705, 0.36964017152786255, 0.3712773025035858, 0.37507402896881104, 0.37434253096580505, 0.37375038862228394, 0.3670974373817444, 0.3695356845855713, 0.37280991673469543, 0.37228742241859436, 0.37378522753715515, 0.37131214141845703, 0.3702671527862549, 0.3739245533943176, 0.3740290403366089, 0.37040647864341736, 0.3676895797252655, 0.3751436769962311, 0.37542232871055603, 0.370510995388031, 0.3740638792514801, 0.37155595421791077, 0.370510995388031, 0.3686648905277252, 0.3732975721359253, 0.372391939163208, 0.3717649579048157, 0.3712076246738434, 0.3752133548259735, 0.3708244860172272, 0.3733324110507965, 0.3717997968196869, 0.3759448230266571, 0.370510995388031, 0.37246158719062805, 0.3714863061904907, 0.37246158719062805, 0.3727054297924042, 0.3723571002483368, 0.37712913751602173, 0.36904802918434143, 0.3729144036769867, 0.37434253096580505, 0.3750391900539398, 0.37078964710235596, 0.3727054297924042, 0.3718346059322357, 0.3696053624153137, 0.3768852949142456, 0.37225261330604553, 0.37319308519363403, 0.37789544463157654, 0.3742032051086426, 0.37134695053100586, 0.37343689799308777, 0.3724267780780792, 0.37225261330604553, 0.37455153465270996, 0.3723571002483368, 0.37382006645202637, 0.3724267780780792, 0.3701278269290924, 0.37155595421791077, 0.3708593249320984, 0.3751785159111023, 0.37284475564956665, 0.3753875195980072, 0.37434253096580505, 0.36835139989852905, 0.3776516020298004, 0.37674596905708313, 0.3759796619415283, 0.37437736988067627, 0.3727402687072754, 0.37287959456443787, 0.37225261330604553, 0.3759099841117859, 0.3757358193397522, 0.3732627332210541, 0.3706851601600647, 0.37190428376197815, 0.3716256320476532, 0.3742380440235138, 0.3722177743911743, 0.37005817890167236, 0.3737155497074127, 0.37343689799308777, 0.37155595421791077, 0.37486502528190613, 0.3729492425918579, 0.37601450085639954, 0.37486502528190613, 0.37510883808135986, 0.3716256320476532, 0.3765021562576294, 0.371590793132782, 0.3743077218532562, 0.3721480965614319, 0.3764673173427582, 0.37134695053100586, 0.3745863735675812, 0.3714166283607483, 0.3759796619415283, 0.37448185682296753, 0.3729840815067291, 0.37563133239746094, 0.37584033608436584, 0.37357622385025024, 0.371660441160202, 0.37322789430618286, 0.37152111530303955, 0.37451669573783875, 0.36998850107192993, 0.3761886656284332, 0.3747953474521637, 0.37343689799308777, 0.3807865083217621, 0.37758195400238037, 0.3800898790359497, 0.3763279914855957, 0.3781392574310303, 0.37455153465270996, 0.3750391900539398, 0.3768852949142456, 0.37866175174713135, 0.3778257668018341, 0.37399423122406006, 0.3682817220687866, 0.3743077218532562, 0.3788010776042938, 0.37378522753715515, 0.3743077218532562, 0.3771987855434418, 0.3787313997745514, 0.3749695122241974, 0.377233624458313, 0.37092897295951843, 0.3795325458049774, 0.3769201338291168, 0.3759796619415283, 0.37507402896881104, 0.3763279914855957, 0.3736807405948639, 0.37486502528190613], 'val_loss': [1.9072153568267822, 1.9484546184539795, 1.8729304075241089, 1.986691951751709, 1.8819999694824219, 1.7626190185546875, 1.7372509241104126, 1.754188060760498, 1.772655725479126, 1.7333359718322754, 1.9468555450439453, 1.7064862251281738, 1.7385849952697754, 1.7784446477890015, 1.7693759202957153, 1.7245885133743286, 1.7021723985671997, 1.7416741847991943, 1.7237462997436523, 1.7212560176849365, 1.6888787746429443, 1.75411057472229, 1.718977689743042, 1.6780320405960083, 1.6713167428970337, 1.7201032638549805, 1.7881487607955933, 1.7387933731079102, 1.7353605031967163, 1.7053276300430298, 1.654656171798706, 2.006554365158081, 1.7315971851348877, 1.6806697845458984, 1.7104653120040894, 1.8063753843307495, 1.6493275165557861, 1.6676572561264038, 1.688430905342102, 1.6537318229675293, 1.6460285186767578, 1.7051398754119873, 1.7988413572311401, 1.8078739643096924, 1.7512686252593994, 1.665060043334961, 1.6336637735366821, 1.6590291261672974, 1.6551278829574585, 1.6759077310562134, 1.6189006567001343, 1.8733891248703003, 1.68599271774292, 1.6151008605957031, 1.7509791851043701, 1.6251403093338013, 1.7855994701385498, 1.6140196323394775, 1.6444146633148193, 1.6438943147659302, 1.6246051788330078, 1.7516640424728394, 1.6677532196044922, 1.8539942502975464, 1.8371831178665161, 1.8565106391906738, 1.7114776372909546, 1.6271429061889648, 1.7493348121643066, 1.6860461235046387, 1.6530370712280273, 1.6177654266357422, 1.658789873123169, 1.7116495370864868, 1.613547921180725, 1.9959369897842407, 1.6403696537017822, 1.6810964345932007, 1.648781180381775, 1.8175979852676392, 1.6135814189910889, 1.6653558015823364, 1.6717370748519897, 1.6886577606201172, 1.6769564151763916, 1.6351768970489502, 1.601159930229187, 1.6118230819702148, 1.601064682006836, 1.6404199600219727, 1.660544514656067, 2.100602149963379, 1.7321562767028809, 1.6077992916107178, 1.8412091732025146, 1.9156123399734497, 1.6363011598587036, 1.7226511240005493, 1.7445194721221924, 1.5917894840240479, 1.7087405920028687, 1.6142770051956177, 1.6302986145019531, 1.6165815591812134, 1.6874690055847168, 1.6983503103256226, 1.6447992324829102, 1.8446458578109741, 1.6437290906906128, 1.7368263006210327, 1.7010247707366943, 1.5588347911834717, 1.690582036972046, 1.804249882698059, 1.8582781553268433, 1.6770631074905396, 1.6866021156311035, 1.605828046798706, 1.6850817203521729, 1.6378573179244995, 1.6015231609344482, 1.6489492654800415, 1.663741946220398, 1.6137585639953613, 1.7768398523330688, 1.7293169498443604, 1.6090141534805298, 1.6731797456741333, 1.692901849746704, 1.599686622619629, 2.0067577362060547, 1.6855932474136353, 1.653211236000061, 1.8879203796386719, 1.6100536584854126, 1.7484264373779297, 1.5975650548934937, 1.6713674068450928, 1.6265807151794434, 1.6476296186447144, 1.72968327999115, 1.7669765949249268, 1.8402529954910278, 1.687195897102356, 1.6961305141448975, 1.614065170288086, 1.6667739152908325, 1.7980247735977173, 1.61089289188385, 1.647533893585205, 1.5847522020339966, 1.7717465162277222, 1.6172778606414795, 1.6909496784210205, 1.6961359977722168, 1.601386308670044, 1.6158932447433472, 2.3458375930786133, 1.7714861631393433, 1.6066426038742065, 1.6025383472442627, 1.6428377628326416, 1.5579333305358887, 1.9057172536849976, 1.6674734354019165, 1.755034327507019, 1.6067105531692505, 1.8200188875198364, 1.82877779006958, 1.5929006338119507, 2.0130624771118164, 1.6502362489700317, 1.5889229774475098, 1.7046362161636353, 1.7187477350234985, 1.6943249702453613, 1.763833999633789, 1.7465192079544067, 1.6459771394729614, 1.824533462524414, 1.727435827255249, 1.5685828924179077, 1.7663112878799438, 1.7262014150619507, 1.5635327100753784, 1.6669644117355347, 1.7000179290771484, 1.6754072904586792, 1.580956220626831, 1.730453610420227, 1.7233619689941406, 1.6884372234344482, 1.781701683998108, 1.6219236850738525, 1.5887494087219238, 1.6133112907409668, 1.7506030797958374, 2.2018182277679443, 1.6622239351272583, 1.5761311054229736, 1.6987439393997192, 1.698441505432129, 1.6341725587844849, 1.7905951738357544, 1.7053004503250122, 1.6035629510879517, 1.5963971614837646, 1.6192668676376343, 1.7147635221481323, 1.6421306133270264, 1.6120764017105103, 1.7302205562591553, 1.5630913972854614, 1.6979272365570068, 1.9207888841629028, 1.5985474586486816, 1.6524721384048462, 1.7262433767318726, 1.582934856414795, 1.6226094961166382, 1.8094698190689087, 1.6697609424591064, 1.6327165365219116, 1.6093450784683228, 1.5462638139724731, 1.6441960334777832, 1.6223926544189453, 1.670257329940796, 1.5586225986480713, 1.5974414348602295, 1.655126690864563, 1.597725749015808, 1.6206610202789307, 1.6367701292037964, 1.6331055164337158, 1.632663369178772, 1.58591628074646, 1.5721797943115234, 1.6061028242111206, 1.5920640230178833, 1.695467472076416, 1.60009765625, 1.6367700099945068, 1.6649267673492432, 1.6258314847946167, 1.6383899450302124, 1.8283426761627197, 1.5764755010604858, 1.6027085781097412, 1.618076205253601, 1.727560043334961, 1.82766854763031, 1.6528446674346924, 1.6322636604309082, 1.7847371101379395, 1.6565419435501099, 1.6990342140197754, 1.5542658567428589, 1.662250280380249, 1.5838987827301025, 1.5409959554672241, 1.5723356008529663, 1.6287798881530762, 1.6692358255386353, 1.7548930644989014, 1.6058440208435059, 1.6758074760437012, 1.673296570777893, 1.577283501625061, 2.0098443031311035, 1.5644701719284058, 1.611254096031189, 1.6455377340316772, 1.6655597686767578, 1.6301816701889038, 1.5765644311904907, 1.6567286252975464, 1.7093201875686646, 1.6089911460876465, 1.595637321472168, 1.7466919422149658, 1.8324642181396484, 1.5578570365905762, 1.592633843421936, 1.715777039527893, 1.8421579599380493, 1.5863093137741089, 1.649048089981079, 1.5379533767700195, 1.7505866289138794, 1.819191336631775, 1.7539005279541016, 1.6396385431289673, 1.5752350091934204, 1.5798410177230835, 1.6276134252548218, 1.6644034385681152, 1.7353575229644775, 1.7250216007232666, 1.6026767492294312, 1.7261179685592651, 1.5360441207885742, 1.5727211236953735, 1.8962464332580566, 1.7133153676986694, 1.8789420127868652, 1.709973692893982, 1.640123963356018, 1.6194056272506714, 1.7184784412384033, 1.6850429773330688, 1.666154146194458, 1.5285203456878662, 1.6157143115997314, 1.857804775238037, 1.7032945156097412, 1.7074416875839233, 1.6232271194458008, 1.603535532951355, 1.8397891521453857, 1.806920051574707, 1.9561125040054321, 1.6156450510025024, 1.717728853225708, 1.5972537994384766, 1.572586178779602, 1.5852453708648682, 1.5745874643325806, 1.5829358100891113, 1.6053383350372314, 1.585259199142456, 1.7440264225006104, 1.7144200801849365, 1.6676181554794312, 1.8342626094818115, 1.6388046741485596, 1.6492242813110352, 1.8896863460540771, 1.7007005214691162, 1.604028582572937, 1.536759376525879, 1.744728684425354, 1.6341397762298584, 1.7729783058166504, 1.5376189947128296, 1.6519572734832764, 1.586663842201233, 1.7600301504135132, 1.6336703300476074, 1.6884572505950928, 1.6190317869186401, 1.6295384168624878, 1.6463580131530762, 1.650628685951233, 1.785424828529358, 1.593792200088501, 1.7706677913665771, 1.693412184715271, 1.645524263381958, 1.8241161108016968, 1.6188769340515137, 1.6310573816299438, 1.5724220275878906, 1.5436580181121826, 1.5845773220062256, 1.5656176805496216, 1.6367335319519043, 1.6004860401153564, 2.08260178565979, 1.5937271118164062, 1.6642663478851318, 1.694020390510559, 1.6240782737731934, 1.5208115577697754, 1.6129982471466064, 1.6053119897842407, 1.735529899597168, 1.6749250888824463, 1.5513722896575928, 1.6844009160995483, 1.6459239721298218, 1.6018404960632324, 1.8781942129135132, 1.5352461338043213, 1.8323801755905151, 1.5589367151260376, 1.6433645486831665, 1.6464735269546509, 1.6191171407699585, 1.5767508745193481, 1.810390830039978, 1.7497509717941284, 1.544466495513916, 1.5508413314819336, 1.673008918762207, 1.6245003938674927, 1.9196515083312988, 1.7944906949996948, 1.7823164463043213, 1.5878654718399048, 1.6310417652130127, 1.7630600929260254, 1.590102195739746, 1.557091236114502, 1.5288705825805664, 1.5469681024551392, 1.6226221323013306, 1.574805498123169, 1.712886929512024, 1.696087121963501, 1.5417003631591797, 1.608918309211731, 1.5929033756256104, 1.5507426261901855, 1.552405595779419, 1.5924534797668457, 1.7190525531768799, 1.6450304985046387, 1.6180330514907837, 1.5610005855560303, 1.6864569187164307, 1.897905945777893, 1.5957599878311157, 1.5381762981414795, 1.6416195631027222, 1.867828607559204, 1.574245572090149, 1.715073585510254, 1.5635613203048706, 1.5637850761413574, 1.6150332689285278, 1.5854703187942505, 1.7072786092758179, 1.5534149408340454, 1.7999050617218018, 2.095942497253418, 1.617480993270874, 1.5730756521224976, 1.587884545326233, 1.6780047416687012, 1.6013855934143066, 1.6155554056167603, 1.839424729347229, 1.6026779413223267, 1.5663609504699707, 1.5815643072128296, 1.5480153560638428, 1.602935552597046, 1.5609462261199951, 1.5544475317001343, 1.5474343299865723, 1.61953866481781, 1.7110453844070435, 1.622606635093689, 1.5588929653167725, 1.6467548608779907, 1.662902593612671, 1.5627692937850952, 1.6246827840805054, 1.5641674995422363, 1.5672621726989746, 1.550326943397522, 1.6542786359786987, 1.5951060056686401, 1.8003267049789429, 1.5672799348831177, 1.565739631652832, 1.6314092874526978, 1.5548368692398071, 1.5991135835647583, 1.5713661909103394, 1.5654222965240479, 1.5544419288635254, 1.5616872310638428, 1.6204588413238525, 1.6067355871200562, 1.6740208864212036, 1.5559951066970825, 1.5483758449554443, 1.6541246175765991, 1.6771498918533325, 1.6778615713119507, 1.665239930152893, 1.6525838375091553, 1.6106241941452026, 1.5802116394042969, 1.5273646116256714, 1.7666652202606201, 1.5289021730422974, 1.5913137197494507, 1.6159815788269043, 1.6343454122543335, 1.5377949476242065, 1.5751487016677856, 1.5598936080932617, 1.7148736715316772, 1.6209429502487183, 1.6822181940078735, 1.7136197090148926, 1.7297532558441162], 'val_accuracy': [0.21426580846309662, 0.2532739043235779, 0.26971301436424255, 0.261075496673584, 0.26469770073890686, 0.2904708981513977, 0.30147674679756165, 0.29367512464523315, 0.3045416474342346, 0.3010587990283966, 0.2708275318145752, 0.31707996129989624, 0.28587350249290466, 0.29381442070007324, 0.24644747376441956, 0.3186124265193939, 0.3216773569583893, 0.29660072922706604, 0.2984118163585663, 0.3069100081920624, 0.31777653098106384, 0.3062134385108948, 0.3126218914985657, 0.342017263174057, 0.3336583971977234, 0.3279465138912201, 0.28670939803123474, 0.319448322057724, 0.28740596771240234, 0.3151295483112335, 0.3519086241722107, 0.2812761068344116, 0.31568682193756104, 0.3227918744087219, 0.3059347867965698, 0.3220953047275543, 0.3630537688732147, 0.3425745368003845, 0.33184731006622314, 0.34940093755722046, 0.35567009449005127, 0.3209807872772217, 0.29297855496406555, 0.2095291167497635, 0.29451102018356323, 0.3425745368003845, 0.349958211183548, 0.3376985192298889, 0.34229591488838196, 0.3399275541305542, 0.3666759431362152, 0.27667874097824097, 0.3390916585922241, 0.3630537688732147, 0.30565616488456726, 0.35706326365470886, 0.33101141452789307, 0.36152130365371704, 0.350376158952713, 0.3609640598297119, 0.3764279782772064, 0.30858176946640015, 0.3448035717010498, 0.2869880199432373, 0.24700473248958588, 0.24212872982025146, 0.3135971128940582, 0.35511285066604614, 0.3234884440898895, 0.331290066242218, 0.35371971130371094, 0.3730844259262085, 0.342713862657547, 0.34034550189971924, 0.3701588213443756, 0.2750069797039032, 0.35929226875305176, 0.31916967034339905, 0.35915297269821167, 0.3208414614200592, 0.36932292580604553, 0.34536081552505493, 0.31833380460739136, 0.3425745368003845, 0.3491223156452179, 0.35302311182022095, 0.369462251663208, 0.3560880422592163, 0.3704374432563782, 0.35720255970954895, 0.3393703103065491, 0.23446641862392426, 0.33588743209838867, 0.3626358211040497, 0.29994428157806396, 0.31401506066322327, 0.35093340277671814, 0.3144329786300659, 0.30565616488456726, 0.3722485303878784, 0.33895236253738403, 0.3595709204673767, 0.35427695512771606, 0.3750348389148712, 0.3439676761627197, 0.334354966878891, 0.3553914725780487, 0.2559208571910858, 0.3527444899082184, 0.3407634496688843, 0.3130398392677307, 0.3852047920227051, 0.3258567750453949, 0.30718863010406494, 0.3172192871570587, 0.3151295483112335, 0.32529953122138977, 0.37977153062820435, 0.32557815313339233, 0.3386737108230591, 0.37489551305770874, 0.34020617604255676, 0.3505154550075531, 0.35789912939071655, 0.2968793511390686, 0.2896350026130676, 0.37336304783821106, 0.3411813974380493, 0.322931170463562, 0.3779604434967041, 0.21774867177009583, 0.32474225759506226, 0.33950960636138916, 0.2348843663930893, 0.37001949548721313, 0.30927833914756775, 0.3836723268032074, 0.34173864126205444, 0.3626358211040497, 0.3736416697502136, 0.30913904309272766, 0.29590415954589844, 0.3129005432128906, 0.3573418855667114, 0.3378378450870514, 0.3633323907852173, 0.34800779819488525, 0.2688771188259125, 0.3569239377975464, 0.34940093755722046, 0.3780997395515442, 0.3216773569583893, 0.35845640301704407, 0.3397882282733917, 0.3370019495487213, 0.37559208273887634, 0.3645862340927124, 0.16843131184577942, 0.32181665301322937, 0.36904430389404297, 0.3716912865638733, 0.35998886823654175, 0.3838116526603699, 0.3193089962005615, 0.31972694396972656, 0.33811646699905396, 0.3744775652885437, 0.303427129983902, 0.2984118163585663, 0.3757314085960388, 0.2206742763519287, 0.3404848277568817, 0.37907496094703674, 0.32752856612205505, 0.32070213556289673, 0.34159934520721436, 0.3186124265193939, 0.3257174789905548, 0.35789912939071655, 0.3083031475543976, 0.32697129249572754, 0.3884090185165405, 0.30774590373039246, 0.3230704963207245, 0.380886048078537, 0.3333797752857208, 0.3442462980747223, 0.35009750723838806, 0.3794929087162018, 0.32543882727622986, 0.3138757348060608, 0.3475898504257202, 0.3172192871570587, 0.3758707046508789, 0.3737809956073761, 0.3581777513027191, 0.35720255970954895, 0.2932571768760681, 0.33477291464805603, 0.3865979313850403, 0.3227918744087219, 0.33616605401039124, 0.34870436787605286, 0.2751462757587433, 0.33672332763671875, 0.3810253441333771, 0.3668152689933777, 0.36500418186187744, 0.3137364089488983, 0.35511285066604614, 0.36862635612487793, 0.2913067638874054, 0.39258846640586853, 0.3243243098258972, 0.30788519978523254, 0.3753134608268738, 0.36291447281837463, 0.30217331647872925, 0.3845082223415375, 0.35859569907188416, 0.34814712405204773, 0.3411813974380493, 0.35636666417121887, 0.3631930947303772, 0.400390088558197, 0.34466424584388733, 0.3661186993122101, 0.3392309844493866, 0.38910558819770813, 0.361799955368042, 0.377124547958374, 0.36834773421287537, 0.3513513505458832, 0.37280580401420593, 0.3573418855667114, 0.3548342287540436, 0.38255780935287476, 0.38186123967170715, 0.35873502492904663, 0.3704374432563782, 0.34522151947021484, 0.36695459485054016, 0.35998886823654175, 0.3421565890312195, 0.3533017635345459, 0.3638896644115448, 0.30147674679756165, 0.3859013617038727, 0.3630537688732147, 0.3569239377975464, 0.3067706823348999, 0.2842017412185669, 0.342017263174057, 0.36277514696121216, 0.3173585832118988, 0.3456394672393799, 0.34034550189971924, 0.3884090185165405, 0.3418779671192169, 0.37921425700187683, 0.3865979313850403, 0.3800501525402069, 0.3567846119403839, 0.32474225759506226, 0.3130398392677307, 0.37266647815704346, 0.35803845524787903, 0.3526051938533783, 0.38757315278053284, 0.25996097922325134, 0.39105600118637085, 0.3624965250492096, 0.3569239377975464, 0.3679297864437103, 0.36347171664237976, 0.3675118386745453, 0.3519086241722107, 0.3126218914985657, 0.3718305826187134, 0.384368896484375, 0.30133742094039917, 0.31220394372940063, 0.38186123967170715, 0.376706600189209, 0.32139870524406433, 0.2942323684692383, 0.3714126646518707, 0.3555307984352112, 0.3984396755695343, 0.32404568791389465, 0.3087210953235626, 0.31694066524505615, 0.3645862340927124, 0.3772638738155365, 0.37322375178337097, 0.37127333879470825, 0.35580942034721375, 0.28113681077957153, 0.36207857728004456, 0.3637503385543823, 0.34508219361305237, 0.40234047174453735, 0.3753134608268738, 0.30161604285240173, 0.37322375178337097, 0.32390639185905457, 0.34090274572372437, 0.35706326365470886, 0.36012816429138184, 0.3237670660018921, 0.32181665301322937, 0.3630537688732147, 0.4069378674030304, 0.36695459485054016, 0.3073279559612274, 0.34382835030555725, 0.3329618275165558, 0.36347171664237976, 0.36848703026771545, 0.32752856612205505, 0.2765394151210785, 0.28239062428474426, 0.36695459485054016, 0.34368905425071716, 0.3751741349697113, 0.37768179178237915, 0.3772638738155365, 0.37350237369537354, 0.3871552050113678, 0.3715519607067108, 0.3737809956073761, 0.3428531587123871, 0.3800501525402069, 0.36068543791770935, 0.3020339906215668, 0.36152130365371704, 0.3708553910255432, 0.27598217129707336, 0.334354966878891, 0.3574811816215515, 0.4012259542942047, 0.31415435671806335, 0.3659793734550476, 0.3432711064815521, 0.3978824317455292, 0.3583170771598816, 0.3595709204673767, 0.3356088101863861, 0.3524658679962158, 0.3390916585922241, 0.35929226875305176, 0.3644469082355499, 0.35218724608421326, 0.3633323907852173, 0.3342156708240509, 0.372945100069046, 0.2607968747615814, 0.37851768732070923, 0.3335190713405609, 0.3223739266395569, 0.36695459485054016, 0.3711340129375458, 0.38882696628570557, 0.39328503608703613, 0.3764279782772064, 0.3914739489555359, 0.36221787333488464, 0.3609640598297119, 0.2637224793434143, 0.36152130365371704, 0.3477291762828827, 0.3581777513027191, 0.3872945010662079, 0.39955419301986694, 0.35859569907188416, 0.3857620358467102, 0.33895236253738403, 0.3779604434967041, 0.3845082223415375, 0.2921426594257355, 0.34173864126205444, 0.37559208273887634, 0.3124825954437256, 0.3872945010662079, 0.3356088101863861, 0.3948175013065338, 0.3668152689933777, 0.35873502492904663, 0.37489551305770874, 0.3871552050113678, 0.30649206042289734, 0.33741989731788635, 0.3928670883178711, 0.39607131481170654, 0.3520479202270508, 0.35636666417121887, 0.3187517523765564, 0.2989690601825714, 0.28392308950424194, 0.3672332167625427, 0.3612426817417145, 0.3158261477947235, 0.3850654661655426, 0.38757315278053284, 0.40080803632736206, 0.3976037800312042, 0.357620507478714, 0.37893563508987427, 0.3025912642478943, 0.34661465883255005, 0.3956533968448639, 0.35929226875305176, 0.3779604434967041, 0.39370298385620117, 0.3877124488353729, 0.37350237369537354, 0.3188910484313965, 0.3604067862033844, 0.3573418855667114, 0.3845082223415375, 0.3583170771598816, 0.27612149715423584, 0.376706600189209, 0.40373364090919495, 0.35511285066604614, 0.3005015254020691, 0.3836723268032074, 0.31638339161872864, 0.3911953270435333, 0.3865979313850403, 0.3595709204673767, 0.37851768732070923, 0.31833380460739136, 0.4040122628211975, 0.28044024109840393, 0.2890777289867401, 0.36570075154304504, 0.37991082668304443, 0.3800501525402069, 0.36207857728004456, 0.3696015477180481, 0.35636666417121887, 0.2935357987880707, 0.3750348389148712, 0.37754249572753906, 0.3817219138145447, 0.39398160576820374, 0.36974087357521057, 0.38241851329803467, 0.40094733238220215, 0.3918918967247009, 0.34731122851371765, 0.32683199644088745, 0.35427695512771606, 0.40094733238220215, 0.3560880422592163, 0.36291447281837463, 0.3899414837360382, 0.36988019943237305, 0.38325437903404236, 0.38339370489120483, 0.38757315278053284, 0.37921425700187683, 0.3828364312648773, 0.31150737404823303, 0.3857620358467102, 0.3836723268032074, 0.357620507478714, 0.3913346230983734, 0.37991082668304443, 0.3800501525402069, 0.37266647815704346, 0.38882696628570557, 0.3941209316253662, 0.36904430389404297, 0.3736416697502136, 0.3512120246887207, 0.38534411787986755, 0.3941209316253662, 0.35218724608421326, 0.35302311182022095, 0.34661465883255005, 0.3630537688732147, 0.34466424584388733, 0.3723878562450409, 0.38813039660453796, 0.40387293696403503, 0.330593466758728, 0.3991362452507019, 0.37057676911354065, 0.36012816429138184, 0.34173864126205444, 0.4049874544143677, 0.39244914054870605, 0.39175257086753845, 0.33045417070388794, 0.37127333879470825, 0.3365840017795563, 0.33755922317504883, 0.32557815313339233]}\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import (Dense, Flatten, BatchNormalization, \n",
    "                                     Dropout, Conv2D, MaxPooling2D)\n",
    "from tensorflow.keras.applications import VGG19\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "# Folder path for training images\n",
    "folder_path = \"images/train\"  \n",
    "\n",
    "# Image specifications\n",
    "picture_size = 48  # Define picture size\n",
    "batch_size = 128\n",
    "\n",
    "# Data Generators with augmentation for training set\n",
    "datagen_train = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Normalization for validation set (no augmentation)\n",
    "datagen_val = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Create training and validation data sets\n",
    "train_set = datagen_train.flow_from_directory(\n",
    "    folder_path,\n",
    "    target_size=(picture_size, picture_size),  # Use picture_size variable\n",
    "    color_mode=\"rgb\",  # Ensure RGB input\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Corrected test set creation\n",
    "test_set = datagen_val.flow_from_directory(\n",
    "    \"images/test\",  # Added a comma at the end of this line\n",
    "    target_size=(picture_size, picture_size),  # Use picture_size variable\n",
    "    color_mode=\"rgb\",  # Ensure RGB input\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "print('Train and Validation sets have been created.')\n",
    "\n",
    "\n",
    "\n",
    "from keras.applications.resnet import ResNet152\n",
    "base_model = ResNet152(\n",
    "    weights='imagenet',\n",
    "    include_top = False,\n",
    "    input_shape=(48,48,3)\n",
    ")\n",
    "\n",
    "# Show the model summary\n",
    "base_model.summary()\n",
    "\n",
    "# Create a Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add base model\n",
    "model.add(base_model)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.30))\n",
    "# Second fully connected layer  \n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.30))\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "# Set base model to not trainable\n",
    "base_model.trainable = False\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "print('CNN model has been created, you can proceed to train your data with this model.')\n",
    "\n",
    "# Training the model\n",
    "epochs = 500\n",
    "\n",
    "history = model.fit(\n",
    "    train_set,\n",
    "    epochs=epochs,\n",
    "    validation_data=test_set\n",
    ")\n",
    "\n",
    "# Print training history\n",
    "print(history.history)\n",
    "\n",
    "# Plotting training history\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Loss Plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Accuracy Plot\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8275527",
   "metadata": {},
   "outputs": [],
   "source": [
    "#exp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5058593",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-12 10:54:59.630071: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-10-12 10:54:59.660127: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-12 10:55:00.234996: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28709 images belonging to 7 classes.\n",
      "Found 7178 images belonging to 7 classes.\n",
      "Train and Validation sets have been created.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-12 10:55:01.454333: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-10-12 10:55:01.498811: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-10-12 10:55:01.498973: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-10-12 10:55:01.499668: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-10-12 10:55:01.499782: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-10-12 10:55:01.499854: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-10-12 10:55:01.878447: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-10-12 10:55:01.878609: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-10-12 10:55:01.878705: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-10-12 10:55:01.878795: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 685 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3070 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"resnet152\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 48, 48, 3)]          0         []                            \n",
      "                                                                                                  \n",
      " conv1_pad (ZeroPadding2D)   (None, 54, 54, 3)            0         ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " conv1_conv (Conv2D)         (None, 24, 24, 64)           9472      ['conv1_pad[0][0]']           \n",
      "                                                                                                  \n",
      " conv1_bn (BatchNormalizati  (None, 24, 24, 64)           256       ['conv1_conv[0][0]']          \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv1_relu (Activation)     (None, 24, 24, 64)           0         ['conv1_bn[0][0]']            \n",
      "                                                                                                  \n",
      " pool1_pad (ZeroPadding2D)   (None, 26, 26, 64)           0         ['conv1_relu[0][0]']          \n",
      "                                                                                                  \n",
      " pool1_pool (MaxPooling2D)   (None, 12, 12, 64)           0         ['pool1_pad[0][0]']           \n",
      "                                                                                                  \n",
      " conv2_block1_1_conv (Conv2  (None, 12, 12, 64)           4160      ['pool1_pool[0][0]']          \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block1_1_bn (BatchNo  (None, 12, 12, 64)           256       ['conv2_block1_1_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv2_block1_1_relu (Activ  (None, 12, 12, 64)           0         ['conv2_block1_1_bn[0][0]']   \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " conv2_block1_2_conv (Conv2  (None, 12, 12, 64)           36928     ['conv2_block1_1_relu[0][0]'] \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block1_2_bn (BatchNo  (None, 12, 12, 64)           256       ['conv2_block1_2_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv2_block1_2_relu (Activ  (None, 12, 12, 64)           0         ['conv2_block1_2_bn[0][0]']   \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " conv2_block1_0_conv (Conv2  (None, 12, 12, 256)          16640     ['pool1_pool[0][0]']          \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block1_3_conv (Conv2  (None, 12, 12, 256)          16640     ['conv2_block1_2_relu[0][0]'] \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block1_0_bn (BatchNo  (None, 12, 12, 256)          1024      ['conv2_block1_0_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv2_block1_3_bn (BatchNo  (None, 12, 12, 256)          1024      ['conv2_block1_3_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv2_block1_add (Add)      (None, 12, 12, 256)          0         ['conv2_block1_0_bn[0][0]',   \n",
      "                                                                     'conv2_block1_3_bn[0][0]']   \n",
      "                                                                                                  \n",
      " conv2_block1_out (Activati  (None, 12, 12, 256)          0         ['conv2_block1_add[0][0]']    \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv2_block2_1_conv (Conv2  (None, 12, 12, 64)           16448     ['conv2_block1_out[0][0]']    \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block2_1_bn (BatchNo  (None, 12, 12, 64)           256       ['conv2_block2_1_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv2_block2_1_relu (Activ  (None, 12, 12, 64)           0         ['conv2_block2_1_bn[0][0]']   \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " conv2_block2_2_conv (Conv2  (None, 12, 12, 64)           36928     ['conv2_block2_1_relu[0][0]'] \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block2_2_bn (BatchNo  (None, 12, 12, 64)           256       ['conv2_block2_2_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv2_block2_2_relu (Activ  (None, 12, 12, 64)           0         ['conv2_block2_2_bn[0][0]']   \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " conv2_block2_3_conv (Conv2  (None, 12, 12, 256)          16640     ['conv2_block2_2_relu[0][0]'] \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block2_3_bn (BatchNo  (None, 12, 12, 256)          1024      ['conv2_block2_3_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv2_block2_add (Add)      (None, 12, 12, 256)          0         ['conv2_block1_out[0][0]',    \n",
      "                                                                     'conv2_block2_3_bn[0][0]']   \n",
      "                                                                                                  \n",
      " conv2_block2_out (Activati  (None, 12, 12, 256)          0         ['conv2_block2_add[0][0]']    \n",
      " on)                                                                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " conv2_block3_1_conv (Conv2  (None, 12, 12, 64)           16448     ['conv2_block2_out[0][0]']    \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block3_1_bn (BatchNo  (None, 12, 12, 64)           256       ['conv2_block3_1_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv2_block3_1_relu (Activ  (None, 12, 12, 64)           0         ['conv2_block3_1_bn[0][0]']   \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " conv2_block3_2_conv (Conv2  (None, 12, 12, 64)           36928     ['conv2_block3_1_relu[0][0]'] \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block3_2_bn (BatchNo  (None, 12, 12, 64)           256       ['conv2_block3_2_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv2_block3_2_relu (Activ  (None, 12, 12, 64)           0         ['conv2_block3_2_bn[0][0]']   \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " conv2_block3_3_conv (Conv2  (None, 12, 12, 256)          16640     ['conv2_block3_2_relu[0][0]'] \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block3_3_bn (BatchNo  (None, 12, 12, 256)          1024      ['conv2_block3_3_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv2_block3_add (Add)      (None, 12, 12, 256)          0         ['conv2_block2_out[0][0]',    \n",
      "                                                                     'conv2_block3_3_bn[0][0]']   \n",
      "                                                                                                  \n",
      " conv2_block3_out (Activati  (None, 12, 12, 256)          0         ['conv2_block3_add[0][0]']    \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv3_block1_1_conv (Conv2  (None, 6, 6, 128)            32896     ['conv2_block3_out[0][0]']    \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block1_1_bn (BatchNo  (None, 6, 6, 128)            512       ['conv3_block1_1_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv3_block1_1_relu (Activ  (None, 6, 6, 128)            0         ['conv3_block1_1_bn[0][0]']   \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " conv3_block1_2_conv (Conv2  (None, 6, 6, 128)            147584    ['conv3_block1_1_relu[0][0]'] \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block1_2_bn (BatchNo  (None, 6, 6, 128)            512       ['conv3_block1_2_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv3_block1_2_relu (Activ  (None, 6, 6, 128)            0         ['conv3_block1_2_bn[0][0]']   \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " conv3_block1_0_conv (Conv2  (None, 6, 6, 512)            131584    ['conv2_block3_out[0][0]']    \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block1_3_conv (Conv2  (None, 6, 6, 512)            66048     ['conv3_block1_2_relu[0][0]'] \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block1_0_bn (BatchNo  (None, 6, 6, 512)            2048      ['conv3_block1_0_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv3_block1_3_bn (BatchNo  (None, 6, 6, 512)            2048      ['conv3_block1_3_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv3_block1_add (Add)      (None, 6, 6, 512)            0         ['conv3_block1_0_bn[0][0]',   \n",
      "                                                                     'conv3_block1_3_bn[0][0]']   \n",
      "                                                                                                  \n",
      " conv3_block1_out (Activati  (None, 6, 6, 512)            0         ['conv3_block1_add[0][0]']    \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv3_block2_1_conv (Conv2  (None, 6, 6, 128)            65664     ['conv3_block1_out[0][0]']    \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block2_1_bn (BatchNo  (None, 6, 6, 128)            512       ['conv3_block2_1_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv3_block2_1_relu (Activ  (None, 6, 6, 128)            0         ['conv3_block2_1_bn[0][0]']   \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " conv3_block2_2_conv (Conv2  (None, 6, 6, 128)            147584    ['conv3_block2_1_relu[0][0]'] \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block2_2_bn (BatchNo  (None, 6, 6, 128)            512       ['conv3_block2_2_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv3_block2_2_relu (Activ  (None, 6, 6, 128)            0         ['conv3_block2_2_bn[0][0]']   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " conv3_block2_3_conv (Conv2  (None, 6, 6, 512)            66048     ['conv3_block2_2_relu[0][0]'] \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block2_3_bn (BatchNo  (None, 6, 6, 512)            2048      ['conv3_block2_3_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv3_block2_add (Add)      (None, 6, 6, 512)            0         ['conv3_block1_out[0][0]',    \n",
      "                                                                     'conv3_block2_3_bn[0][0]']   \n",
      "                                                                                                  \n",
      " conv3_block2_out (Activati  (None, 6, 6, 512)            0         ['conv3_block2_add[0][0]']    \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv3_block3_1_conv (Conv2  (None, 6, 6, 128)            65664     ['conv3_block2_out[0][0]']    \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block3_1_bn (BatchNo  (None, 6, 6, 128)            512       ['conv3_block3_1_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv3_block3_1_relu (Activ  (None, 6, 6, 128)            0         ['conv3_block3_1_bn[0][0]']   \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " conv3_block3_2_conv (Conv2  (None, 6, 6, 128)            147584    ['conv3_block3_1_relu[0][0]'] \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block3_2_bn (BatchNo  (None, 6, 6, 128)            512       ['conv3_block3_2_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv3_block3_2_relu (Activ  (None, 6, 6, 128)            0         ['conv3_block3_2_bn[0][0]']   \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " conv3_block3_3_conv (Conv2  (None, 6, 6, 512)            66048     ['conv3_block3_2_relu[0][0]'] \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block3_3_bn (BatchNo  (None, 6, 6, 512)            2048      ['conv3_block3_3_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv3_block3_add (Add)      (None, 6, 6, 512)            0         ['conv3_block2_out[0][0]',    \n",
      "                                                                     'conv3_block3_3_bn[0][0]']   \n",
      "                                                                                                  \n",
      " conv3_block3_out (Activati  (None, 6, 6, 512)            0         ['conv3_block3_add[0][0]']    \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv3_block4_1_conv (Conv2  (None, 6, 6, 128)            65664     ['conv3_block3_out[0][0]']    \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block4_1_bn (BatchNo  (None, 6, 6, 128)            512       ['conv3_block4_1_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv3_block4_1_relu (Activ  (None, 6, 6, 128)            0         ['conv3_block4_1_bn[0][0]']   \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " conv3_block4_2_conv (Conv2  (None, 6, 6, 128)            147584    ['conv3_block4_1_relu[0][0]'] \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block4_2_bn (BatchNo  (None, 6, 6, 128)            512       ['conv3_block4_2_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv3_block4_2_relu (Activ  (None, 6, 6, 128)            0         ['conv3_block4_2_bn[0][0]']   \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " conv3_block4_3_conv (Conv2  (None, 6, 6, 512)            66048     ['conv3_block4_2_relu[0][0]'] \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block4_3_bn (BatchNo  (None, 6, 6, 512)            2048      ['conv3_block4_3_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv3_block4_add (Add)      (None, 6, 6, 512)            0         ['conv3_block3_out[0][0]',    \n",
      "                                                                     'conv3_block4_3_bn[0][0]']   \n",
      "                                                                                                  \n",
      " conv3_block4_out (Activati  (None, 6, 6, 512)            0         ['conv3_block4_add[0][0]']    \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv3_block5_1_conv (Conv2  (None, 6, 6, 128)            65664     ['conv3_block4_out[0][0]']    \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block5_1_bn (BatchNo  (None, 6, 6, 128)            512       ['conv3_block5_1_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv3_block5_1_relu (Activ  (None, 6, 6, 128)            0         ['conv3_block5_1_bn[0][0]']   \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " conv3_block5_2_conv (Conv2  (None, 6, 6, 128)            147584    ['conv3_block5_1_relu[0][0]'] \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block5_2_bn (BatchNo  (None, 6, 6, 128)            512       ['conv3_block5_2_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv3_block5_2_relu (Activ  (None, 6, 6, 128)            0         ['conv3_block5_2_bn[0][0]']   \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " conv3_block5_3_conv (Conv2  (None, 6, 6, 512)            66048     ['conv3_block5_2_relu[0][0]'] \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block5_3_bn (BatchNo  (None, 6, 6, 512)            2048      ['conv3_block5_3_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv3_block5_add (Add)      (None, 6, 6, 512)            0         ['conv3_block4_out[0][0]',    \n",
      "                                                                     'conv3_block5_3_bn[0][0]']   \n",
      "                                                                                                  \n",
      " conv3_block5_out (Activati  (None, 6, 6, 512)            0         ['conv3_block5_add[0][0]']    \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv3_block6_1_conv (Conv2  (None, 6, 6, 128)            65664     ['conv3_block5_out[0][0]']    \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block6_1_bn (BatchNo  (None, 6, 6, 128)            512       ['conv3_block6_1_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv3_block6_1_relu (Activ  (None, 6, 6, 128)            0         ['conv3_block6_1_bn[0][0]']   \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " conv3_block6_2_conv (Conv2  (None, 6, 6, 128)            147584    ['conv3_block6_1_relu[0][0]'] \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block6_2_bn (BatchNo  (None, 6, 6, 128)            512       ['conv3_block6_2_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv3_block6_2_relu (Activ  (None, 6, 6, 128)            0         ['conv3_block6_2_bn[0][0]']   \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " conv3_block6_3_conv (Conv2  (None, 6, 6, 512)            66048     ['conv3_block6_2_relu[0][0]'] \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block6_3_bn (BatchNo  (None, 6, 6, 512)            2048      ['conv3_block6_3_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv3_block6_add (Add)      (None, 6, 6, 512)            0         ['conv3_block5_out[0][0]',    \n",
      "                                                                     'conv3_block6_3_bn[0][0]']   \n",
      "                                                                                                  \n",
      " conv3_block6_out (Activati  (None, 6, 6, 512)            0         ['conv3_block6_add[0][0]']    \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv3_block7_1_conv (Conv2  (None, 6, 6, 128)            65664     ['conv3_block6_out[0][0]']    \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block7_1_bn (BatchNo  (None, 6, 6, 128)            512       ['conv3_block7_1_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv3_block7_1_relu (Activ  (None, 6, 6, 128)            0         ['conv3_block7_1_bn[0][0]']   \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " conv3_block7_2_conv (Conv2  (None, 6, 6, 128)            147584    ['conv3_block7_1_relu[0][0]'] \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block7_2_bn (BatchNo  (None, 6, 6, 128)            512       ['conv3_block7_2_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv3_block7_2_relu (Activ  (None, 6, 6, 128)            0         ['conv3_block7_2_bn[0][0]']   \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " conv3_block7_3_conv (Conv2  (None, 6, 6, 512)            66048     ['conv3_block7_2_relu[0][0]'] \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block7_3_bn (BatchNo  (None, 6, 6, 512)            2048      ['conv3_block7_3_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv3_block7_add (Add)      (None, 6, 6, 512)            0         ['conv3_block6_out[0][0]',    \n",
      "                                                                     'conv3_block7_3_bn[0][0]']   \n",
      "                                                                                                  \n",
      " conv3_block7_out (Activati  (None, 6, 6, 512)            0         ['conv3_block7_add[0][0]']    \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv3_block8_1_conv (Conv2  (None, 6, 6, 128)            65664     ['conv3_block7_out[0][0]']    \n",
      " D)                                                                                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " conv3_block8_1_bn (BatchNo  (None, 6, 6, 128)            512       ['conv3_block8_1_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv3_block8_1_relu (Activ  (None, 6, 6, 128)            0         ['conv3_block8_1_bn[0][0]']   \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " conv3_block8_2_conv (Conv2  (None, 6, 6, 128)            147584    ['conv3_block8_1_relu[0][0]'] \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block8_2_bn (BatchNo  (None, 6, 6, 128)            512       ['conv3_block8_2_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv3_block8_2_relu (Activ  (None, 6, 6, 128)            0         ['conv3_block8_2_bn[0][0]']   \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " conv3_block8_3_conv (Conv2  (None, 6, 6, 512)            66048     ['conv3_block8_2_relu[0][0]'] \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block8_3_bn (BatchNo  (None, 6, 6, 512)            2048      ['conv3_block8_3_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv3_block8_add (Add)      (None, 6, 6, 512)            0         ['conv3_block7_out[0][0]',    \n",
      "                                                                     'conv3_block8_3_bn[0][0]']   \n",
      "                                                                                                  \n",
      " conv3_block8_out (Activati  (None, 6, 6, 512)            0         ['conv3_block8_add[0][0]']    \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block1_1_conv (Conv2  (None, 3, 3, 256)            131328    ['conv3_block8_out[0][0]']    \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block1_1_bn (BatchNo  (None, 3, 3, 256)            1024      ['conv4_block1_1_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv4_block1_1_relu (Activ  (None, 3, 3, 256)            0         ['conv4_block1_1_bn[0][0]']   \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " conv4_block1_2_conv (Conv2  (None, 3, 3, 256)            590080    ['conv4_block1_1_relu[0][0]'] \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block1_2_bn (BatchNo  (None, 3, 3, 256)            1024      ['conv4_block1_2_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv4_block1_2_relu (Activ  (None, 3, 3, 256)            0         ['conv4_block1_2_bn[0][0]']   \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " conv4_block1_0_conv (Conv2  (None, 3, 3, 1024)           525312    ['conv3_block8_out[0][0]']    \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block1_3_conv (Conv2  (None, 3, 3, 1024)           263168    ['conv4_block1_2_relu[0][0]'] \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block1_0_bn (BatchNo  (None, 3, 3, 1024)           4096      ['conv4_block1_0_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv4_block1_3_bn (BatchNo  (None, 3, 3, 1024)           4096      ['conv4_block1_3_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv4_block1_add (Add)      (None, 3, 3, 1024)           0         ['conv4_block1_0_bn[0][0]',   \n",
      "                                                                     'conv4_block1_3_bn[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block1_out (Activati  (None, 3, 3, 1024)           0         ['conv4_block1_add[0][0]']    \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block2_1_conv (Conv2  (None, 3, 3, 256)            262400    ['conv4_block1_out[0][0]']    \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block2_1_bn (BatchNo  (None, 3, 3, 256)            1024      ['conv4_block2_1_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv4_block2_1_relu (Activ  (None, 3, 3, 256)            0         ['conv4_block2_1_bn[0][0]']   \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " conv4_block2_2_conv (Conv2  (None, 3, 3, 256)            590080    ['conv4_block2_1_relu[0][0]'] \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block2_2_bn (BatchNo  (None, 3, 3, 256)            1024      ['conv4_block2_2_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv4_block2_2_relu (Activ  (None, 3, 3, 256)            0         ['conv4_block2_2_bn[0][0]']   \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " conv4_block2_3_conv (Conv2  (None, 3, 3, 1024)           263168    ['conv4_block2_2_relu[0][0]'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block2_3_bn (BatchNo  (None, 3, 3, 1024)           4096      ['conv4_block2_3_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv4_block2_add (Add)      (None, 3, 3, 1024)           0         ['conv4_block1_out[0][0]',    \n",
      "                                                                     'conv4_block2_3_bn[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block2_out (Activati  (None, 3, 3, 1024)           0         ['conv4_block2_add[0][0]']    \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block3_1_conv (Conv2  (None, 3, 3, 256)            262400    ['conv4_block2_out[0][0]']    \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block3_1_bn (BatchNo  (None, 3, 3, 256)            1024      ['conv4_block3_1_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv4_block3_1_relu (Activ  (None, 3, 3, 256)            0         ['conv4_block3_1_bn[0][0]']   \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " conv4_block3_2_conv (Conv2  (None, 3, 3, 256)            590080    ['conv4_block3_1_relu[0][0]'] \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block3_2_bn (BatchNo  (None, 3, 3, 256)            1024      ['conv4_block3_2_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv4_block3_2_relu (Activ  (None, 3, 3, 256)            0         ['conv4_block3_2_bn[0][0]']   \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " conv4_block3_3_conv (Conv2  (None, 3, 3, 1024)           263168    ['conv4_block3_2_relu[0][0]'] \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block3_3_bn (BatchNo  (None, 3, 3, 1024)           4096      ['conv4_block3_3_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv4_block3_add (Add)      (None, 3, 3, 1024)           0         ['conv4_block2_out[0][0]',    \n",
      "                                                                     'conv4_block3_3_bn[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block3_out (Activati  (None, 3, 3, 1024)           0         ['conv4_block3_add[0][0]']    \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block4_1_conv (Conv2  (None, 3, 3, 256)            262400    ['conv4_block3_out[0][0]']    \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block4_1_bn (BatchNo  (None, 3, 3, 256)            1024      ['conv4_block4_1_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv4_block4_1_relu (Activ  (None, 3, 3, 256)            0         ['conv4_block4_1_bn[0][0]']   \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " conv4_block4_2_conv (Conv2  (None, 3, 3, 256)            590080    ['conv4_block4_1_relu[0][0]'] \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block4_2_bn (BatchNo  (None, 3, 3, 256)            1024      ['conv4_block4_2_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv4_block4_2_relu (Activ  (None, 3, 3, 256)            0         ['conv4_block4_2_bn[0][0]']   \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " conv4_block4_3_conv (Conv2  (None, 3, 3, 1024)           263168    ['conv4_block4_2_relu[0][0]'] \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block4_3_bn (BatchNo  (None, 3, 3, 1024)           4096      ['conv4_block4_3_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv4_block4_add (Add)      (None, 3, 3, 1024)           0         ['conv4_block3_out[0][0]',    \n",
      "                                                                     'conv4_block4_3_bn[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block4_out (Activati  (None, 3, 3, 1024)           0         ['conv4_block4_add[0][0]']    \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block5_1_conv (Conv2  (None, 3, 3, 256)            262400    ['conv4_block4_out[0][0]']    \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block5_1_bn (BatchNo  (None, 3, 3, 256)            1024      ['conv4_block5_1_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv4_block5_1_relu (Activ  (None, 3, 3, 256)            0         ['conv4_block5_1_bn[0][0]']   \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " conv4_block5_2_conv (Conv2  (None, 3, 3, 256)            590080    ['conv4_block5_1_relu[0][0]'] \n",
      " D)                                                                                               \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " conv4_block5_2_bn (BatchNo  (None, 3, 3, 256)            1024      ['conv4_block5_2_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv4_block5_2_relu (Activ  (None, 3, 3, 256)            0         ['conv4_block5_2_bn[0][0]']   \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " conv4_block5_3_conv (Conv2  (None, 3, 3, 1024)           263168    ['conv4_block5_2_relu[0][0]'] \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block5_3_bn (BatchNo  (None, 3, 3, 1024)           4096      ['conv4_block5_3_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv4_block5_add (Add)      (None, 3, 3, 1024)           0         ['conv4_block4_out[0][0]',    \n",
      "                                                                     'conv4_block5_3_bn[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block5_out (Activati  (None, 3, 3, 1024)           0         ['conv4_block5_add[0][0]']    \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block6_1_conv (Conv2  (None, 3, 3, 256)            262400    ['conv4_block5_out[0][0]']    \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block6_1_bn (BatchNo  (None, 3, 3, 256)            1024      ['conv4_block6_1_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv4_block6_1_relu (Activ  (None, 3, 3, 256)            0         ['conv4_block6_1_bn[0][0]']   \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " conv4_block6_2_conv (Conv2  (None, 3, 3, 256)            590080    ['conv4_block6_1_relu[0][0]'] \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block6_2_bn (BatchNo  (None, 3, 3, 256)            1024      ['conv4_block6_2_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv4_block6_2_relu (Activ  (None, 3, 3, 256)            0         ['conv4_block6_2_bn[0][0]']   \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " conv4_block6_3_conv (Conv2  (None, 3, 3, 1024)           263168    ['conv4_block6_2_relu[0][0]'] \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block6_3_bn (BatchNo  (None, 3, 3, 1024)           4096      ['conv4_block6_3_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv4_block6_add (Add)      (None, 3, 3, 1024)           0         ['conv4_block5_out[0][0]',    \n",
      "                                                                     'conv4_block6_3_bn[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block6_out (Activati  (None, 3, 3, 1024)           0         ['conv4_block6_add[0][0]']    \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block7_1_conv (Conv2  (None, 3, 3, 256)            262400    ['conv4_block6_out[0][0]']    \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block7_1_bn (BatchNo  (None, 3, 3, 256)            1024      ['conv4_block7_1_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv4_block7_1_relu (Activ  (None, 3, 3, 256)            0         ['conv4_block7_1_bn[0][0]']   \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " conv4_block7_2_conv (Conv2  (None, 3, 3, 256)            590080    ['conv4_block7_1_relu[0][0]'] \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block7_2_bn (BatchNo  (None, 3, 3, 256)            1024      ['conv4_block7_2_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv4_block7_2_relu (Activ  (None, 3, 3, 256)            0         ['conv4_block7_2_bn[0][0]']   \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " conv4_block7_3_conv (Conv2  (None, 3, 3, 1024)           263168    ['conv4_block7_2_relu[0][0]'] \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block7_3_bn (BatchNo  (None, 3, 3, 1024)           4096      ['conv4_block7_3_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv4_block7_add (Add)      (None, 3, 3, 1024)           0         ['conv4_block6_out[0][0]',    \n",
      "                                                                     'conv4_block7_3_bn[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block7_out (Activati  (None, 3, 3, 1024)           0         ['conv4_block7_add[0][0]']    \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block8_1_conv (Conv2  (None, 3, 3, 256)            262400    ['conv4_block7_out[0][0]']    \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block8_1_bn (BatchNo  (None, 3, 3, 256)            1024      ['conv4_block8_1_conv[0][0]'] \n",
      " rmalization)                                                                                     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " conv4_block8_1_relu (Activ  (None, 3, 3, 256)            0         ['conv4_block8_1_bn[0][0]']   \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " conv4_block8_2_conv (Conv2  (None, 3, 3, 256)            590080    ['conv4_block8_1_relu[0][0]'] \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block8_2_bn (BatchNo  (None, 3, 3, 256)            1024      ['conv4_block8_2_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv4_block8_2_relu (Activ  (None, 3, 3, 256)            0         ['conv4_block8_2_bn[0][0]']   \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " conv4_block8_3_conv (Conv2  (None, 3, 3, 1024)           263168    ['conv4_block8_2_relu[0][0]'] \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block8_3_bn (BatchNo  (None, 3, 3, 1024)           4096      ['conv4_block8_3_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv4_block8_add (Add)      (None, 3, 3, 1024)           0         ['conv4_block7_out[0][0]',    \n",
      "                                                                     'conv4_block8_3_bn[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block8_out (Activati  (None, 3, 3, 1024)           0         ['conv4_block8_add[0][0]']    \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block9_1_conv (Conv2  (None, 3, 3, 256)            262400    ['conv4_block8_out[0][0]']    \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block9_1_bn (BatchNo  (None, 3, 3, 256)            1024      ['conv4_block9_1_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv4_block9_1_relu (Activ  (None, 3, 3, 256)            0         ['conv4_block9_1_bn[0][0]']   \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " conv4_block9_2_conv (Conv2  (None, 3, 3, 256)            590080    ['conv4_block9_1_relu[0][0]'] \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block9_2_bn (BatchNo  (None, 3, 3, 256)            1024      ['conv4_block9_2_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv4_block9_2_relu (Activ  (None, 3, 3, 256)            0         ['conv4_block9_2_bn[0][0]']   \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " conv4_block9_3_conv (Conv2  (None, 3, 3, 1024)           263168    ['conv4_block9_2_relu[0][0]'] \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block9_3_bn (BatchNo  (None, 3, 3, 1024)           4096      ['conv4_block9_3_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv4_block9_add (Add)      (None, 3, 3, 1024)           0         ['conv4_block8_out[0][0]',    \n",
      "                                                                     'conv4_block9_3_bn[0][0]']   \n",
      "                                                                                                  \n",
      " conv4_block9_out (Activati  (None, 3, 3, 1024)           0         ['conv4_block9_add[0][0]']    \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block10_1_conv (Conv  (None, 3, 3, 256)            262400    ['conv4_block9_out[0][0]']    \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block10_1_bn (BatchN  (None, 3, 3, 256)            1024      ['conv4_block10_1_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block10_1_relu (Acti  (None, 3, 3, 256)            0         ['conv4_block10_1_bn[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv4_block10_2_conv (Conv  (None, 3, 3, 256)            590080    ['conv4_block10_1_relu[0][0]']\n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block10_2_bn (BatchN  (None, 3, 3, 256)            1024      ['conv4_block10_2_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block10_2_relu (Acti  (None, 3, 3, 256)            0         ['conv4_block10_2_bn[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv4_block10_3_conv (Conv  (None, 3, 3, 1024)           263168    ['conv4_block10_2_relu[0][0]']\n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block10_3_bn (BatchN  (None, 3, 3, 1024)           4096      ['conv4_block10_3_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block10_add (Add)     (None, 3, 3, 1024)           0         ['conv4_block9_out[0][0]',    \n",
      "                                                                     'conv4_block10_3_bn[0][0]']  \n",
      "                                                                                                  \n",
      " conv4_block10_out (Activat  (None, 3, 3, 1024)           0         ['conv4_block10_add[0][0]']   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ion)                                                                                             \n",
      "                                                                                                  \n",
      " conv4_block11_1_conv (Conv  (None, 3, 3, 256)            262400    ['conv4_block10_out[0][0]']   \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block11_1_bn (BatchN  (None, 3, 3, 256)            1024      ['conv4_block11_1_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block11_1_relu (Acti  (None, 3, 3, 256)            0         ['conv4_block11_1_bn[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv4_block11_2_conv (Conv  (None, 3, 3, 256)            590080    ['conv4_block11_1_relu[0][0]']\n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block11_2_bn (BatchN  (None, 3, 3, 256)            1024      ['conv4_block11_2_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block11_2_relu (Acti  (None, 3, 3, 256)            0         ['conv4_block11_2_bn[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv4_block11_3_conv (Conv  (None, 3, 3, 1024)           263168    ['conv4_block11_2_relu[0][0]']\n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block11_3_bn (BatchN  (None, 3, 3, 1024)           4096      ['conv4_block11_3_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block11_add (Add)     (None, 3, 3, 1024)           0         ['conv4_block10_out[0][0]',   \n",
      "                                                                     'conv4_block11_3_bn[0][0]']  \n",
      "                                                                                                  \n",
      " conv4_block11_out (Activat  (None, 3, 3, 1024)           0         ['conv4_block11_add[0][0]']   \n",
      " ion)                                                                                             \n",
      "                                                                                                  \n",
      " conv4_block12_1_conv (Conv  (None, 3, 3, 256)            262400    ['conv4_block11_out[0][0]']   \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block12_1_bn (BatchN  (None, 3, 3, 256)            1024      ['conv4_block12_1_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block12_1_relu (Acti  (None, 3, 3, 256)            0         ['conv4_block12_1_bn[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv4_block12_2_conv (Conv  (None, 3, 3, 256)            590080    ['conv4_block12_1_relu[0][0]']\n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block12_2_bn (BatchN  (None, 3, 3, 256)            1024      ['conv4_block12_2_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block12_2_relu (Acti  (None, 3, 3, 256)            0         ['conv4_block12_2_bn[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv4_block12_3_conv (Conv  (None, 3, 3, 1024)           263168    ['conv4_block12_2_relu[0][0]']\n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block12_3_bn (BatchN  (None, 3, 3, 1024)           4096      ['conv4_block12_3_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block12_add (Add)     (None, 3, 3, 1024)           0         ['conv4_block11_out[0][0]',   \n",
      "                                                                     'conv4_block12_3_bn[0][0]']  \n",
      "                                                                                                  \n",
      " conv4_block12_out (Activat  (None, 3, 3, 1024)           0         ['conv4_block12_add[0][0]']   \n",
      " ion)                                                                                             \n",
      "                                                                                                  \n",
      " conv4_block13_1_conv (Conv  (None, 3, 3, 256)            262400    ['conv4_block12_out[0][0]']   \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block13_1_bn (BatchN  (None, 3, 3, 256)            1024      ['conv4_block13_1_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block13_1_relu (Acti  (None, 3, 3, 256)            0         ['conv4_block13_1_bn[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv4_block13_2_conv (Conv  (None, 3, 3, 256)            590080    ['conv4_block13_1_relu[0][0]']\n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block13_2_bn (BatchN  (None, 3, 3, 256)            1024      ['conv4_block13_2_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block13_2_relu (Acti  (None, 3, 3, 256)            0         ['conv4_block13_2_bn[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv4_block13_3_conv (Conv  (None, 3, 3, 1024)           263168    ['conv4_block13_2_relu[0][0]']\n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " conv4_block13_3_bn (BatchN  (None, 3, 3, 1024)           4096      ['conv4_block13_3_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block13_add (Add)     (None, 3, 3, 1024)           0         ['conv4_block12_out[0][0]',   \n",
      "                                                                     'conv4_block13_3_bn[0][0]']  \n",
      "                                                                                                  \n",
      " conv4_block13_out (Activat  (None, 3, 3, 1024)           0         ['conv4_block13_add[0][0]']   \n",
      " ion)                                                                                             \n",
      "                                                                                                  \n",
      " conv4_block14_1_conv (Conv  (None, 3, 3, 256)            262400    ['conv4_block13_out[0][0]']   \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block14_1_bn (BatchN  (None, 3, 3, 256)            1024      ['conv4_block14_1_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block14_1_relu (Acti  (None, 3, 3, 256)            0         ['conv4_block14_1_bn[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv4_block14_2_conv (Conv  (None, 3, 3, 256)            590080    ['conv4_block14_1_relu[0][0]']\n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block14_2_bn (BatchN  (None, 3, 3, 256)            1024      ['conv4_block14_2_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block14_2_relu (Acti  (None, 3, 3, 256)            0         ['conv4_block14_2_bn[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv4_block14_3_conv (Conv  (None, 3, 3, 1024)           263168    ['conv4_block14_2_relu[0][0]']\n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block14_3_bn (BatchN  (None, 3, 3, 1024)           4096      ['conv4_block14_3_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block14_add (Add)     (None, 3, 3, 1024)           0         ['conv4_block13_out[0][0]',   \n",
      "                                                                     'conv4_block14_3_bn[0][0]']  \n",
      "                                                                                                  \n",
      " conv4_block14_out (Activat  (None, 3, 3, 1024)           0         ['conv4_block14_add[0][0]']   \n",
      " ion)                                                                                             \n",
      "                                                                                                  \n",
      " conv4_block15_1_conv (Conv  (None, 3, 3, 256)            262400    ['conv4_block14_out[0][0]']   \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block15_1_bn (BatchN  (None, 3, 3, 256)            1024      ['conv4_block15_1_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block15_1_relu (Acti  (None, 3, 3, 256)            0         ['conv4_block15_1_bn[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv4_block15_2_conv (Conv  (None, 3, 3, 256)            590080    ['conv4_block15_1_relu[0][0]']\n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block15_2_bn (BatchN  (None, 3, 3, 256)            1024      ['conv4_block15_2_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block15_2_relu (Acti  (None, 3, 3, 256)            0         ['conv4_block15_2_bn[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv4_block15_3_conv (Conv  (None, 3, 3, 1024)           263168    ['conv4_block15_2_relu[0][0]']\n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block15_3_bn (BatchN  (None, 3, 3, 1024)           4096      ['conv4_block15_3_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block15_add (Add)     (None, 3, 3, 1024)           0         ['conv4_block14_out[0][0]',   \n",
      "                                                                     'conv4_block15_3_bn[0][0]']  \n",
      "                                                                                                  \n",
      " conv4_block15_out (Activat  (None, 3, 3, 1024)           0         ['conv4_block15_add[0][0]']   \n",
      " ion)                                                                                             \n",
      "                                                                                                  \n",
      " conv4_block16_1_conv (Conv  (None, 3, 3, 256)            262400    ['conv4_block15_out[0][0]']   \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block16_1_bn (BatchN  (None, 3, 3, 256)            1024      ['conv4_block16_1_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block16_1_relu (Acti  (None, 3, 3, 256)            0         ['conv4_block16_1_bn[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv4_block16_2_conv (Conv  (None, 3, 3, 256)            590080    ['conv4_block16_1_relu[0][0]']\n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block16_2_bn (BatchN  (None, 3, 3, 256)            1024      ['conv4_block16_2_conv[0][0]']\n",
      " ormalization)                                                                                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " conv4_block16_2_relu (Acti  (None, 3, 3, 256)            0         ['conv4_block16_2_bn[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv4_block16_3_conv (Conv  (None, 3, 3, 1024)           263168    ['conv4_block16_2_relu[0][0]']\n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block16_3_bn (BatchN  (None, 3, 3, 1024)           4096      ['conv4_block16_3_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block16_add (Add)     (None, 3, 3, 1024)           0         ['conv4_block15_out[0][0]',   \n",
      "                                                                     'conv4_block16_3_bn[0][0]']  \n",
      "                                                                                                  \n",
      " conv4_block16_out (Activat  (None, 3, 3, 1024)           0         ['conv4_block16_add[0][0]']   \n",
      " ion)                                                                                             \n",
      "                                                                                                  \n",
      " conv4_block17_1_conv (Conv  (None, 3, 3, 256)            262400    ['conv4_block16_out[0][0]']   \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block17_1_bn (BatchN  (None, 3, 3, 256)            1024      ['conv4_block17_1_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block17_1_relu (Acti  (None, 3, 3, 256)            0         ['conv4_block17_1_bn[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv4_block17_2_conv (Conv  (None, 3, 3, 256)            590080    ['conv4_block17_1_relu[0][0]']\n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block17_2_bn (BatchN  (None, 3, 3, 256)            1024      ['conv4_block17_2_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block17_2_relu (Acti  (None, 3, 3, 256)            0         ['conv4_block17_2_bn[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv4_block17_3_conv (Conv  (None, 3, 3, 1024)           263168    ['conv4_block17_2_relu[0][0]']\n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block17_3_bn (BatchN  (None, 3, 3, 1024)           4096      ['conv4_block17_3_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block17_add (Add)     (None, 3, 3, 1024)           0         ['conv4_block16_out[0][0]',   \n",
      "                                                                     'conv4_block17_3_bn[0][0]']  \n",
      "                                                                                                  \n",
      " conv4_block17_out (Activat  (None, 3, 3, 1024)           0         ['conv4_block17_add[0][0]']   \n",
      " ion)                                                                                             \n",
      "                                                                                                  \n",
      " conv4_block18_1_conv (Conv  (None, 3, 3, 256)            262400    ['conv4_block17_out[0][0]']   \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block18_1_bn (BatchN  (None, 3, 3, 256)            1024      ['conv4_block18_1_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block18_1_relu (Acti  (None, 3, 3, 256)            0         ['conv4_block18_1_bn[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv4_block18_2_conv (Conv  (None, 3, 3, 256)            590080    ['conv4_block18_1_relu[0][0]']\n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block18_2_bn (BatchN  (None, 3, 3, 256)            1024      ['conv4_block18_2_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block18_2_relu (Acti  (None, 3, 3, 256)            0         ['conv4_block18_2_bn[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv4_block18_3_conv (Conv  (None, 3, 3, 1024)           263168    ['conv4_block18_2_relu[0][0]']\n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block18_3_bn (BatchN  (None, 3, 3, 1024)           4096      ['conv4_block18_3_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block18_add (Add)     (None, 3, 3, 1024)           0         ['conv4_block17_out[0][0]',   \n",
      "                                                                     'conv4_block18_3_bn[0][0]']  \n",
      "                                                                                                  \n",
      " conv4_block18_out (Activat  (None, 3, 3, 1024)           0         ['conv4_block18_add[0][0]']   \n",
      " ion)                                                                                             \n",
      "                                                                                                  \n",
      " conv4_block19_1_conv (Conv  (None, 3, 3, 256)            262400    ['conv4_block18_out[0][0]']   \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block19_1_bn (BatchN  (None, 3, 3, 256)            1024      ['conv4_block19_1_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block19_1_relu (Acti  (None, 3, 3, 256)            0         ['conv4_block19_1_bn[0][0]']  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv4_block19_2_conv (Conv  (None, 3, 3, 256)            590080    ['conv4_block19_1_relu[0][0]']\n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block19_2_bn (BatchN  (None, 3, 3, 256)            1024      ['conv4_block19_2_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block19_2_relu (Acti  (None, 3, 3, 256)            0         ['conv4_block19_2_bn[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv4_block19_3_conv (Conv  (None, 3, 3, 1024)           263168    ['conv4_block19_2_relu[0][0]']\n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block19_3_bn (BatchN  (None, 3, 3, 1024)           4096      ['conv4_block19_3_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block19_add (Add)     (None, 3, 3, 1024)           0         ['conv4_block18_out[0][0]',   \n",
      "                                                                     'conv4_block19_3_bn[0][0]']  \n",
      "                                                                                                  \n",
      " conv4_block19_out (Activat  (None, 3, 3, 1024)           0         ['conv4_block19_add[0][0]']   \n",
      " ion)                                                                                             \n",
      "                                                                                                  \n",
      " conv4_block20_1_conv (Conv  (None, 3, 3, 256)            262400    ['conv4_block19_out[0][0]']   \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block20_1_bn (BatchN  (None, 3, 3, 256)            1024      ['conv4_block20_1_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block20_1_relu (Acti  (None, 3, 3, 256)            0         ['conv4_block20_1_bn[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv4_block20_2_conv (Conv  (None, 3, 3, 256)            590080    ['conv4_block20_1_relu[0][0]']\n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block20_2_bn (BatchN  (None, 3, 3, 256)            1024      ['conv4_block20_2_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block20_2_relu (Acti  (None, 3, 3, 256)            0         ['conv4_block20_2_bn[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv4_block20_3_conv (Conv  (None, 3, 3, 1024)           263168    ['conv4_block20_2_relu[0][0]']\n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block20_3_bn (BatchN  (None, 3, 3, 1024)           4096      ['conv4_block20_3_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block20_add (Add)     (None, 3, 3, 1024)           0         ['conv4_block19_out[0][0]',   \n",
      "                                                                     'conv4_block20_3_bn[0][0]']  \n",
      "                                                                                                  \n",
      " conv4_block20_out (Activat  (None, 3, 3, 1024)           0         ['conv4_block20_add[0][0]']   \n",
      " ion)                                                                                             \n",
      "                                                                                                  \n",
      " conv4_block21_1_conv (Conv  (None, 3, 3, 256)            262400    ['conv4_block20_out[0][0]']   \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block21_1_bn (BatchN  (None, 3, 3, 256)            1024      ['conv4_block21_1_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block21_1_relu (Acti  (None, 3, 3, 256)            0         ['conv4_block21_1_bn[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv4_block21_2_conv (Conv  (None, 3, 3, 256)            590080    ['conv4_block21_1_relu[0][0]']\n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block21_2_bn (BatchN  (None, 3, 3, 256)            1024      ['conv4_block21_2_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block21_2_relu (Acti  (None, 3, 3, 256)            0         ['conv4_block21_2_bn[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv4_block21_3_conv (Conv  (None, 3, 3, 1024)           263168    ['conv4_block21_2_relu[0][0]']\n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block21_3_bn (BatchN  (None, 3, 3, 1024)           4096      ['conv4_block21_3_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block21_add (Add)     (None, 3, 3, 1024)           0         ['conv4_block20_out[0][0]',   \n",
      "                                                                     'conv4_block21_3_bn[0][0]']  \n",
      "                                                                                                  \n",
      " conv4_block21_out (Activat  (None, 3, 3, 1024)           0         ['conv4_block21_add[0][0]']   \n",
      " ion)                                                                                             \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " conv4_block22_1_conv (Conv  (None, 3, 3, 256)            262400    ['conv4_block21_out[0][0]']   \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block22_1_bn (BatchN  (None, 3, 3, 256)            1024      ['conv4_block22_1_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block22_1_relu (Acti  (None, 3, 3, 256)            0         ['conv4_block22_1_bn[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv4_block22_2_conv (Conv  (None, 3, 3, 256)            590080    ['conv4_block22_1_relu[0][0]']\n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block22_2_bn (BatchN  (None, 3, 3, 256)            1024      ['conv4_block22_2_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block22_2_relu (Acti  (None, 3, 3, 256)            0         ['conv4_block22_2_bn[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv4_block22_3_conv (Conv  (None, 3, 3, 1024)           263168    ['conv4_block22_2_relu[0][0]']\n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block22_3_bn (BatchN  (None, 3, 3, 1024)           4096      ['conv4_block22_3_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block22_add (Add)     (None, 3, 3, 1024)           0         ['conv4_block21_out[0][0]',   \n",
      "                                                                     'conv4_block22_3_bn[0][0]']  \n",
      "                                                                                                  \n",
      " conv4_block22_out (Activat  (None, 3, 3, 1024)           0         ['conv4_block22_add[0][0]']   \n",
      " ion)                                                                                             \n",
      "                                                                                                  \n",
      " conv4_block23_1_conv (Conv  (None, 3, 3, 256)            262400    ['conv4_block22_out[0][0]']   \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block23_1_bn (BatchN  (None, 3, 3, 256)            1024      ['conv4_block23_1_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block23_1_relu (Acti  (None, 3, 3, 256)            0         ['conv4_block23_1_bn[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv4_block23_2_conv (Conv  (None, 3, 3, 256)            590080    ['conv4_block23_1_relu[0][0]']\n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block23_2_bn (BatchN  (None, 3, 3, 256)            1024      ['conv4_block23_2_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block23_2_relu (Acti  (None, 3, 3, 256)            0         ['conv4_block23_2_bn[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv4_block23_3_conv (Conv  (None, 3, 3, 1024)           263168    ['conv4_block23_2_relu[0][0]']\n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block23_3_bn (BatchN  (None, 3, 3, 1024)           4096      ['conv4_block23_3_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block23_add (Add)     (None, 3, 3, 1024)           0         ['conv4_block22_out[0][0]',   \n",
      "                                                                     'conv4_block23_3_bn[0][0]']  \n",
      "                                                                                                  \n",
      " conv4_block23_out (Activat  (None, 3, 3, 1024)           0         ['conv4_block23_add[0][0]']   \n",
      " ion)                                                                                             \n",
      "                                                                                                  \n",
      " conv4_block24_1_conv (Conv  (None, 3, 3, 256)            262400    ['conv4_block23_out[0][0]']   \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block24_1_bn (BatchN  (None, 3, 3, 256)            1024      ['conv4_block24_1_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block24_1_relu (Acti  (None, 3, 3, 256)            0         ['conv4_block24_1_bn[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv4_block24_2_conv (Conv  (None, 3, 3, 256)            590080    ['conv4_block24_1_relu[0][0]']\n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block24_2_bn (BatchN  (None, 3, 3, 256)            1024      ['conv4_block24_2_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block24_2_relu (Acti  (None, 3, 3, 256)            0         ['conv4_block24_2_bn[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv4_block24_3_conv (Conv  (None, 3, 3, 1024)           263168    ['conv4_block24_2_relu[0][0]']\n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block24_3_bn (BatchN  (None, 3, 3, 1024)           4096      ['conv4_block24_3_conv[0][0]']\n",
      " ormalization)                                                                                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " conv4_block24_add (Add)     (None, 3, 3, 1024)           0         ['conv4_block23_out[0][0]',   \n",
      "                                                                     'conv4_block24_3_bn[0][0]']  \n",
      "                                                                                                  \n",
      " conv4_block24_out (Activat  (None, 3, 3, 1024)           0         ['conv4_block24_add[0][0]']   \n",
      " ion)                                                                                             \n",
      "                                                                                                  \n",
      " conv4_block25_1_conv (Conv  (None, 3, 3, 256)            262400    ['conv4_block24_out[0][0]']   \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block25_1_bn (BatchN  (None, 3, 3, 256)            1024      ['conv4_block25_1_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block25_1_relu (Acti  (None, 3, 3, 256)            0         ['conv4_block25_1_bn[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv4_block25_2_conv (Conv  (None, 3, 3, 256)            590080    ['conv4_block25_1_relu[0][0]']\n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block25_2_bn (BatchN  (None, 3, 3, 256)            1024      ['conv4_block25_2_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block25_2_relu (Acti  (None, 3, 3, 256)            0         ['conv4_block25_2_bn[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv4_block25_3_conv (Conv  (None, 3, 3, 1024)           263168    ['conv4_block25_2_relu[0][0]']\n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block25_3_bn (BatchN  (None, 3, 3, 1024)           4096      ['conv4_block25_3_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block25_add (Add)     (None, 3, 3, 1024)           0         ['conv4_block24_out[0][0]',   \n",
      "                                                                     'conv4_block25_3_bn[0][0]']  \n",
      "                                                                                                  \n",
      " conv4_block25_out (Activat  (None, 3, 3, 1024)           0         ['conv4_block25_add[0][0]']   \n",
      " ion)                                                                                             \n",
      "                                                                                                  \n",
      " conv4_block26_1_conv (Conv  (None, 3, 3, 256)            262400    ['conv4_block25_out[0][0]']   \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block26_1_bn (BatchN  (None, 3, 3, 256)            1024      ['conv4_block26_1_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block26_1_relu (Acti  (None, 3, 3, 256)            0         ['conv4_block26_1_bn[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv4_block26_2_conv (Conv  (None, 3, 3, 256)            590080    ['conv4_block26_1_relu[0][0]']\n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block26_2_bn (BatchN  (None, 3, 3, 256)            1024      ['conv4_block26_2_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block26_2_relu (Acti  (None, 3, 3, 256)            0         ['conv4_block26_2_bn[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv4_block26_3_conv (Conv  (None, 3, 3, 1024)           263168    ['conv4_block26_2_relu[0][0]']\n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block26_3_bn (BatchN  (None, 3, 3, 1024)           4096      ['conv4_block26_3_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block26_add (Add)     (None, 3, 3, 1024)           0         ['conv4_block25_out[0][0]',   \n",
      "                                                                     'conv4_block26_3_bn[0][0]']  \n",
      "                                                                                                  \n",
      " conv4_block26_out (Activat  (None, 3, 3, 1024)           0         ['conv4_block26_add[0][0]']   \n",
      " ion)                                                                                             \n",
      "                                                                                                  \n",
      " conv4_block27_1_conv (Conv  (None, 3, 3, 256)            262400    ['conv4_block26_out[0][0]']   \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block27_1_bn (BatchN  (None, 3, 3, 256)            1024      ['conv4_block27_1_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block27_1_relu (Acti  (None, 3, 3, 256)            0         ['conv4_block27_1_bn[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv4_block27_2_conv (Conv  (None, 3, 3, 256)            590080    ['conv4_block27_1_relu[0][0]']\n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block27_2_bn (BatchN  (None, 3, 3, 256)            1024      ['conv4_block27_2_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block27_2_relu (Acti  (None, 3, 3, 256)            0         ['conv4_block27_2_bn[0][0]']  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv4_block27_3_conv (Conv  (None, 3, 3, 1024)           263168    ['conv4_block27_2_relu[0][0]']\n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block27_3_bn (BatchN  (None, 3, 3, 1024)           4096      ['conv4_block27_3_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block27_add (Add)     (None, 3, 3, 1024)           0         ['conv4_block26_out[0][0]',   \n",
      "                                                                     'conv4_block27_3_bn[0][0]']  \n",
      "                                                                                                  \n",
      " conv4_block27_out (Activat  (None, 3, 3, 1024)           0         ['conv4_block27_add[0][0]']   \n",
      " ion)                                                                                             \n",
      "                                                                                                  \n",
      " conv4_block28_1_conv (Conv  (None, 3, 3, 256)            262400    ['conv4_block27_out[0][0]']   \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block28_1_bn (BatchN  (None, 3, 3, 256)            1024      ['conv4_block28_1_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block28_1_relu (Acti  (None, 3, 3, 256)            0         ['conv4_block28_1_bn[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv4_block28_2_conv (Conv  (None, 3, 3, 256)            590080    ['conv4_block28_1_relu[0][0]']\n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block28_2_bn (BatchN  (None, 3, 3, 256)            1024      ['conv4_block28_2_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block28_2_relu (Acti  (None, 3, 3, 256)            0         ['conv4_block28_2_bn[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv4_block28_3_conv (Conv  (None, 3, 3, 1024)           263168    ['conv4_block28_2_relu[0][0]']\n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block28_3_bn (BatchN  (None, 3, 3, 1024)           4096      ['conv4_block28_3_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block28_add (Add)     (None, 3, 3, 1024)           0         ['conv4_block27_out[0][0]',   \n",
      "                                                                     'conv4_block28_3_bn[0][0]']  \n",
      "                                                                                                  \n",
      " conv4_block28_out (Activat  (None, 3, 3, 1024)           0         ['conv4_block28_add[0][0]']   \n",
      " ion)                                                                                             \n",
      "                                                                                                  \n",
      " conv4_block29_1_conv (Conv  (None, 3, 3, 256)            262400    ['conv4_block28_out[0][0]']   \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block29_1_bn (BatchN  (None, 3, 3, 256)            1024      ['conv4_block29_1_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block29_1_relu (Acti  (None, 3, 3, 256)            0         ['conv4_block29_1_bn[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv4_block29_2_conv (Conv  (None, 3, 3, 256)            590080    ['conv4_block29_1_relu[0][0]']\n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block29_2_bn (BatchN  (None, 3, 3, 256)            1024      ['conv4_block29_2_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block29_2_relu (Acti  (None, 3, 3, 256)            0         ['conv4_block29_2_bn[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv4_block29_3_conv (Conv  (None, 3, 3, 1024)           263168    ['conv4_block29_2_relu[0][0]']\n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block29_3_bn (BatchN  (None, 3, 3, 1024)           4096      ['conv4_block29_3_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block29_add (Add)     (None, 3, 3, 1024)           0         ['conv4_block28_out[0][0]',   \n",
      "                                                                     'conv4_block29_3_bn[0][0]']  \n",
      "                                                                                                  \n",
      " conv4_block29_out (Activat  (None, 3, 3, 1024)           0         ['conv4_block29_add[0][0]']   \n",
      " ion)                                                                                             \n",
      "                                                                                                  \n",
      " conv4_block30_1_conv (Conv  (None, 3, 3, 256)            262400    ['conv4_block29_out[0][0]']   \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block30_1_bn (BatchN  (None, 3, 3, 256)            1024      ['conv4_block30_1_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block30_1_relu (Acti  (None, 3, 3, 256)            0         ['conv4_block30_1_bn[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " conv4_block30_2_conv (Conv  (None, 3, 3, 256)            590080    ['conv4_block30_1_relu[0][0]']\n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block30_2_bn (BatchN  (None, 3, 3, 256)            1024      ['conv4_block30_2_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block30_2_relu (Acti  (None, 3, 3, 256)            0         ['conv4_block30_2_bn[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv4_block30_3_conv (Conv  (None, 3, 3, 1024)           263168    ['conv4_block30_2_relu[0][0]']\n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block30_3_bn (BatchN  (None, 3, 3, 1024)           4096      ['conv4_block30_3_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block30_add (Add)     (None, 3, 3, 1024)           0         ['conv4_block29_out[0][0]',   \n",
      "                                                                     'conv4_block30_3_bn[0][0]']  \n",
      "                                                                                                  \n",
      " conv4_block30_out (Activat  (None, 3, 3, 1024)           0         ['conv4_block30_add[0][0]']   \n",
      " ion)                                                                                             \n",
      "                                                                                                  \n",
      " conv4_block31_1_conv (Conv  (None, 3, 3, 256)            262400    ['conv4_block30_out[0][0]']   \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block31_1_bn (BatchN  (None, 3, 3, 256)            1024      ['conv4_block31_1_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block31_1_relu (Acti  (None, 3, 3, 256)            0         ['conv4_block31_1_bn[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv4_block31_2_conv (Conv  (None, 3, 3, 256)            590080    ['conv4_block31_1_relu[0][0]']\n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block31_2_bn (BatchN  (None, 3, 3, 256)            1024      ['conv4_block31_2_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block31_2_relu (Acti  (None, 3, 3, 256)            0         ['conv4_block31_2_bn[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv4_block31_3_conv (Conv  (None, 3, 3, 1024)           263168    ['conv4_block31_2_relu[0][0]']\n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block31_3_bn (BatchN  (None, 3, 3, 1024)           4096      ['conv4_block31_3_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block31_add (Add)     (None, 3, 3, 1024)           0         ['conv4_block30_out[0][0]',   \n",
      "                                                                     'conv4_block31_3_bn[0][0]']  \n",
      "                                                                                                  \n",
      " conv4_block31_out (Activat  (None, 3, 3, 1024)           0         ['conv4_block31_add[0][0]']   \n",
      " ion)                                                                                             \n",
      "                                                                                                  \n",
      " conv4_block32_1_conv (Conv  (None, 3, 3, 256)            262400    ['conv4_block31_out[0][0]']   \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block32_1_bn (BatchN  (None, 3, 3, 256)            1024      ['conv4_block32_1_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block32_1_relu (Acti  (None, 3, 3, 256)            0         ['conv4_block32_1_bn[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv4_block32_2_conv (Conv  (None, 3, 3, 256)            590080    ['conv4_block32_1_relu[0][0]']\n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block32_2_bn (BatchN  (None, 3, 3, 256)            1024      ['conv4_block32_2_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block32_2_relu (Acti  (None, 3, 3, 256)            0         ['conv4_block32_2_bn[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv4_block32_3_conv (Conv  (None, 3, 3, 1024)           263168    ['conv4_block32_2_relu[0][0]']\n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block32_3_bn (BatchN  (None, 3, 3, 1024)           4096      ['conv4_block32_3_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block32_add (Add)     (None, 3, 3, 1024)           0         ['conv4_block31_out[0][0]',   \n",
      "                                                                     'conv4_block32_3_bn[0][0]']  \n",
      "                                                                                                  \n",
      " conv4_block32_out (Activat  (None, 3, 3, 1024)           0         ['conv4_block32_add[0][0]']   \n",
      " ion)                                                                                             \n",
      "                                                                                                  \n",
      " conv4_block33_1_conv (Conv  (None, 3, 3, 256)            262400    ['conv4_block32_out[0][0]']   \n",
      " 2D)                                                                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " conv4_block33_1_bn (BatchN  (None, 3, 3, 256)            1024      ['conv4_block33_1_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block33_1_relu (Acti  (None, 3, 3, 256)            0         ['conv4_block33_1_bn[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv4_block33_2_conv (Conv  (None, 3, 3, 256)            590080    ['conv4_block33_1_relu[0][0]']\n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block33_2_bn (BatchN  (None, 3, 3, 256)            1024      ['conv4_block33_2_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block33_2_relu (Acti  (None, 3, 3, 256)            0         ['conv4_block33_2_bn[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv4_block33_3_conv (Conv  (None, 3, 3, 1024)           263168    ['conv4_block33_2_relu[0][0]']\n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block33_3_bn (BatchN  (None, 3, 3, 1024)           4096      ['conv4_block33_3_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block33_add (Add)     (None, 3, 3, 1024)           0         ['conv4_block32_out[0][0]',   \n",
      "                                                                     'conv4_block33_3_bn[0][0]']  \n",
      "                                                                                                  \n",
      " conv4_block33_out (Activat  (None, 3, 3, 1024)           0         ['conv4_block33_add[0][0]']   \n",
      " ion)                                                                                             \n",
      "                                                                                                  \n",
      " conv4_block34_1_conv (Conv  (None, 3, 3, 256)            262400    ['conv4_block33_out[0][0]']   \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block34_1_bn (BatchN  (None, 3, 3, 256)            1024      ['conv4_block34_1_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block34_1_relu (Acti  (None, 3, 3, 256)            0         ['conv4_block34_1_bn[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv4_block34_2_conv (Conv  (None, 3, 3, 256)            590080    ['conv4_block34_1_relu[0][0]']\n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block34_2_bn (BatchN  (None, 3, 3, 256)            1024      ['conv4_block34_2_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block34_2_relu (Acti  (None, 3, 3, 256)            0         ['conv4_block34_2_bn[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv4_block34_3_conv (Conv  (None, 3, 3, 1024)           263168    ['conv4_block34_2_relu[0][0]']\n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block34_3_bn (BatchN  (None, 3, 3, 1024)           4096      ['conv4_block34_3_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block34_add (Add)     (None, 3, 3, 1024)           0         ['conv4_block33_out[0][0]',   \n",
      "                                                                     'conv4_block34_3_bn[0][0]']  \n",
      "                                                                                                  \n",
      " conv4_block34_out (Activat  (None, 3, 3, 1024)           0         ['conv4_block34_add[0][0]']   \n",
      " ion)                                                                                             \n",
      "                                                                                                  \n",
      " conv4_block35_1_conv (Conv  (None, 3, 3, 256)            262400    ['conv4_block34_out[0][0]']   \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block35_1_bn (BatchN  (None, 3, 3, 256)            1024      ['conv4_block35_1_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block35_1_relu (Acti  (None, 3, 3, 256)            0         ['conv4_block35_1_bn[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv4_block35_2_conv (Conv  (None, 3, 3, 256)            590080    ['conv4_block35_1_relu[0][0]']\n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block35_2_bn (BatchN  (None, 3, 3, 256)            1024      ['conv4_block35_2_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block35_2_relu (Acti  (None, 3, 3, 256)            0         ['conv4_block35_2_bn[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv4_block35_3_conv (Conv  (None, 3, 3, 1024)           263168    ['conv4_block35_2_relu[0][0]']\n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block35_3_bn (BatchN  (None, 3, 3, 1024)           4096      ['conv4_block35_3_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block35_add (Add)     (None, 3, 3, 1024)           0         ['conv4_block34_out[0][0]',   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                     'conv4_block35_3_bn[0][0]']  \n",
      "                                                                                                  \n",
      " conv4_block35_out (Activat  (None, 3, 3, 1024)           0         ['conv4_block35_add[0][0]']   \n",
      " ion)                                                                                             \n",
      "                                                                                                  \n",
      " conv4_block36_1_conv (Conv  (None, 3, 3, 256)            262400    ['conv4_block35_out[0][0]']   \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block36_1_bn (BatchN  (None, 3, 3, 256)            1024      ['conv4_block36_1_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block36_1_relu (Acti  (None, 3, 3, 256)            0         ['conv4_block36_1_bn[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv4_block36_2_conv (Conv  (None, 3, 3, 256)            590080    ['conv4_block36_1_relu[0][0]']\n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block36_2_bn (BatchN  (None, 3, 3, 256)            1024      ['conv4_block36_2_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block36_2_relu (Acti  (None, 3, 3, 256)            0         ['conv4_block36_2_bn[0][0]']  \n",
      " vation)                                                                                          \n",
      "                                                                                                  \n",
      " conv4_block36_3_conv (Conv  (None, 3, 3, 1024)           263168    ['conv4_block36_2_relu[0][0]']\n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " conv4_block36_3_bn (BatchN  (None, 3, 3, 1024)           4096      ['conv4_block36_3_conv[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv4_block36_add (Add)     (None, 3, 3, 1024)           0         ['conv4_block35_out[0][0]',   \n",
      "                                                                     'conv4_block36_3_bn[0][0]']  \n",
      "                                                                                                  \n",
      " conv4_block36_out (Activat  (None, 3, 3, 1024)           0         ['conv4_block36_add[0][0]']   \n",
      " ion)                                                                                             \n",
      "                                                                                                  \n",
      " conv5_block1_1_conv (Conv2  (None, 2, 2, 512)            524800    ['conv4_block36_out[0][0]']   \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block1_1_bn (BatchNo  (None, 2, 2, 512)            2048      ['conv5_block1_1_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv5_block1_1_relu (Activ  (None, 2, 2, 512)            0         ['conv5_block1_1_bn[0][0]']   \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " conv5_block1_2_conv (Conv2  (None, 2, 2, 512)            2359808   ['conv5_block1_1_relu[0][0]'] \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block1_2_bn (BatchNo  (None, 2, 2, 512)            2048      ['conv5_block1_2_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv5_block1_2_relu (Activ  (None, 2, 2, 512)            0         ['conv5_block1_2_bn[0][0]']   \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " conv5_block1_0_conv (Conv2  (None, 2, 2, 2048)           2099200   ['conv4_block36_out[0][0]']   \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block1_3_conv (Conv2  (None, 2, 2, 2048)           1050624   ['conv5_block1_2_relu[0][0]'] \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block1_0_bn (BatchNo  (None, 2, 2, 2048)           8192      ['conv5_block1_0_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv5_block1_3_bn (BatchNo  (None, 2, 2, 2048)           8192      ['conv5_block1_3_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv5_block1_add (Add)      (None, 2, 2, 2048)           0         ['conv5_block1_0_bn[0][0]',   \n",
      "                                                                     'conv5_block1_3_bn[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block1_out (Activati  (None, 2, 2, 2048)           0         ['conv5_block1_add[0][0]']    \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block2_1_conv (Conv2  (None, 2, 2, 512)            1049088   ['conv5_block1_out[0][0]']    \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block2_1_bn (BatchNo  (None, 2, 2, 512)            2048      ['conv5_block2_1_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv5_block2_1_relu (Activ  (None, 2, 2, 512)            0         ['conv5_block2_1_bn[0][0]']   \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " conv5_block2_2_conv (Conv2  (None, 2, 2, 512)            2359808   ['conv5_block2_1_relu[0][0]'] \n",
      " D)                                                                                               \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " conv5_block2_2_bn (BatchNo  (None, 2, 2, 512)            2048      ['conv5_block2_2_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv5_block2_2_relu (Activ  (None, 2, 2, 512)            0         ['conv5_block2_2_bn[0][0]']   \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " conv5_block2_3_conv (Conv2  (None, 2, 2, 2048)           1050624   ['conv5_block2_2_relu[0][0]'] \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block2_3_bn (BatchNo  (None, 2, 2, 2048)           8192      ['conv5_block2_3_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv5_block2_add (Add)      (None, 2, 2, 2048)           0         ['conv5_block1_out[0][0]',    \n",
      "                                                                     'conv5_block2_3_bn[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block2_out (Activati  (None, 2, 2, 2048)           0         ['conv5_block2_add[0][0]']    \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " conv5_block3_1_conv (Conv2  (None, 2, 2, 512)            1049088   ['conv5_block2_out[0][0]']    \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block3_1_bn (BatchNo  (None, 2, 2, 512)            2048      ['conv5_block3_1_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv5_block3_1_relu (Activ  (None, 2, 2, 512)            0         ['conv5_block3_1_bn[0][0]']   \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " conv5_block3_2_conv (Conv2  (None, 2, 2, 512)            2359808   ['conv5_block3_1_relu[0][0]'] \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block3_2_bn (BatchNo  (None, 2, 2, 512)            2048      ['conv5_block3_2_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv5_block3_2_relu (Activ  (None, 2, 2, 512)            0         ['conv5_block3_2_bn[0][0]']   \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " conv5_block3_3_conv (Conv2  (None, 2, 2, 2048)           1050624   ['conv5_block3_2_relu[0][0]'] \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block3_3_bn (BatchNo  (None, 2, 2, 2048)           8192      ['conv5_block3_3_conv[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv5_block3_add (Add)      (None, 2, 2, 2048)           0         ['conv5_block2_out[0][0]',    \n",
      "                                                                     'conv5_block3_3_bn[0][0]']   \n",
      "                                                                                                  \n",
      " conv5_block3_out (Activati  (None, 2, 2, 2048)           0         ['conv5_block3_add[0][0]']    \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 58370944 (222.67 MB)\n",
      "Trainable params: 58219520 (222.09 MB)\n",
      "Non-trainable params: 151424 (591.50 KB)\n",
      "__________________________________________________________________________________________________\n",
      "CNN model has been created, you can proceed to train your data with this model.\n",
      "Epoch 1/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-12 10:55:11.094143: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:437] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED\n",
      "2024-10-12 10:55:11.094208: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:441] Memory usage: 13565952 bytes free, 8361017344 bytes total.\n",
      "2024-10-12 10:55:11.094266: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:451] Possibly insufficient driver version: 535.154.5\n",
      "2024-10-12 10:55:11.094301: W tensorflow/core/framework/op_kernel.cc:1828] OP_REQUIRES failed at conv_ops_impl.h:770 : UNIMPLEMENTED: DNN library is not found.\n"
     ]
    },
    {
     "ename": "UnimplementedError",
     "evalue": "Graph execution error:\n\nDetected at node 'sequential/resnet152/conv1_conv/Conv2D' defined at (most recent call last):\n    File \"<frozen runpy>\", line 198, in _run_module_as_main\n    File \"<frozen runpy>\", line 88, in _run_code\n    File \"/home/rajesh/anaconda3/lib/python3.11/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/home/rajesh/anaconda3/lib/python3.11/site-packages/traitlets/config/application.py\", line 992, in launch_instance\n      app.start()\n    File \"/home/rajesh/anaconda3/lib/python3.11/site-packages/ipykernel/kernelapp.py\", line 736, in start\n      self.io_loop.start()\n    File \"/home/rajesh/anaconda3/lib/python3.11/site-packages/tornado/platform/asyncio.py\", line 195, in start\n      self.asyncio_loop.run_forever()\n    File \"/home/rajesh/anaconda3/lib/python3.11/asyncio/base_events.py\", line 607, in run_forever\n      self._run_once()\n    File \"/home/rajesh/anaconda3/lib/python3.11/asyncio/base_events.py\", line 1922, in _run_once\n      handle._run()\n    File \"/home/rajesh/anaconda3/lib/python3.11/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/home/rajesh/anaconda3/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 516, in dispatch_queue\n      await self.process_one()\n    File \"/home/rajesh/anaconda3/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 505, in process_one\n      await dispatch(*args)\n    File \"/home/rajesh/anaconda3/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 412, in dispatch_shell\n      await result\n    File \"/home/rajesh/anaconda3/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 740, in execute_request\n      reply_content = await reply_content\n    File \"/home/rajesh/anaconda3/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n      res = shell.run_cell(\n    File \"/home/rajesh/anaconda3/lib/python3.11/site-packages/ipykernel/zmqshell.py\", line 546, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/home/rajesh/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3024, in run_cell\n      result = self._run_cell(\n    File \"/home/rajesh/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3079, in _run_cell\n      result = runner(coro)\n    File \"/home/rajesh/anaconda3/lib/python3.11/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/home/rajesh/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3284, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/home/rajesh/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3466, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/home/rajesh/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3526, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_96440/588288564.py\", line 95, in <module>\n      history = model.fit(\n    File \"/home/rajesh/anaconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/rajesh/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1742, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/home/rajesh/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1338, in train_function\n      return step_function(self, iterator)\n    File \"/home/rajesh/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1322, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/rajesh/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1303, in run_step\n      outputs = model.train_step(data)\n    File \"/home/rajesh/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1080, in train_step\n      y_pred = self(x, training=True)\n    File \"/home/rajesh/anaconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/rajesh/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py\", line 569, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"/home/rajesh/anaconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/rajesh/anaconda3/lib/python3.11/site-packages/keras/src/engine/base_layer.py\", line 1150, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/home/rajesh/anaconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/rajesh/anaconda3/lib/python3.11/site-packages/keras/src/engine/sequential.py\", line 405, in call\n      return super().call(inputs, training=training, mask=mask)\n    File \"/home/rajesh/anaconda3/lib/python3.11/site-packages/keras/src/engine/functional.py\", line 512, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"/home/rajesh/anaconda3/lib/python3.11/site-packages/keras/src/engine/functional.py\", line 669, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"/home/rajesh/anaconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/rajesh/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py\", line 569, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"/home/rajesh/anaconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/rajesh/anaconda3/lib/python3.11/site-packages/keras/src/engine/base_layer.py\", line 1150, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/home/rajesh/anaconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/rajesh/anaconda3/lib/python3.11/site-packages/keras/src/engine/functional.py\", line 512, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"/home/rajesh/anaconda3/lib/python3.11/site-packages/keras/src/engine/functional.py\", line 669, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"/home/rajesh/anaconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/rajesh/anaconda3/lib/python3.11/site-packages/keras/src/engine/base_layer.py\", line 1150, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/home/rajesh/anaconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/rajesh/anaconda3/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py\", line 290, in call\n      outputs = self.convolution_op(inputs, self.kernel)\n    File \"/home/rajesh/anaconda3/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py\", line 262, in convolution_op\n      return tf.nn.convolution(\nNode: 'sequential/resnet152/conv1_conv/Conv2D'\nDNN library is not found.\n\t [[{{node sequential/resnet152/conv1_conv/Conv2D}}]] [Op:__inference_train_function_29441]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnimplementedError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 95\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# Training the model\u001b[39;00m\n\u001b[1;32m     93\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m500\u001b[39m\n\u001b[0;32m---> 95\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[1;32m     96\u001b[0m     train_set,\n\u001b[1;32m     97\u001b[0m     epochs\u001b[38;5;241m=\u001b[39mepochs,\n\u001b[1;32m     98\u001b[0m     validation_data\u001b[38;5;241m=\u001b[39mtest_set\n\u001b[1;32m     99\u001b[0m )\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# Print training history\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28mprint\u001b[39m(history\u001b[38;5;241m.\u001b[39mhistory)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mUnimplementedError\u001b[0m: Graph execution error:\n\nDetected at node 'sequential/resnet152/conv1_conv/Conv2D' defined at (most recent call last):\n    File \"<frozen runpy>\", line 198, in _run_module_as_main\n    File \"<frozen runpy>\", line 88, in _run_code\n    File \"/home/rajesh/anaconda3/lib/python3.11/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/home/rajesh/anaconda3/lib/python3.11/site-packages/traitlets/config/application.py\", line 992, in launch_instance\n      app.start()\n    File \"/home/rajesh/anaconda3/lib/python3.11/site-packages/ipykernel/kernelapp.py\", line 736, in start\n      self.io_loop.start()\n    File \"/home/rajesh/anaconda3/lib/python3.11/site-packages/tornado/platform/asyncio.py\", line 195, in start\n      self.asyncio_loop.run_forever()\n    File \"/home/rajesh/anaconda3/lib/python3.11/asyncio/base_events.py\", line 607, in run_forever\n      self._run_once()\n    File \"/home/rajesh/anaconda3/lib/python3.11/asyncio/base_events.py\", line 1922, in _run_once\n      handle._run()\n    File \"/home/rajesh/anaconda3/lib/python3.11/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/home/rajesh/anaconda3/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 516, in dispatch_queue\n      await self.process_one()\n    File \"/home/rajesh/anaconda3/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 505, in process_one\n      await dispatch(*args)\n    File \"/home/rajesh/anaconda3/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 412, in dispatch_shell\n      await result\n    File \"/home/rajesh/anaconda3/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 740, in execute_request\n      reply_content = await reply_content\n    File \"/home/rajesh/anaconda3/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n      res = shell.run_cell(\n    File \"/home/rajesh/anaconda3/lib/python3.11/site-packages/ipykernel/zmqshell.py\", line 546, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/home/rajesh/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3024, in run_cell\n      result = self._run_cell(\n    File \"/home/rajesh/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3079, in _run_cell\n      result = runner(coro)\n    File \"/home/rajesh/anaconda3/lib/python3.11/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/home/rajesh/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3284, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/home/rajesh/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3466, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/home/rajesh/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3526, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_96440/588288564.py\", line 95, in <module>\n      history = model.fit(\n    File \"/home/rajesh/anaconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/rajesh/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1742, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/home/rajesh/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1338, in train_function\n      return step_function(self, iterator)\n    File \"/home/rajesh/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1322, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/rajesh/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1303, in run_step\n      outputs = model.train_step(data)\n    File \"/home/rajesh/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1080, in train_step\n      y_pred = self(x, training=True)\n    File \"/home/rajesh/anaconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/rajesh/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py\", line 569, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"/home/rajesh/anaconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/rajesh/anaconda3/lib/python3.11/site-packages/keras/src/engine/base_layer.py\", line 1150, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/home/rajesh/anaconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/rajesh/anaconda3/lib/python3.11/site-packages/keras/src/engine/sequential.py\", line 405, in call\n      return super().call(inputs, training=training, mask=mask)\n    File \"/home/rajesh/anaconda3/lib/python3.11/site-packages/keras/src/engine/functional.py\", line 512, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"/home/rajesh/anaconda3/lib/python3.11/site-packages/keras/src/engine/functional.py\", line 669, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"/home/rajesh/anaconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/rajesh/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py\", line 569, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"/home/rajesh/anaconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/rajesh/anaconda3/lib/python3.11/site-packages/keras/src/engine/base_layer.py\", line 1150, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/home/rajesh/anaconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/rajesh/anaconda3/lib/python3.11/site-packages/keras/src/engine/functional.py\", line 512, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"/home/rajesh/anaconda3/lib/python3.11/site-packages/keras/src/engine/functional.py\", line 669, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"/home/rajesh/anaconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/rajesh/anaconda3/lib/python3.11/site-packages/keras/src/engine/base_layer.py\", line 1150, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/home/rajesh/anaconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/rajesh/anaconda3/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py\", line 290, in call\n      outputs = self.convolution_op(inputs, self.kernel)\n    File \"/home/rajesh/anaconda3/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py\", line 262, in convolution_op\n      return tf.nn.convolution(\nNode: 'sequential/resnet152/conv1_conv/Conv2D'\nDNN library is not found.\n\t [[{{node sequential/resnet152/conv1_conv/Conv2D}}]] [Op:__inference_train_function_29441]"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import (Dense, Flatten, BatchNormalization, \n",
    "                                     Dropout, Conv2D, MaxPooling2D)\n",
    "from tensorflow.keras.applications import VGG19\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "# Folder path for training images\n",
    "folder_path = \"images/train\"  \n",
    "\n",
    "# Image specifications\n",
    "picture_size = 48  # Define picture size\n",
    "batch_size = 128\n",
    "\n",
    "# Data Generators with augmentation for training set\n",
    "datagen_train = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Normalization for validation set (no augmentation)\n",
    "datagen_val = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Create training and validation data sets\n",
    "train_set = datagen_train.flow_from_directory(\n",
    "    folder_path,\n",
    "    target_size=(picture_size, picture_size),  # Use picture_size variable\n",
    "    color_mode=\"rgb\",  # Ensure RGB input\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Corrected test set creation\n",
    "test_set = datagen_val.flow_from_directory(\n",
    "    \"images/test\",  # Added a comma at the end of this line\n",
    "    target_size=(picture_size, picture_size),  # Use picture_size variable\n",
    "    color_mode=\"rgb\",  # Ensure RGB input\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "print('Train and Validation sets have been created.')\n",
    "\n",
    "\n",
    "\n",
    "from keras.applications.resnet import ResNet152\n",
    "base_model = ResNet152(\n",
    "    weights='imagenet',\n",
    "    include_top = False,\n",
    "    input_shape=(48,48,3)\n",
    ")\n",
    "\n",
    "# Show the model summary\n",
    "base_model.summary()\n",
    "\n",
    "# Create a Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add base model\n",
    "model.add(base_model)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.30))\n",
    "# Second fully connected layer  \n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.30))\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "# Set base model to not trainable\n",
    "base_model.trainable = False\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "print('CNN model has been created, you can proceed to train your data with this model.')\n",
    "\n",
    "# Training the model\n",
    "epochs = 500\n",
    "\n",
    "history = model.fit(\n",
    "    train_set,\n",
    "    epochs=epochs,\n",
    "    validation_data=test_set\n",
    ")\n",
    "\n",
    "# Print training history\n",
    "print(history.history)\n",
    "\n",
    "# Plotting training history\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Loss Plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Accuracy Plot\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98de5c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#exp 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21143f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import (Dense, Flatten, BatchNormalization, \n",
    "                                     Dropout, Conv2D, MaxPooling2D)\n",
    "from tensorflow.keras.applications import VGG19\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "# Folder path for training images\n",
    "folder_path = \"images/train\"  \n",
    "\n",
    "# Image specifications\n",
    "picture_size = 48  # Define picture size\n",
    "batch_size = 128\n",
    "\n",
    "# Data Generators with augmentation for training set\n",
    "datagen_train = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Normalization for validation set (no augmentation)\n",
    "datagen_val = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Create training and validation data sets\n",
    "train_set = datagen_train.flow_from_directory(\n",
    "    folder_path,\n",
    "    target_size=(picture_size, picture_size),  # Use picture_size variable\n",
    "    color_mode=\"rgb\",  # Ensure RGB input\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Corrected test set creation\n",
    "test_set = datagen_val.flow_from_directory(\n",
    "    \"images/test\",  # Added a comma at the end of this line\n",
    "    target_size=(picture_size, picture_size),  # Use picture_size variable\n",
    "    color_mode=\"rgb\",  # Ensure RGB input\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "print('Train and Validation sets have been created.')\n",
    "\n",
    "\n",
    "\n",
    "from keras.applications.resnet import ResNet152\n",
    "base_model = ResNet152(\n",
    "    weights='imagenet',\n",
    "    include_top = False,\n",
    "    input_shape=(48,48,3)\n",
    ")\n",
    "\n",
    "# Show the model summary\n",
    "base_model.summary()\n",
    "\n",
    "# Create a Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add base model\n",
    "model.add(base_model)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.30))\n",
    "# Second fully connected layer  \n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.30))\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "# Set base model to not trainable\n",
    "base_model.trainable = False\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "print('CNN model has been created, you can proceed to train your data with this model.')\n",
    "\n",
    "# Training the model\n",
    "epochs = 500\n",
    "\n",
    "history = model.fit(\n",
    "    train_set,\n",
    "    epochs=epochs,\n",
    "    validation_data=test_set\n",
    ")\n",
    "\n",
    "# Print training history\n",
    "print(history.history)\n",
    "\n",
    "# Plotting training history\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Loss Plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Accuracy Plot\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8772349",
   "metadata": {},
   "outputs": [],
   "source": [
    "#exp4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e1945f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9bec58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#exp5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ba23de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Jupyter Notebook\n",
    "JaffeVGG197ocotberexp2 Last Checkpoint: an hour ago (autosaved) Current Kernel Logo \n",
    "\n",
    "Python 3 (ipykernel)\n",
    "\n",
    "    File\n",
    "    Edit\n",
    "    View\n",
    "    Insert\n",
    "    Cell\n",
    "    Kernel\n",
    "    Widgets\n",
    "    Help\n",
    "\n",
    "import tensorflow\n",
    "\n",
    "from keras.layers import Dense,Flatten\n",
    "\n",
    "from keras.applications.vgg16 import VGG16\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "from keras import Sequential\n",
    "\n",
    "from keras.layers import Dense,Conv2D,MaxPooling2D,Flatten,BatchNormalization,Dropout\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam,SGD,RMSprop\n",
    "\n",
    "from tensorflow.keras.layers import Dense,Input,Dropout,GlobalAveragePooling2D,Flatten,Conv2D,BatchNormalization,Activation,MaxPooling2D\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Flatten, BatchNormalization, Dropout\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from tensorflow.keras.applications.vgg19 import VGG19\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Importing Deep Learning Libraries\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "from keras import regularizers\n",
    "\n",
    "from keras.preprocessing import image\n",
    "\n",
    "​\n",
    "\n",
    "#HISTOGRAM CODE\n",
    "\n",
    "#histogram code\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "\n",
    "​\n",
    "\n",
    "emotions = [\"happy\", \"sadness\", \"anger\", \"disgust\", \"neutral\", \"fear\", \"surprise\"]\n",
    "\n",
    "​\n",
    "\n",
    "folder_path = \"Jaffetrainvalidation/train\"\n",
    "\n",
    "# Counting the number of images per emotion\n",
    "\n",
    "counts = [len(os.listdir(os.path.join(folder_path, emotion))) for emotion in emotions]\n",
    "\n",
    "​\n",
    "\n",
    "# Plotting the bar chart\n",
    "\n",
    "colors = ['red', 'yellow', 'black', 'blue', 'orange', 'green', 'pink']\n",
    "\n",
    "plt.bar(emotions, height=counts, color=colors)\n",
    "\n",
    "plt.ylabel('Number')\n",
    "\n",
    "plt.xlabel('Emotions')\n",
    "\n",
    "​\n",
    "\n",
    "​\n",
    "\n",
    "#plt.savefig('hostgoarm.png')\n",
    "\n",
    "#plt.show()\n",
    "\n",
    "​\n",
    "\n",
    "​\n",
    "\n",
    "# Data generators\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "​\n",
    "\n",
    "# Data augmentation for training set\n",
    "\n",
    "datagen_train = ImageDataGenerator(\n",
    "\n",
    "    rescale=1./255,\n",
    "\n",
    "    rotation_range=15,\n",
    "\n",
    "    width_shift_range=0.1,\n",
    "\n",
    "    height_shift_range=0.1,\n",
    "\n",
    "    shear_range=0.2,\n",
    "\n",
    "    zoom_range=0.2,\n",
    "\n",
    "    horizontal_flip=True,\n",
    "\n",
    "    fill_mode='nearest'\n",
    "\n",
    ")\n",
    "\n",
    "​\n",
    "\n",
    "# Normalization for validation set (no augmentation)\n",
    "\n",
    "datagen_val = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "​\n",
    "\n",
    "​\n",
    "\n",
    "​\n",
    "\n",
    "train_ds = datagen_train.flow_from_directory(\"Jaffetrainvalidation/train\",\n",
    "\n",
    "                                             target_size=(256, 256),\n",
    "\n",
    "                                             color_mode=\"rgb\",\n",
    "\n",
    "                                             batch_size=batch_size,\n",
    "\n",
    "                                             class_mode='categorical',\n",
    "\n",
    "                                             shuffle=True)\n",
    "\n",
    "​\n",
    "\n",
    "test_ds = datagen_val.flow_from_directory(\"Jaffetrainvalidation/validation\",\n",
    "\n",
    "                                         target_size=(256, 256),\n",
    "\n",
    "                                         color_mode=\"rgb\",\n",
    "\n",
    "                                         batch_size=batch_size,\n",
    "\n",
    "                                         class_mode='categorical',\n",
    "\n",
    "                                         shuffle=False)\n",
    "\n",
    "print('Train and Validation sets have been created.')\n",
    "\n",
    "​\n",
    "\n",
    "​\n",
    "\n",
    "#model vgg19\n",
    "\n",
    "​\n",
    "\n",
    "from keras.applications.vgg19 import VGG19\n",
    "\n",
    "conv_base = VGG19(weights='imagenet', include_top=False, input_shape=(256, 256, 3))\n",
    "\n",
    "conv_base.summary()\n",
    "\n",
    "​\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(conv_base)\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(256,activation = 'relu'))\n",
    "\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dropout(0.30))\n",
    "\n",
    "# Second fully connected layer  \n",
    "\n",
    "model.add(Dense(512,activation = 'relu'))\n",
    "\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dropout(0.30))\n",
    "\n",
    "​\n",
    "\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "​\n",
    "\n",
    "# model.summary()\n",
    "\n",
    "# Visualize the model.\n",
    "\n",
    "#plot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True)\n",
    "\n",
    "​\n",
    "\n",
    "conv_base.trainable = False\n",
    "\n",
    "​\n",
    "\n",
    "model.compile(optimizer = Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "​\n",
    "\n",
    "print('CNN model has been created you can proceed to train you data with this model.')\n",
    "\n",
    "​\n",
    "\n",
    "​\n",
    "\n",
    "​\n",
    "\n",
    "​\n",
    "\n",
    "# Training the model\n",
    "\n",
    "epochs = 500\n",
    "\n",
    "​\n",
    "\n",
    "history = model.fit(x=train_ds,\n",
    "\n",
    "                    epochs=epochs,\n",
    "\n",
    "                    validation_data=test_ds)\n",
    "\n",
    "​\n",
    "\n",
    "# Print training history\n",
    "\n",
    "print(history.history)\n",
    "\n",
    "​\n",
    "\n",
    "# Plotting training history\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "​\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "\n",
    "plt.title('Loss')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "​\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "\n",
    "plt.title('Accuracy')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "Found 168 images belonging to 7 classes.\n",
    "Found 45 images belonging to 7 classes.\n",
    "Train and Validation sets have been created.\n",
    "Model: \"vgg19\"\n",
    "_________________________________________________________________\n",
    " Layer (type)                Output Shape              Param #   \n",
    "=================================================================\n",
    " input_2 (InputLayer)        [(None, 256, 256, 3)]     0         \n",
    "                                                                 \n",
    " block1_conv1 (Conv2D)       (None, 256, 256, 64)      1792      \n",
    "                                                                 \n",
    " block1_conv2 (Conv2D)       (None, 256, 256, 64)      36928     \n",
    "                                                                 \n",
    " block1_pool (MaxPooling2D)  (None, 128, 128, 64)      0         \n",
    "                                                                 \n",
    " block2_conv1 (Conv2D)       (None, 128, 128, 128)     73856     \n",
    "                                                                 \n",
    " block2_conv2 (Conv2D)       (None, 128, 128, 128)     147584    \n",
    "                                                                 \n",
    " block2_pool (MaxPooling2D)  (None, 64, 64, 128)       0         \n",
    "                                                                 \n",
    " block3_conv1 (Conv2D)       (None, 64, 64, 256)       295168    \n",
    "                                                                 \n",
    " block3_conv2 (Conv2D)       (None, 64, 64, 256)       590080    \n",
    "                                                                 \n",
    " block3_conv3 (Conv2D)       (None, 64, 64, 256)       590080    \n",
    "                                                                 \n",
    " block3_conv4 (Conv2D)       (None, 64, 64, 256)       590080    \n",
    "                                                                 \n",
    " block3_pool (MaxPooling2D)  (None, 32, 32, 256)       0         \n",
    "                                                                 \n",
    " block4_conv1 (Conv2D)       (None, 32, 32, 512)       1180160   \n",
    "                                                                 \n",
    " block4_conv2 (Conv2D)       (None, 32, 32, 512)       2359808   \n",
    "                                                                 \n",
    " block4_conv3 (Conv2D)       (None, 32, 32, 512)       2359808   \n",
    "                                                                 \n",
    " block4_conv4 (Conv2D)       (None, 32, 32, 512)       2359808   \n",
    "                                                                 \n",
    " block4_pool (MaxPooling2D)  (None, 16, 16, 512)       0         \n",
    "                                                                 \n",
    " block5_conv1 (Conv2D)       (None, 16, 16, 512)       2359808   \n",
    "                                                                 \n",
    " block5_conv2 (Conv2D)       (None, 16, 16, 512)       2359808   \n",
    "                                                                 \n",
    " block5_conv3 (Conv2D)       (None, 16, 16, 512)       2359808   \n",
    "                                                                 \n",
    " block5_conv4 (Conv2D)       (None, 16, 16, 512)       2359808   \n",
    "                                                                 \n",
    " block5_pool (MaxPooling2D)  (None, 8, 8, 512)         0         \n",
    "                                                                 \n",
    "=================================================================\n",
    "Total params: 20024384 (76.39 MB)\n",
    "Trainable params: 20024384 (76.39 MB)\n",
    "Non-trainable params: 0 (0.00 Byte)\n",
    "_________________________________________________________________\n",
    "CNN model has been created you can proceed to train you data with this model.\n",
    "Epoch 1/500\n",
    "3/3 [==============================] - 3s 562ms/step - loss: 2.9591 - accuracy: 0.1607 - val_loss: 2.3374 - val_accuracy: 0.1333\n",
    "Epoch 2/500\n",
    "3/3 [==============================] - 2s 669ms/step - loss: 2.1094 - accuracy: 0.3274 - val_loss: 2.9485 - val_accuracy: 0.1333\n",
    "Epoch 3/500\n",
    "3/3 [==============================] - 2s 513ms/step - loss: 2.1381 - accuracy: 0.2917 - val_loss: 3.3270 - val_accuracy: 0.1556\n",
    "Epoch 4/500\n",
    "3/3 [==============================] - 2s 527ms/step - loss: 1.9270 - accuracy: 0.3452 - val_loss: 3.5544 - val_accuracy: 0.3111\n",
    "Epoch 5/500\n",
    "3/3 [==============================] - 2s 531ms/step - loss: 1.7741 - accuracy: 0.3869 - val_loss: 4.7872 - val_accuracy: 0.1556\n",
    "Epoch 6/500\n",
    "3/3 [==============================] - 2s 532ms/step - loss: 1.7332 - accuracy: 0.4583 - val_loss: 6.4278 - val_accuracy: 0.1556\n",
    "Epoch 7/500\n",
    "3/3 [==============================] - 2s 509ms/step - loss: 1.6603 - accuracy: 0.4107 - val_loss: 6.8751 - val_accuracy: 0.1556\n",
    "Epoch 8/500\n",
    "3/3 [==============================] - 2s 658ms/step - loss: 1.4563 - accuracy: 0.5060 - val_loss: 6.6626 - val_accuracy: 0.1556\n",
    "Epoch 9/500\n",
    "3/3 [==============================] - 2s 515ms/step - loss: 1.3662 - accuracy: 0.4940 - val_loss: 6.2024 - val_accuracy: 0.1556\n",
    "Epoch 10/500\n",
    "3/3 [==============================] - 2s 532ms/step - loss: 1.5511 - accuracy: 0.4464 - val_loss: 6.4514 - val_accuracy: 0.1556\n",
    "Epoch 11/500\n",
    "3/3 [==============================] - 2s 660ms/step - loss: 1.4026 - accuracy: 0.5060 - val_loss: 7.5452 - val_accuracy: 0.1556\n",
    "Epoch 12/500\n",
    "3/3 [==============================] - 2s 508ms/step - loss: 1.3299 - accuracy: 0.5357 - val_loss: 7.6131 - val_accuracy: 0.1556\n",
    "Epoch 13/500\n",
    "3/3 [==============================] - 2s 672ms/step - loss: 1.3929 - accuracy: 0.4821 - val_loss: 7.0907 - val_accuracy: 0.1556\n",
    "Epoch 14/500\n",
    "3/3 [==============================] - 2s 529ms/step - loss: 1.1438 - accuracy: 0.5536 - val_loss: 6.4706 - val_accuracy: 0.1556\n",
    "Epoch 15/500\n",
    "3/3 [==============================] - 2s 644ms/step - loss: 1.1432 - accuracy: 0.5238 - val_loss: 6.1594 - val_accuracy: 0.1556\n",
    "Epoch 16/500\n",
    "3/3 [==============================] - 2s 515ms/step - loss: 1.0839 - accuracy: 0.5536 - val_loss: 5.8179 - val_accuracy: 0.1556\n",
    "Epoch 17/500\n",
    "3/3 [==============================] - 2s 499ms/step - loss: 1.0909 - accuracy: 0.5655 - val_loss: 5.6971 - val_accuracy: 0.1556\n",
    "Epoch 18/500\n",
    "3/3 [==============================] - 2s 497ms/step - loss: 1.0745 - accuracy: 0.5833 - val_loss: 5.4085 - val_accuracy: 0.1778\n",
    "Epoch 19/500\n",
    "3/3 [==============================] - 2s 500ms/step - loss: 0.9454 - accuracy: 0.6845 - val_loss: 4.8075 - val_accuracy: 0.2000\n",
    "Epoch 20/500\n",
    "3/3 [==============================] - 2s 524ms/step - loss: 0.9348 - accuracy: 0.6726 - val_loss: 4.0920 - val_accuracy: 0.2000\n",
    "Epoch 21/500\n",
    "3/3 [==============================] - 2s 652ms/step - loss: 1.0142 - accuracy: 0.6250 - val_loss: 3.6588 - val_accuracy: 0.2000\n",
    "Epoch 22/500\n",
    "3/3 [==============================] - 2s 504ms/step - loss: 0.7819 - accuracy: 0.7321 - val_loss: 3.4263 - val_accuracy: 0.2000\n",
    "Epoch 23/500\n",
    "3/3 [==============================] - 2s 505ms/step - loss: 0.8863 - accuracy: 0.6905 - val_loss: 3.3581 - val_accuracy: 0.2222\n",
    "Epoch 24/500\n",
    "3/3 [==============================] - 2s 498ms/step - loss: 0.8642 - accuracy: 0.7083 - val_loss: 3.3748 - val_accuracy: 0.2444\n",
    "Epoch 25/500\n",
    "3/3 [==============================] - 2s 521ms/step - loss: 0.8094 - accuracy: 0.7262 - val_loss: 3.2030 - val_accuracy: 0.2889\n",
    "Epoch 26/500\n",
    "3/3 [==============================] - 2s 517ms/step - loss: 0.7292 - accuracy: 0.7381 - val_loss: 3.3549 - val_accuracy: 0.4000\n",
    "Epoch 27/500\n",
    "3/3 [==============================] - 2s 503ms/step - loss: 0.6432 - accuracy: 0.7440 - val_loss: 3.6765 - val_accuracy: 0.4000\n",
    "Epoch 28/500\n",
    "3/3 [==============================] - 2s 513ms/step - loss: 0.6737 - accuracy: 0.7500 - val_loss: 3.7778 - val_accuracy: 0.4222\n",
    "Epoch 29/500\n",
    "3/3 [==============================] - 2s 650ms/step - loss: 0.5984 - accuracy: 0.7917 - val_loss: 3.7234 - val_accuracy: 0.3111\n",
    "Epoch 30/500\n",
    "3/3 [==============================] - 2s 655ms/step - loss: 0.6591 - accuracy: 0.7738 - val_loss: 3.5275 - val_accuracy: 0.3111\n",
    "Epoch 31/500\n",
    "3/3 [==============================] - 2s 508ms/step - loss: 0.7381 - accuracy: 0.7381 - val_loss: 2.9284 - val_accuracy: 0.4889\n",
    "Epoch 32/500\n",
    "3/3 [==============================] - 2s 510ms/step - loss: 0.5630 - accuracy: 0.7679 - val_loss: 2.6781 - val_accuracy: 0.4889\n",
    "Epoch 33/500\n",
    "3/3 [==============================] - 2s 664ms/step - loss: 0.5224 - accuracy: 0.7976 - val_loss: 2.7228 - val_accuracy: 0.5111\n",
    "\n",
    "Epoch 34/500\n",
    "3/3 [==============================] - 2s 518ms/step - loss: 0.4159 - accuracy: 0.8393 - val_loss: 2.7385 - val_accuracy: 0.5111\n",
    "Epoch 35/500\n",
    "3/3 [==============================] - 2s 506ms/step - loss: 0.5651 - accuracy: 0.7917 - val_loss: 3.2920 - val_accuracy: 0.4000\n",
    "Epoch 36/500\n",
    "3/3 [==============================] - 2s 514ms/step - loss: 0.5512 - accuracy: 0.8036 - val_loss: 4.6525 - val_accuracy: 0.1556\n",
    "Epoch 37/500\n",
    "3/3 [==============================] - 2s 507ms/step - loss: 0.4456 - accuracy: 0.8512 - val_loss: 5.3335 - val_accuracy: 0.1556\n",
    "Epoch 38/500\n",
    "3/3 [==============================] - 2s 656ms/step - loss: 0.6253 - accuracy: 0.7798 - val_loss: 4.2264 - val_accuracy: 0.2000\n",
    "Epoch 39/500\n",
    "3/3 [==============================] - 2s 509ms/step - loss: 0.4732 - accuracy: 0.7917 - val_loss: 3.2454 - val_accuracy: 0.2667\n",
    "Epoch 40/500\n",
    "3/3 [==============================] - 2s 500ms/step - loss: 0.5555 - accuracy: 0.7976 - val_loss: 2.5500 - val_accuracy: 0.3778\n",
    "Epoch 41/500\n",
    "3/3 [==============================] - 2s 647ms/step - loss: 0.4013 - accuracy: 0.8452 - val_loss: 2.1075 - val_accuracy: 0.4667\n",
    "Epoch 42/500\n",
    "3/3 [==============================] - 2s 504ms/step - loss: 0.5340 - accuracy: 0.7917 - val_loss: 1.9407 - val_accuracy: 0.4889\n",
    "Epoch 43/500\n",
    "3/3 [==============================] - 2s 518ms/step - loss: 0.4155 - accuracy: 0.8333 - val_loss: 1.9759 - val_accuracy: 0.5111\n",
    "Epoch 44/500\n",
    "3/3 [==============================] - 2s 516ms/step - loss: 0.4649 - accuracy: 0.8512 - val_loss: 1.9863 - val_accuracy: 0.5333\n",
    "Epoch 45/500\n",
    "3/3 [==============================] - 2s 646ms/step - loss: 0.4634 - accuracy: 0.8214 - val_loss: 1.9085 - val_accuracy: 0.5778\n",
    "Epoch 46/500\n",
    "3/3 [==============================] - 2s 506ms/step - loss: 0.4324 - accuracy: 0.8214 - val_loss: 1.8044 - val_accuracy: 0.5778\n",
    "Epoch 47/500\n",
    "3/3 [==============================] - 2s 515ms/step - loss: 0.3920 - accuracy: 0.8393 - val_loss: 1.7201 - val_accuracy: 0.5778\n",
    "Epoch 48/500\n",
    "3/3 [==============================] - 2s 502ms/step - loss: 0.4417 - accuracy: 0.8571 - val_loss: 1.7768 - val_accuracy: 0.5778\n",
    "Epoch 49/500\n",
    "3/3 [==============================] - 2s 509ms/step - loss: 0.3420 - accuracy: 0.8929 - val_loss: 1.8669 - val_accuracy: 0.5333\n",
    "Epoch 50/500\n",
    "3/3 [==============================] - 2s 520ms/step - loss: 0.3670 - accuracy: 0.8452 - val_loss: 1.8288 - val_accuracy: 0.5333\n",
    "Epoch 51/500\n",
    "3/3 [==============================] - 2s 522ms/step - loss: 0.4620 - accuracy: 0.8810 - val_loss: 1.6138 - val_accuracy: 0.5778\n",
    "Epoch 52/500\n",
    "3/3 [==============================] - 2s 510ms/step - loss: 0.4031 - accuracy: 0.8214 - val_loss: 1.4214 - val_accuracy: 0.6222\n",
    "Epoch 53/500\n",
    "3/3 [==============================] - 2s 511ms/step - loss: 0.4289 - accuracy: 0.8571 - val_loss: 1.2836 - val_accuracy: 0.6444\n",
    "Epoch 54/500\n",
    "3/3 [==============================] - 2s 501ms/step - loss: 0.3713 - accuracy: 0.8631 - val_loss: 1.1692 - val_accuracy: 0.6889\n",
    "Epoch 55/500\n",
    "3/3 [==============================] - 2s 514ms/step - loss: 0.2932 - accuracy: 0.8810 - val_loss: 1.1156 - val_accuracy: 0.7556\n",
    "Epoch 56/500\n",
    "3/3 [==============================] - 2s 515ms/step - loss: 0.3487 - accuracy: 0.8929 - val_loss: 1.1421 - val_accuracy: 0.6889\n",
    "Epoch 57/500\n",
    "3/3 [==============================] - 2s 511ms/step - loss: 0.3672 - accuracy: 0.8214 - val_loss: 1.1841 - val_accuracy: 0.7111\n",
    "Epoch 58/500\n",
    "3/3 [==============================] - 2s 501ms/step - loss: 0.2731 - accuracy: 0.8929 - val_loss: 1.2641 - val_accuracy: 0.6222\n",
    "Epoch 59/500\n",
    "3/3 [==============================] - 2s 522ms/step - loss: 0.4957 - accuracy: 0.8571 - val_loss: 1.2276 - val_accuracy: 0.6444\n",
    "Epoch 60/500\n",
    "3/3 [==============================] - 2s 500ms/step - loss: 0.2461 - accuracy: 0.8988 - val_loss: 1.1203 - val_accuracy: 0.7111\n",
    "Epoch 61/500\n",
    "3/3 [==============================] - 2s 649ms/step - loss: 0.4250 - accuracy: 0.8036 - val_loss: 0.9178 - val_accuracy: 0.7778\n",
    "Epoch 62/500\n",
    "3/3 [==============================] - 2s 648ms/step - loss: 0.2408 - accuracy: 0.9107 - val_loss: 0.8166 - val_accuracy: 0.7778\n",
    "Epoch 63/500\n",
    "3/3 [==============================] - 2s 496ms/step - loss: 0.3249 - accuracy: 0.8869 - val_loss: 0.8127 - val_accuracy: 0.8000\n",
    "Epoch 64/500\n",
    "3/3 [==============================] - 2s 644ms/step - loss: 0.2250 - accuracy: 0.9226 - val_loss: 0.8984 - val_accuracy: 0.8000\n",
    "Epoch 65/500\n",
    "3/3 [==============================] - 2s 508ms/step - loss: 0.3126 - accuracy: 0.8929 - val_loss: 0.9692 - val_accuracy: 0.7556\n",
    "Epoch 66/500\n",
    "3/3 [==============================] - 2s 645ms/step - loss: 0.3086 - accuracy: 0.8869 - val_loss: 1.0460 - val_accuracy: 0.7556\n",
    "Epoch 67/500\n",
    "3/3 [==============================] - 2s 508ms/step - loss: 0.3338 - accuracy: 0.8631 - val_loss: 1.1399 - val_accuracy: 0.7111\n",
    "Epoch 68/500\n",
    "3/3 [==============================] - 2s 519ms/step - loss: 0.2421 - accuracy: 0.8988 - val_loss: 1.0809 - val_accuracy: 0.7333\n",
    "Epoch 69/500\n",
    "3/3 [==============================] - 2s 510ms/step - loss: 0.2961 - accuracy: 0.8750 - val_loss: 0.9842 - val_accuracy: 0.8222\n",
    "Epoch 70/500\n",
    "3/3 [==============================] - 2s 499ms/step - loss: 0.3047 - accuracy: 0.8869 - val_loss: 0.8731 - val_accuracy: 0.8222\n",
    "Epoch 71/500\n",
    "3/3 [==============================] - 2s 518ms/step - loss: 0.2736 - accuracy: 0.8869 - val_loss: 0.7848 - val_accuracy: 0.8222\n",
    "Epoch 72/500\n",
    "3/3 [==============================] - 2s 504ms/step - loss: 0.2940 - accuracy: 0.8690 - val_loss: 0.7545 - val_accuracy: 0.8444\n",
    "Epoch 73/500\n",
    "3/3 [==============================] - 2s 643ms/step - loss: 0.3094 - accuracy: 0.8988 - val_loss: 0.7750 - val_accuracy: 0.8222\n",
    "Epoch 74/500\n",
    "3/3 [==============================] - 2s 512ms/step - loss: 0.3446 - accuracy: 0.8631 - val_loss: 0.8068 - val_accuracy: 0.7778\n",
    "Epoch 75/500\n",
    "3/3 [==============================] - 2s 644ms/step - loss: 0.3274 - accuracy: 0.8988 - val_loss: 0.8931 - val_accuracy: 0.7778\n",
    "Epoch 76/500\n",
    "3/3 [==============================] - 2s 516ms/step - loss: 0.3185 - accuracy: 0.8929 - val_loss: 1.4074 - val_accuracy: 0.6889\n",
    "Epoch 77/500\n",
    "3/3 [==============================] - 2s 646ms/step - loss: 0.2246 - accuracy: 0.9226 - val_loss: 1.6744 - val_accuracy: 0.6667\n",
    "Epoch 78/500\n",
    "3/3 [==============================] - 2s 505ms/step - loss: 0.2179 - accuracy: 0.9226 - val_loss: 1.8886 - val_accuracy: 0.6667\n",
    "Epoch 79/500\n",
    "3/3 [==============================] - 2s 661ms/step - loss: 0.2979 - accuracy: 0.8750 - val_loss: 1.9724 - val_accuracy: 0.6667\n",
    "Epoch 80/500\n",
    "3/3 [==============================] - 2s 509ms/step - loss: 0.2902 - accuracy: 0.9226 - val_loss: 1.8842 - val_accuracy: 0.6444\n",
    "Epoch 81/500\n",
    "3/3 [==============================] - 2s 652ms/step - loss: 0.2756 - accuracy: 0.8988 - val_loss: 1.6335 - val_accuracy: 0.6667\n",
    "Epoch 82/500\n",
    "3/3 [==============================] - 2s 659ms/step - loss: 0.2079 - accuracy: 0.9345 - val_loss: 1.4145 - val_accuracy: 0.7111\n",
    "Epoch 83/500\n",
    "3/3 [==============================] - 2s 516ms/step - loss: 0.2120 - accuracy: 0.9167 - val_loss: 1.1923 - val_accuracy: 0.7333\n",
    "Epoch 84/500\n",
    "3/3 [==============================] - 2s 515ms/step - loss: 0.2137 - accuracy: 0.9167 - val_loss: 0.9911 - val_accuracy: 0.7778\n",
    "Epoch 85/500\n",
    "3/3 [==============================] - 2s 641ms/step - loss: 0.2385 - accuracy: 0.9107 - val_loss: 0.7918 - val_accuracy: 0.8444\n",
    "Epoch 86/500\n",
    "3/3 [==============================] - 2s 497ms/step - loss: 0.2323 - accuracy: 0.9048 - val_loss: 0.6808 - val_accuracy: 0.8667\n",
    "Epoch 87/500\n",
    "3/3 [==============================] - 2s 505ms/step - loss: 0.2451 - accuracy: 0.9226 - val_loss: 0.6378 - val_accuracy: 0.8667\n",
    "Epoch 88/500\n",
    "3/3 [==============================] - 2s 514ms/step - loss: 0.2595 - accuracy: 0.8988 - val_loss: 0.6488 - val_accuracy: 0.8444\n",
    "Epoch 89/500\n",
    "3/3 [==============================] - 2s 507ms/step - loss: 0.1956 - accuracy: 0.9345 - val_loss: 0.6951 - val_accuracy: 0.8444\n",
    "Epoch 90/500\n",
    "3/3 [==============================] - 2s 516ms/step - loss: 0.2912 - accuracy: 0.8929 - val_loss: 0.6191 - val_accuracy: 0.8667\n",
    "Epoch 91/500\n",
    "3/3 [==============================] - 2s 647ms/step - loss: 0.2436 - accuracy: 0.9405 - val_loss: 0.6550 - val_accuracy: 0.8444\n",
    "\n",
    "Epoch 92/500\n",
    "3/3 [==============================] - 2s 495ms/step - loss: 0.2557 - accuracy: 0.9107 - val_loss: 0.7674 - val_accuracy: 0.8222\n",
    "Epoch 93/500\n",
    "3/3 [==============================] - 2s 655ms/step - loss: 0.2738 - accuracy: 0.9107 - val_loss: 0.8128 - val_accuracy: 0.8222\n",
    "Epoch 94/500\n",
    "3/3 [==============================] - 2s 646ms/step - loss: 0.2050 - accuracy: 0.8988 - val_loss: 0.8177 - val_accuracy: 0.8222\n",
    "Epoch 95/500\n",
    "3/3 [==============================] - 2s 508ms/step - loss: 0.2134 - accuracy: 0.9167 - val_loss: 0.8144 - val_accuracy: 0.8444\n",
    "Epoch 96/500\n",
    "3/3 [==============================] - 2s 515ms/step - loss: 0.1884 - accuracy: 0.9167 - val_loss: 0.7992 - val_accuracy: 0.8444\n",
    "Epoch 97/500\n",
    "3/3 [==============================] - 2s 514ms/step - loss: 0.2855 - accuracy: 0.8810 - val_loss: 0.8432 - val_accuracy: 0.8000\n",
    "Epoch 98/500\n",
    "3/3 [==============================] - 2s 652ms/step - loss: 0.1637 - accuracy: 0.9286 - val_loss: 0.9309 - val_accuracy: 0.7556\n",
    "Epoch 99/500\n",
    "3/3 [==============================] - 2s 644ms/step - loss: 0.2146 - accuracy: 0.9226 - val_loss: 0.9235 - val_accuracy: 0.8222\n",
    "Epoch 100/500\n",
    "3/3 [==============================] - 2s 494ms/step - loss: 0.3652 - accuracy: 0.8929 - val_loss: 1.0249 - val_accuracy: 0.8222\n",
    "Epoch 101/500\n",
    "3/3 [==============================] - 2s 503ms/step - loss: 0.2612 - accuracy: 0.9167 - val_loss: 1.1158 - val_accuracy: 0.8000\n",
    "Epoch 102/500\n",
    "3/3 [==============================] - 2s 651ms/step - loss: 0.2135 - accuracy: 0.9226 - val_loss: 1.1974 - val_accuracy: 0.7556\n",
    "Epoch 103/500\n",
    "3/3 [==============================] - 2s 500ms/step - loss: 0.3110 - accuracy: 0.8631 - val_loss: 1.1228 - val_accuracy: 0.7778\n",
    "Epoch 104/500\n",
    "3/3 [==============================] - 2s 653ms/step - loss: 0.2435 - accuracy: 0.9226 - val_loss: 0.9906 - val_accuracy: 0.8222\n",
    "Epoch 105/500\n",
    "3/3 [==============================] - 2s 516ms/step - loss: 0.2396 - accuracy: 0.9107 - val_loss: 0.8469 - val_accuracy: 0.8444\n",
    "Epoch 106/500\n",
    "3/3 [==============================] - 2s 517ms/step - loss: 0.1999 - accuracy: 0.9226 - val_loss: 0.7883 - val_accuracy: 0.8667\n",
    "Epoch 107/500\n",
    "3/3 [==============================] - 2s 651ms/step - loss: 0.2348 - accuracy: 0.9107 - val_loss: 0.7593 - val_accuracy: 0.8667\n",
    "Epoch 108/500\n",
    "3/3 [==============================] - 2s 659ms/step - loss: 0.1837 - accuracy: 0.9286 - val_loss: 0.7681 - val_accuracy: 0.8667\n",
    "Epoch 109/500\n",
    "3/3 [==============================] - 2s 513ms/step - loss: 0.2415 - accuracy: 0.8988 - val_loss: 0.7866 - val_accuracy: 0.8667\n",
    "Epoch 110/500\n",
    "3/3 [==============================] - 2s 654ms/step - loss: 0.3452 - accuracy: 0.8929 - val_loss: 0.7938 - val_accuracy: 0.8667\n",
    "Epoch 111/500\n",
    "3/3 [==============================] - 2s 646ms/step - loss: 0.1604 - accuracy: 0.9226 - val_loss: 0.7833 - val_accuracy: 0.8889\n",
    "Epoch 112/500\n",
    "3/3 [==============================] - 2s 501ms/step - loss: 0.2566 - accuracy: 0.8988 - val_loss: 0.7881 - val_accuracy: 0.9111\n",
    "Epoch 113/500\n",
    "3/3 [==============================] - 2s 513ms/step - loss: 0.1365 - accuracy: 0.9524 - val_loss: 0.7988 - val_accuracy: 0.9111\n",
    "Epoch 114/500\n",
    "3/3 [==============================] - 2s 505ms/step - loss: 0.2483 - accuracy: 0.9226 - val_loss: 0.7928 - val_accuracy: 0.9111\n",
    "Epoch 115/500\n",
    "3/3 [==============================] - 2s 643ms/step - loss: 0.1289 - accuracy: 0.9643 - val_loss: 0.8252 - val_accuracy: 0.9111\n",
    "Epoch 116/500\n",
    "3/3 [==============================] - 2s 651ms/step - loss: 0.2012 - accuracy: 0.9226 - val_loss: 0.8503 - val_accuracy: 0.9111\n",
    "Epoch 117/500\n",
    "3/3 [==============================] - 2s 520ms/step - loss: 0.2754 - accuracy: 0.9226 - val_loss: 0.9252 - val_accuracy: 0.8667\n",
    "Epoch 118/500\n",
    "3/3 [==============================] - 2s 520ms/step - loss: 0.2387 - accuracy: 0.8988 - val_loss: 0.8773 - val_accuracy: 0.8667\n",
    "Epoch 119/500\n",
    "3/3 [==============================] - 2s 506ms/step - loss: 0.2809 - accuracy: 0.9286 - val_loss: 0.7984 - val_accuracy: 0.8889\n",
    "Epoch 120/500\n",
    "3/3 [==============================] - 2s 504ms/step - loss: 0.1794 - accuracy: 0.9167 - val_loss: 0.7354 - val_accuracy: 0.8889\n",
    "Epoch 121/500\n",
    "3/3 [==============================] - 2s 506ms/step - loss: 0.2124 - accuracy: 0.9286 - val_loss: 5.5376 - val_accuracy: 0.3333\n",
    "Epoch 122/500\n",
    "3/3 [==============================] - 2s 516ms/step - loss: 0.1404 - accuracy: 0.9405 - val_loss: 20.1213 - val_accuracy: 0.1778\n",
    "Epoch 123/500\n",
    "3/3 [==============================] - 2s 502ms/step - loss: 0.1491 - accuracy: 0.9405 - val_loss: 14.1800 - val_accuracy: 0.2000\n",
    "Epoch 124/500\n",
    "3/3 [==============================] - 2s 648ms/step - loss: 0.1842 - accuracy: 0.9345 - val_loss: 10.6383 - val_accuracy: 0.2222\n",
    "Epoch 125/500\n",
    "3/3 [==============================] - 2s 515ms/step - loss: 0.1358 - accuracy: 0.9583 - val_loss: 10.1534 - val_accuracy: 0.2000\n",
    "Epoch 126/500\n",
    "3/3 [==============================] - 2s 518ms/step - loss: 0.3128 - accuracy: 0.8929 - val_loss: 8.9694 - val_accuracy: 0.2222\n",
    "Epoch 127/500\n",
    "3/3 [==============================] - 2s 510ms/step - loss: 0.2208 - accuracy: 0.9286 - val_loss: 6.3845 - val_accuracy: 0.2889\n",
    "Epoch 128/500\n",
    "3/3 [==============================] - 2s 515ms/step - loss: 0.2224 - accuracy: 0.9286 - val_loss: 4.2781 - val_accuracy: 0.3556\n",
    "Epoch 129/500\n",
    "3/3 [==============================] - 2s 504ms/step - loss: 0.3019 - accuracy: 0.8810 - val_loss: 2.5080 - val_accuracy: 0.5333\n",
    "Epoch 130/500\n",
    "3/3 [==============================] - 2s 509ms/step - loss: 0.1264 - accuracy: 0.9702 - val_loss: 2.0888 - val_accuracy: 0.6667\n",
    "Epoch 131/500\n",
    "3/3 [==============================] - 2s 496ms/step - loss: 0.1996 - accuracy: 0.9464 - val_loss: 2.2071 - val_accuracy: 0.6667\n",
    "Epoch 132/500\n",
    "3/3 [==============================] - 2s 647ms/step - loss: 0.2137 - accuracy: 0.9048 - val_loss: 2.1784 - val_accuracy: 0.7333\n",
    "Epoch 133/500\n",
    "3/3 [==============================] - 2s 513ms/step - loss: 0.2660 - accuracy: 0.9107 - val_loss: 1.9325 - val_accuracy: 0.8000\n",
    "Epoch 134/500\n",
    "3/3 [==============================] - 2s 496ms/step - loss: 0.1870 - accuracy: 0.9226 - val_loss: 1.5741 - val_accuracy: 0.7556\n",
    "Epoch 135/500\n",
    "3/3 [==============================] - 2s 651ms/step - loss: 0.1288 - accuracy: 0.9583 - val_loss: 1.3323 - val_accuracy: 0.7556\n",
    "Epoch 136/500\n",
    "3/3 [==============================] - 2s 503ms/step - loss: 0.3575 - accuracy: 0.8988 - val_loss: 1.2390 - val_accuracy: 0.7778\n",
    "Epoch 137/500\n",
    "3/3 [==============================] - 2s 512ms/step - loss: 0.2224 - accuracy: 0.9226 - val_loss: 1.1639 - val_accuracy: 0.8000\n",
    "Epoch 138/500\n",
    "3/3 [==============================] - 2s 515ms/step - loss: 0.2544 - accuracy: 0.9167 - val_loss: 1.2671 - val_accuracy: 0.8000\n",
    "Epoch 139/500\n",
    "3/3 [==============================] - 2s 503ms/step - loss: 0.3037 - accuracy: 0.8750 - val_loss: 1.3198 - val_accuracy: 0.8222\n",
    "Epoch 140/500\n",
    "3/3 [==============================] - 2s 503ms/step - loss: 0.2052 - accuracy: 0.9464 - val_loss: 1.3499 - val_accuracy: 0.7778\n",
    "Epoch 141/500\n",
    "3/3 [==============================] - 2s 510ms/step - loss: 0.2026 - accuracy: 0.9107 - val_loss: 1.2727 - val_accuracy: 0.8222\n",
    "Epoch 142/500\n",
    "3/3 [==============================] - 2s 521ms/step - loss: 0.2101 - accuracy: 0.9286 - val_loss: 1.1061 - val_accuracy: 0.8444\n",
    "Epoch 143/500\n",
    "3/3 [==============================] - 2s 645ms/step - loss: 0.2058 - accuracy: 0.9226 - val_loss: 0.9403 - val_accuracy: 0.8444\n",
    "Epoch 144/500\n",
    "3/3 [==============================] - 2s 523ms/step - loss: 0.1969 - accuracy: 0.9167 - val_loss: 0.8671 - val_accuracy: 0.8889\n",
    "Epoch 145/500\n",
    "3/3 [==============================] - 2s 511ms/step - loss: 0.1701 - accuracy: 0.9167 - val_loss: 0.8529 - val_accuracy: 0.8889\n",
    "Epoch 146/500\n",
    "3/3 [==============================] - 2s 517ms/step - loss: 0.1851 - accuracy: 0.9345 - val_loss: 0.8358 - val_accuracy: 0.8889\n",
    "Epoch 147/500\n",
    "3/3 [==============================] - 2s 500ms/step - loss: 0.1996 - accuracy: 0.9167 - val_loss: 0.8210 - val_accuracy: 0.8444\n",
    "Epoch 148/500\n",
    "3/3 [==============================] - 2s 513ms/step - loss: 0.1883 - accuracy: 0.9405 - val_loss: 0.9347 - val_accuracy: 0.8444\n",
    "Epoch 149/500\n",
    "\n",
    "3/3 [==============================] - 2s 507ms/step - loss: 0.1426 - accuracy: 0.9345 - val_loss: 1.0047 - val_accuracy: 0.8444\n",
    "Epoch 150/500\n",
    "3/3 [==============================] - 2s 663ms/step - loss: 0.1669 - accuracy: 0.9524 - val_loss: 1.0558 - val_accuracy: 0.8444\n",
    "Epoch 151/500\n",
    "3/3 [==============================] - 2s 508ms/step - loss: 0.0952 - accuracy: 0.9643 - val_loss: 1.0220 - val_accuracy: 0.8444\n",
    "Epoch 152/500\n",
    "3/3 [==============================] - 2s 515ms/step - loss: 0.1184 - accuracy: 0.9583 - val_loss: 0.9885 - val_accuracy: 0.8444\n",
    "Epoch 153/500\n",
    "3/3 [==============================] - 2s 501ms/step - loss: 0.1832 - accuracy: 0.9286 - val_loss: 0.9077 - val_accuracy: 0.8444\n",
    "Epoch 154/500\n",
    "3/3 [==============================] - 2s 502ms/step - loss: 0.1576 - accuracy: 0.9286 - val_loss: 0.8658 - val_accuracy: 0.8889\n",
    "Epoch 155/500\n",
    "3/3 [==============================] - 2s 651ms/step - loss: 0.1604 - accuracy: 0.9405 - val_loss: 0.8757 - val_accuracy: 0.8889\n",
    "Epoch 156/500\n",
    "3/3 [==============================] - 2s 518ms/step - loss: 0.1855 - accuracy: 0.9167 - val_loss: 0.9151 - val_accuracy: 0.8889\n",
    "Epoch 157/500\n",
    "3/3 [==============================] - 2s 645ms/step - loss: 0.1201 - accuracy: 0.9702 - val_loss: 0.9791 - val_accuracy: 0.8667\n",
    "Epoch 158/500\n",
    "3/3 [==============================] - 2s 648ms/step - loss: 0.2017 - accuracy: 0.9583 - val_loss: 1.0206 - val_accuracy: 0.8667\n",
    "Epoch 159/500\n",
    "3/3 [==============================] - 2s 519ms/step - loss: 0.1216 - accuracy: 0.9405 - val_loss: 1.0457 - val_accuracy: 0.8667\n",
    "Epoch 160/500\n",
    "3/3 [==============================] - 2s 510ms/step - loss: 0.1558 - accuracy: 0.9524 - val_loss: 1.0110 - val_accuracy: 0.8889\n",
    "Epoch 161/500\n",
    "3/3 [==============================] - 2s 649ms/step - loss: 0.1538 - accuracy: 0.9524 - val_loss: 0.9741 - val_accuracy: 0.8889\n",
    "Epoch 162/500\n",
    "3/3 [==============================] - 2s 504ms/step - loss: 0.1556 - accuracy: 0.9643 - val_loss: 0.9353 - val_accuracy: 0.8889\n",
    "Epoch 163/500\n",
    "3/3 [==============================] - 2s 512ms/step - loss: 0.1064 - accuracy: 0.9702 - val_loss: 0.9215 - val_accuracy: 0.8667\n",
    "Epoch 164/500\n",
    "3/3 [==============================] - 2s 513ms/step - loss: 0.1825 - accuracy: 0.9345 - val_loss: 0.9411 - val_accuracy: 0.9111\n",
    "Epoch 165/500\n",
    "3/3 [==============================] - 2s 646ms/step - loss: 0.1837 - accuracy: 0.9464 - val_loss: 0.9997 - val_accuracy: 0.8889\n",
    "Epoch 166/500\n",
    "3/3 [==============================] - 2s 517ms/step - loss: 0.1567 - accuracy: 0.9345 - val_loss: 1.0350 - val_accuracy: 0.8889\n",
    "Epoch 167/500\n",
    "3/3 [==============================] - 2s 510ms/step - loss: 0.1093 - accuracy: 0.9643 - val_loss: 1.0613 - val_accuracy: 0.8889\n",
    "Epoch 168/500\n",
    "3/3 [==============================] - 2s 520ms/step - loss: 0.2159 - accuracy: 0.9286 - val_loss: 1.0124 - val_accuracy: 0.8667\n",
    "Epoch 169/500\n",
    "3/3 [==============================] - 2s 505ms/step - loss: 0.1552 - accuracy: 0.9345 - val_loss: 0.9845 - val_accuracy: 0.8444\n",
    "Epoch 170/500\n",
    "3/3 [==============================] - 2s 648ms/step - loss: 0.1763 - accuracy: 0.9464 - val_loss: 1.0166 - val_accuracy: 0.8667\n",
    "Epoch 171/500\n",
    "3/3 [==============================] - 2s 499ms/step - loss: 0.1341 - accuracy: 0.9524 - val_loss: 1.1297 - val_accuracy: 0.8444\n",
    "Epoch 172/500\n",
    "3/3 [==============================] - 2s 514ms/step - loss: 0.0623 - accuracy: 0.9762 - val_loss: 1.2223 - val_accuracy: 0.8444\n",
    "Epoch 173/500\n",
    "3/3 [==============================] - 2s 652ms/step - loss: 0.1340 - accuracy: 0.9643 - val_loss: 1.2877 - val_accuracy: 0.8000\n",
    "Epoch 174/500\n",
    "3/3 [==============================] - 2s 646ms/step - loss: 0.1825 - accuracy: 0.9167 - val_loss: 1.0968 - val_accuracy: 0.8444\n",
    "Epoch 175/500\n",
    "3/3 [==============================] - 2s 515ms/step - loss: 0.0943 - accuracy: 0.9643 - val_loss: 1.0224 - val_accuracy: 0.8222\n",
    "Epoch 176/500\n",
    "3/3 [==============================] - 2s 503ms/step - loss: 0.1010 - accuracy: 0.9702 - val_loss: 0.9201 - val_accuracy: 0.8444\n",
    "Epoch 177/500\n",
    "3/3 [==============================] - 2s 655ms/step - loss: 0.2272 - accuracy: 0.9345 - val_loss: 0.9087 - val_accuracy: 0.8444\n",
    "Epoch 178/500\n",
    "3/3 [==============================] - 2s 499ms/step - loss: 0.1280 - accuracy: 0.9464 - val_loss: 0.9291 - val_accuracy: 0.8444\n",
    "Epoch 179/500\n",
    "3/3 [==============================] - 2s 655ms/step - loss: 0.2028 - accuracy: 0.9345 - val_loss: 0.8947 - val_accuracy: 0.8000\n",
    "Epoch 180/500\n",
    "3/3 [==============================] - 2s 656ms/step - loss: 0.1091 - accuracy: 0.9762 - val_loss: 0.9557 - val_accuracy: 0.8000\n",
    "Epoch 181/500\n",
    "3/3 [==============================] - 2s 654ms/step - loss: 0.0971 - accuracy: 0.9524 - val_loss: 1.0007 - val_accuracy: 0.8000\n",
    "Epoch 182/500\n",
    "3/3 [==============================] - 2s 506ms/step - loss: 0.1497 - accuracy: 0.9524 - val_loss: 1.0334 - val_accuracy: 0.8000\n",
    "Epoch 183/500\n",
    "3/3 [==============================] - 2s 514ms/step - loss: 0.2113 - accuracy: 0.9226 - val_loss: 1.0854 - val_accuracy: 0.8000\n",
    "Epoch 184/500\n",
    "3/3 [==============================] - 2s 501ms/step - loss: 0.1005 - accuracy: 0.9583 - val_loss: 1.1017 - val_accuracy: 0.8000\n",
    "Epoch 185/500\n",
    "3/3 [==============================] - 2s 649ms/step - loss: 0.0986 - accuracy: 0.9643 - val_loss: 1.0722 - val_accuracy: 0.8000\n",
    "Epoch 186/500\n",
    "3/3 [==============================] - 2s 503ms/step - loss: 0.1538 - accuracy: 0.9405 - val_loss: 1.0042 - val_accuracy: 0.8222\n",
    "Epoch 187/500\n",
    "3/3 [==============================] - 2s 653ms/step - loss: 0.1103 - accuracy: 0.9702 - val_loss: 0.9726 - val_accuracy: 0.8889\n",
    "Epoch 188/500\n",
    "3/3 [==============================] - 2s 640ms/step - loss: 0.1115 - accuracy: 0.9345 - val_loss: 0.9996 - val_accuracy: 0.8889\n",
    "Epoch 189/500\n",
    "3/3 [==============================] - 2s 657ms/step - loss: 0.0721 - accuracy: 0.9762 - val_loss: 1.0203 - val_accuracy: 0.8889\n",
    "Epoch 190/500\n",
    "3/3 [==============================] - 2s 661ms/step - loss: 0.1169 - accuracy: 0.9702 - val_loss: 1.0565 - val_accuracy: 0.8889\n",
    "Epoch 191/500\n",
    "3/3 [==============================] - 2s 648ms/step - loss: 0.0804 - accuracy: 0.9702 - val_loss: 1.0505 - val_accuracy: 0.8667\n",
    "Epoch 192/500\n",
    "3/3 [==============================] - 2s 501ms/step - loss: 0.1827 - accuracy: 0.9464 - val_loss: 1.0777 - val_accuracy: 0.8222\n",
    "Epoch 193/500\n",
    "3/3 [==============================] - 2s 498ms/step - loss: 0.0988 - accuracy: 0.9583 - val_loss: 1.0677 - val_accuracy: 0.8000\n",
    "Epoch 194/500\n",
    "3/3 [==============================] - 2s 518ms/step - loss: 0.0827 - accuracy: 0.9583 - val_loss: 0.9990 - val_accuracy: 0.8222\n",
    "Epoch 195/500\n",
    "3/3 [==============================] - 2s 517ms/step - loss: 0.1044 - accuracy: 0.9762 - val_loss: 0.9358 - val_accuracy: 0.8889\n",
    "Epoch 196/500\n",
    "3/3 [==============================] - 2s 645ms/step - loss: 0.0710 - accuracy: 0.9702 - val_loss: 0.9031 - val_accuracy: 0.8667\n",
    "Epoch 197/500\n",
    "3/3 [==============================] - 2s 505ms/step - loss: 0.0938 - accuracy: 0.9583 - val_loss: 0.8982 - val_accuracy: 0.8667\n",
    "Epoch 198/500\n",
    "3/3 [==============================] - 2s 645ms/step - loss: 0.0803 - accuracy: 0.9643 - val_loss: 0.8689 - val_accuracy: 0.8889\n",
    "Epoch 199/500\n",
    "3/3 [==============================] - 2s 517ms/step - loss: 0.1656 - accuracy: 0.9464 - val_loss: 0.7973 - val_accuracy: 0.8667\n",
    "Epoch 200/500\n",
    "3/3 [==============================] - 2s 652ms/step - loss: 0.0760 - accuracy: 0.9702 - val_loss: 0.7156 - val_accuracy: 0.8889\n",
    "Epoch 201/500\n",
    "3/3 [==============================] - 2s 645ms/step - loss: 0.0812 - accuracy: 0.9524 - val_loss: 0.6701 - val_accuracy: 0.8889\n",
    "Epoch 202/500\n",
    "3/3 [==============================] - 2s 510ms/step - loss: 0.1158 - accuracy: 0.9643 - val_loss: 0.6134 - val_accuracy: 0.8889\n",
    "Epoch 203/500\n",
    "3/3 [==============================] - 2s 510ms/step - loss: 0.1288 - accuracy: 0.9405 - val_loss: 0.5618 - val_accuracy: 0.9111\n",
    "Epoch 204/500\n",
    "3/3 [==============================] - 2s 646ms/step - loss: 0.0751 - accuracy: 0.9821 - val_loss: 0.5655 - val_accuracy: 0.9111\n",
    "Epoch 205/500\n",
    "3/3 [==============================] - 2s 506ms/step - loss: 0.1015 - accuracy: 0.9524 - val_loss: 0.5840 - val_accuracy: 0.8889\n",
    "Epoch 206/500\n",
    "\n",
    "3/3 [==============================] - 2s 511ms/step - loss: 0.1187 - accuracy: 0.9524 - val_loss: 0.6497 - val_accuracy: 0.8889\n",
    "Epoch 207/500\n",
    "3/3 [==============================] - 2s 492ms/step - loss: 0.0843 - accuracy: 0.9762 - val_loss: 0.7335 - val_accuracy: 0.8889\n",
    "Epoch 208/500\n",
    "3/3 [==============================] - 2s 516ms/step - loss: 0.0593 - accuracy: 0.9881 - val_loss: 0.8204 - val_accuracy: 0.9111\n",
    "Epoch 209/500\n",
    "3/3 [==============================] - 2s 518ms/step - loss: 0.1013 - accuracy: 0.9643 - val_loss: 0.8910 - val_accuracy: 0.8889\n",
    "Epoch 210/500\n",
    "3/3 [==============================] - 2s 642ms/step - loss: 0.0851 - accuracy: 0.9702 - val_loss: 0.9126 - val_accuracy: 0.8889\n",
    "Epoch 211/500\n",
    "3/3 [==============================] - 2s 523ms/step - loss: 0.1479 - accuracy: 0.9583 - val_loss: 0.9811 - val_accuracy: 0.8444\n",
    "Epoch 212/500\n",
    "3/3 [==============================] - 2s 501ms/step - loss: 0.1095 - accuracy: 0.9643 - val_loss: 1.0827 - val_accuracy: 0.8667\n",
    "Epoch 213/500\n",
    "3/3 [==============================] - 2s 508ms/step - loss: 0.0721 - accuracy: 0.9762 - val_loss: 1.1992 - val_accuracy: 0.7556\n",
    "Epoch 214/500\n",
    "3/3 [==============================] - 2s 646ms/step - loss: 0.0598 - accuracy: 0.9821 - val_loss: 1.1833 - val_accuracy: 0.7556\n",
    "Epoch 215/500\n",
    "3/3 [==============================] - 2s 505ms/step - loss: 0.0807 - accuracy: 0.9702 - val_loss: 83.5435 - val_accuracy: 0.1556\n",
    "Epoch 216/500\n",
    "3/3 [==============================] - 2s 510ms/step - loss: 0.1113 - accuracy: 0.9524 - val_loss: 45.6235 - val_accuracy: 0.1556\n",
    "Epoch 217/500\n",
    "3/3 [==============================] - 2s 657ms/step - loss: 0.0970 - accuracy: 0.9643 - val_loss: 26.7341 - val_accuracy: 0.1556\n",
    "Epoch 218/500\n",
    "3/3 [==============================] - 2s 499ms/step - loss: 0.1131 - accuracy: 0.9583 - val_loss: 15.6697 - val_accuracy: 0.2222\n",
    "Epoch 219/500\n",
    "3/3 [==============================] - 2s 503ms/step - loss: 0.1344 - accuracy: 0.9405 - val_loss: 9.7742 - val_accuracy: 0.3778\n",
    "Epoch 220/500\n",
    "3/3 [==============================] - 2s 503ms/step - loss: 0.1619 - accuracy: 0.9524 - val_loss: 6.8447 - val_accuracy: 0.4889\n",
    "Epoch 221/500\n",
    "3/3 [==============================] - 2s 500ms/step - loss: 0.1838 - accuracy: 0.9464 - val_loss: 5.1105 - val_accuracy: 0.6222\n",
    "Epoch 222/500\n",
    "3/3 [==============================] - 2s 652ms/step - loss: 0.0726 - accuracy: 0.9643 - val_loss: 4.1914 - val_accuracy: 0.6222\n",
    "Epoch 223/500\n",
    "3/3 [==============================] - 2s 651ms/step - loss: 0.0619 - accuracy: 0.9762 - val_loss: 3.6118 - val_accuracy: 0.6667\n",
    "Epoch 224/500\n",
    "3/3 [==============================] - 2s 513ms/step - loss: 0.0994 - accuracy: 0.9762 - val_loss: 3.1339 - val_accuracy: 0.7111\n",
    "Epoch 225/500\n",
    "3/3 [==============================] - 2s 510ms/step - loss: 0.1259 - accuracy: 0.9702 - val_loss: 2.8180 - val_accuracy: 0.7111\n",
    "Epoch 226/500\n",
    "3/3 [==============================] - 2s 500ms/step - loss: 0.0983 - accuracy: 0.9583 - val_loss: 2.5283 - val_accuracy: 0.7333\n",
    "Epoch 227/500\n",
    "3/3 [==============================] - 2s 503ms/step - loss: 0.0894 - accuracy: 0.9702 - val_loss: 2.2000 - val_accuracy: 0.7333\n",
    "Epoch 228/500\n",
    "3/3 [==============================] - 2s 500ms/step - loss: 0.0958 - accuracy: 0.9583 - val_loss: 1.9458 - val_accuracy: 0.7333\n",
    "Epoch 229/500\n",
    "3/3 [==============================] - 2s 498ms/step - loss: 0.1486 - accuracy: 0.9464 - val_loss: 1.6882 - val_accuracy: 0.7333\n",
    "Epoch 230/500\n",
    "3/3 [==============================] - 2s 498ms/step - loss: 0.0719 - accuracy: 0.9702 - val_loss: 1.5793 - val_accuracy: 0.7778\n",
    "Epoch 231/500\n",
    "3/3 [==============================] - 2s 522ms/step - loss: 0.0512 - accuracy: 0.9881 - val_loss: 1.5356 - val_accuracy: 0.7778\n",
    "Epoch 232/500\n",
    "3/3 [==============================] - 2s 496ms/step - loss: 0.1282 - accuracy: 0.9643 - val_loss: 1.4355 - val_accuracy: 0.7778\n",
    "Epoch 233/500\n",
    "3/3 [==============================] - 2s 656ms/step - loss: 0.1242 - accuracy: 0.9405 - val_loss: 1.3408 - val_accuracy: 0.7778\n",
    "Epoch 234/500\n",
    "3/3 [==============================] - 2s 492ms/step - loss: 0.1133 - accuracy: 0.9702 - val_loss: 1.3118 - val_accuracy: 0.7778\n",
    "Epoch 235/500\n",
    "3/3 [==============================] - 2s 650ms/step - loss: 0.1655 - accuracy: 0.9405 - val_loss: 1.2448 - val_accuracy: 0.7778\n",
    "Epoch 236/500\n",
    "3/3 [==============================] - 2s 496ms/step - loss: 0.0602 - accuracy: 0.9762 - val_loss: 1.1328 - val_accuracy: 0.8444\n",
    "Epoch 237/500\n",
    "3/3 [==============================] - 2s 511ms/step - loss: 0.0706 - accuracy: 0.9702 - val_loss: 1.0754 - val_accuracy: 0.8444\n",
    "Epoch 238/500\n",
    "3/3 [==============================] - 2s 503ms/step - loss: 0.1525 - accuracy: 0.9583 - val_loss: 1.0469 - val_accuracy: 0.8444\n",
    "Epoch 239/500\n",
    "3/3 [==============================] - 2s 653ms/step - loss: 0.0894 - accuracy: 0.9702 - val_loss: 1.0276 - val_accuracy: 0.8444\n",
    "Epoch 240/500\n",
    "3/3 [==============================] - 2s 510ms/step - loss: 0.0589 - accuracy: 0.9881 - val_loss: 1.0529 - val_accuracy: 0.8222\n",
    "Epoch 241/500\n",
    "3/3 [==============================] - 2s 506ms/step - loss: 0.2439 - accuracy: 0.9286 - val_loss: 1.1079 - val_accuracy: 0.8000\n",
    "Epoch 242/500\n",
    "3/3 [==============================] - 2s 501ms/step - loss: 0.1471 - accuracy: 0.9583 - val_loss: 1.1038 - val_accuracy: 0.8000\n",
    "Epoch 243/500\n",
    "3/3 [==============================] - 2s 647ms/step - loss: 0.1621 - accuracy: 0.9405 - val_loss: 1.0583 - val_accuracy: 0.8222\n",
    "Epoch 244/500\n",
    "3/3 [==============================] - 2s 499ms/step - loss: 0.0808 - accuracy: 0.9643 - val_loss: 0.9987 - val_accuracy: 0.8667\n",
    "Epoch 245/500\n",
    "3/3 [==============================] - 2s 507ms/step - loss: 0.0847 - accuracy: 0.9643 - val_loss: 0.9041 - val_accuracy: 0.8889\n",
    "Epoch 246/500\n",
    "3/3 [==============================] - 2s 644ms/step - loss: 0.1145 - accuracy: 0.9643 - val_loss: 0.9179 - val_accuracy: 0.8667\n",
    "Epoch 247/500\n",
    "3/3 [==============================] - 2s 648ms/step - loss: 0.1043 - accuracy: 0.9643 - val_loss: 0.8949 - val_accuracy: 0.8889\n",
    "Epoch 248/500\n",
    "3/3 [==============================] - 2s 650ms/step - loss: 0.0834 - accuracy: 0.9762 - val_loss: 0.8979 - val_accuracy: 0.8889\n",
    "Epoch 249/500\n",
    "3/3 [==============================] - 2s 516ms/step - loss: 0.1582 - accuracy: 0.9405 - val_loss: 0.9046 - val_accuracy: 0.8889\n",
    "Epoch 250/500\n",
    "3/3 [==============================] - 2s 646ms/step - loss: 0.1486 - accuracy: 0.9643 - val_loss: 0.8917 - val_accuracy: 0.8889\n",
    "Epoch 251/500\n",
    "3/3 [==============================] - 2s 522ms/step - loss: 0.0639 - accuracy: 0.9762 - val_loss: 0.8957 - val_accuracy: 0.8889\n",
    "Epoch 252/500\n",
    "3/3 [==============================] - 2s 507ms/step - loss: 0.0807 - accuracy: 0.9643 - val_loss: 0.9358 - val_accuracy: 0.8889\n",
    "Epoch 253/500\n",
    "3/3 [==============================] - 2s 511ms/step - loss: 0.1042 - accuracy: 0.9702 - val_loss: 0.9752 - val_accuracy: 0.8889\n",
    "Epoch 254/500\n",
    "3/3 [==============================] - 2s 508ms/step - loss: 0.0860 - accuracy: 0.9643 - val_loss: 0.9915 - val_accuracy: 0.8889\n",
    "Epoch 255/500\n",
    "3/3 [==============================] - 2s 505ms/step - loss: 0.0850 - accuracy: 0.9702 - val_loss: 1.0862 - val_accuracy: 0.8667\n",
    "Epoch 256/500\n",
    "3/3 [==============================] - 2s 510ms/step - loss: 0.1091 - accuracy: 0.9643 - val_loss: 1.3125 - val_accuracy: 0.8222\n",
    "Epoch 257/500\n",
    "3/3 [==============================] - 2s 499ms/step - loss: 0.1053 - accuracy: 0.9643 - val_loss: 1.4116 - val_accuracy: 0.8222\n",
    "Epoch 258/500\n",
    "3/3 [==============================] - 2s 507ms/step - loss: 0.0563 - accuracy: 0.9881 - val_loss: 1.3190 - val_accuracy: 0.8444\n",
    "Epoch 259/500\n",
    "3/3 [==============================] - 2s 507ms/step - loss: 0.1061 - accuracy: 0.9643 - val_loss: 1.0725 - val_accuracy: 0.8889\n",
    "Epoch 260/500\n",
    "3/3 [==============================] - 2s 524ms/step - loss: 0.0772 - accuracy: 0.9643 - val_loss: 0.9123 - val_accuracy: 0.8889\n",
    "Epoch 261/500\n",
    "3/3 [==============================] - 2s 528ms/step - loss: 0.1139 - accuracy: 0.9583 - val_loss: 0.7317 - val_accuracy: 0.8889\n",
    "Epoch 262/500\n",
    "3/3 [==============================] - 2s 517ms/step - loss: 0.0739 - accuracy: 0.9762 - val_loss: 0.6588 - val_accuracy: 0.8889\n",
    "Epoch 263/500\n",
    "\n",
    "3/3 [==============================] - 2s 499ms/step - loss: 0.0572 - accuracy: 0.9821 - val_loss: 0.6311 - val_accuracy: 0.8889\n",
    "Epoch 264/500\n",
    "3/3 [==============================] - 2s 651ms/step - loss: 0.1105 - accuracy: 0.9524 - val_loss: 0.6060 - val_accuracy: 0.8889\n",
    "Epoch 265/500\n",
    "3/3 [==============================] - 2s 512ms/step - loss: 0.1123 - accuracy: 0.9583 - val_loss: 0.6205 - val_accuracy: 0.8889\n",
    "Epoch 266/500\n",
    "3/3 [==============================] - 2s 517ms/step - loss: 0.1039 - accuracy: 0.9583 - val_loss: 0.6310 - val_accuracy: 0.9111\n",
    "Epoch 267/500\n",
    "3/3 [==============================] - 2s 659ms/step - loss: 0.0768 - accuracy: 0.9762 - val_loss: 0.6625 - val_accuracy: 0.9111\n",
    "Epoch 268/500\n",
    "3/3 [==============================] - 2s 506ms/step - loss: 0.1142 - accuracy: 0.9583 - val_loss: 0.7343 - val_accuracy: 0.9111\n",
    "Epoch 269/500\n",
    "3/3 [==============================] - 2s 519ms/step - loss: 0.0678 - accuracy: 0.9702 - val_loss: 0.7948 - val_accuracy: 0.8889\n",
    "Epoch 270/500\n",
    "3/3 [==============================] - 2s 646ms/step - loss: 0.1132 - accuracy: 0.9643 - val_loss: 0.7712 - val_accuracy: 0.9111\n",
    "Epoch 271/500\n",
    "3/3 [==============================] - 2s 525ms/step - loss: 0.0986 - accuracy: 0.9702 - val_loss: 0.7326 - val_accuracy: 0.9111\n",
    "Epoch 272/500\n",
    "3/3 [==============================] - 2s 645ms/step - loss: 0.0930 - accuracy: 0.9762 - val_loss: 0.7291 - val_accuracy: 0.8889\n",
    "Epoch 273/500\n",
    "3/3 [==============================] - 2s 506ms/step - loss: 0.1126 - accuracy: 0.9583 - val_loss: 0.8718 - val_accuracy: 0.8889\n",
    "Epoch 274/500\n",
    "3/3 [==============================] - 2s 516ms/step - loss: 0.1030 - accuracy: 0.9524 - val_loss: 1.0593 - val_accuracy: 0.8444\n",
    "Epoch 275/500\n",
    "3/3 [==============================] - 2s 645ms/step - loss: 0.1027 - accuracy: 0.9643 - val_loss: 1.2163 - val_accuracy: 0.8222\n",
    "Epoch 276/500\n",
    "3/3 [==============================] - 2s 648ms/step - loss: 0.1103 - accuracy: 0.9702 - val_loss: 1.2477 - val_accuracy: 0.8444\n",
    "Epoch 277/500\n",
    "3/3 [==============================] - 2s 502ms/step - loss: 0.0746 - accuracy: 0.9881 - val_loss: 1.2171 - val_accuracy: 0.8667\n",
    "Epoch 278/500\n",
    "3/3 [==============================] - 2s 506ms/step - loss: 0.0792 - accuracy: 0.9762 - val_loss: 1.2029 - val_accuracy: 0.8667\n",
    "Epoch 279/500\n",
    "3/3 [==============================] - 2s 654ms/step - loss: 0.0609 - accuracy: 0.9702 - val_loss: 1.2006 - val_accuracy: 0.8667\n",
    "Epoch 280/500\n",
    "3/3 [==============================] - 2s 499ms/step - loss: 0.0766 - accuracy: 0.9762 - val_loss: 1.2481 - val_accuracy: 0.8444\n",
    "Epoch 281/500\n",
    "3/3 [==============================] - 2s 513ms/step - loss: 0.0654 - accuracy: 0.9881 - val_loss: 1.2406 - val_accuracy: 0.8444\n",
    "Epoch 282/500\n",
    "3/3 [==============================] - 2s 654ms/step - loss: 0.0959 - accuracy: 0.9702 - val_loss: 1.3009 - val_accuracy: 0.8222\n",
    "Epoch 283/500\n",
    "3/3 [==============================] - 2s 645ms/step - loss: 0.1016 - accuracy: 0.9702 - val_loss: 1.3043 - val_accuracy: 0.8444\n",
    "Epoch 284/500\n",
    "3/3 [==============================] - 2s 502ms/step - loss: 0.1371 - accuracy: 0.9702 - val_loss: 1.2376 - val_accuracy: 0.8667\n",
    "Epoch 285/500\n",
    "3/3 [==============================] - 2s 655ms/step - loss: 0.1202 - accuracy: 0.9762 - val_loss: 1.2422 - val_accuracy: 0.8667\n",
    "Epoch 286/500\n",
    "3/3 [==============================] - 2s 504ms/step - loss: 0.1161 - accuracy: 0.9702 - val_loss: 1.2448 - val_accuracy: 0.8667\n",
    "Epoch 287/500\n",
    "3/3 [==============================] - 2s 521ms/step - loss: 0.0699 - accuracy: 0.9583 - val_loss: 1.2793 - val_accuracy: 0.8667\n",
    "Epoch 288/500\n",
    "3/3 [==============================] - 2s 507ms/step - loss: 0.0394 - accuracy: 0.9881 - val_loss: 1.2823 - val_accuracy: 0.8667\n",
    "Epoch 289/500\n",
    "3/3 [==============================] - 2s 505ms/step - loss: 0.1777 - accuracy: 0.9464 - val_loss: 1.2577 - val_accuracy: 0.8667\n",
    "Epoch 290/500\n",
    "3/3 [==============================] - 2s 497ms/step - loss: 0.1081 - accuracy: 0.9702 - val_loss: 1.1854 - val_accuracy: 0.8889\n",
    "Epoch 291/500\n",
    "3/3 [==============================] - 2s 657ms/step - loss: 0.1301 - accuracy: 0.9405 - val_loss: 1.1308 - val_accuracy: 0.8444\n",
    "Epoch 292/500\n",
    "3/3 [==============================] - 2s 647ms/step - loss: 0.0840 - accuracy: 0.9702 - val_loss: 1.1706 - val_accuracy: 0.8667\n",
    "Epoch 293/500\n",
    "3/3 [==============================] - 2s 654ms/step - loss: 0.1408 - accuracy: 0.9524 - val_loss: 1.2203 - val_accuracy: 0.8667\n",
    "Epoch 294/500\n",
    "3/3 [==============================] - 2s 492ms/step - loss: 0.0673 - accuracy: 0.9762 - val_loss: 1.2524 - val_accuracy: 0.8444\n",
    "Epoch 295/500\n",
    "3/3 [==============================] - 2s 507ms/step - loss: 0.1645 - accuracy: 0.9286 - val_loss: 1.1636 - val_accuracy: 0.8667\n",
    "Epoch 296/500\n",
    "3/3 [==============================] - 2s 511ms/step - loss: 0.0637 - accuracy: 0.9821 - val_loss: 1.0881 - val_accuracy: 0.8444\n",
    "Epoch 297/500\n",
    "3/3 [==============================] - 2s 660ms/step - loss: 0.0627 - accuracy: 0.9881 - val_loss: 1.0771 - val_accuracy: 0.8667\n",
    "Epoch 298/500\n",
    "3/3 [==============================] - 2s 656ms/step - loss: 0.1488 - accuracy: 0.9464 - val_loss: 1.1195 - val_accuracy: 0.8667\n",
    "Epoch 299/500\n",
    "3/3 [==============================] - 2s 498ms/step - loss: 0.2378 - accuracy: 0.9107 - val_loss: 1.0992 - val_accuracy: 0.8444\n",
    "Epoch 300/500\n",
    "3/3 [==============================] - 2s 649ms/step - loss: 0.1538 - accuracy: 0.9583 - val_loss: 1.0329 - val_accuracy: 0.8444\n",
    "Epoch 301/500\n",
    "3/3 [==============================] - 2s 651ms/step - loss: 0.0909 - accuracy: 0.9702 - val_loss: 0.9863 - val_accuracy: 0.8667\n",
    "Epoch 302/500\n",
    "3/3 [==============================] - 2s 652ms/step - loss: 0.1218 - accuracy: 0.9583 - val_loss: 1.0695 - val_accuracy: 0.8667\n",
    "Epoch 303/500\n",
    "3/3 [==============================] - 2s 521ms/step - loss: 0.1409 - accuracy: 0.9464 - val_loss: 1.1087 - val_accuracy: 0.8444\n",
    "Epoch 304/500\n",
    "3/3 [==============================] - 2s 658ms/step - loss: 0.1016 - accuracy: 0.9702 - val_loss: 1.1394 - val_accuracy: 0.8667\n",
    "Epoch 305/500\n",
    "3/3 [==============================] - 2s 499ms/step - loss: 0.1636 - accuracy: 0.9405 - val_loss: 1.1797 - val_accuracy: 0.8222\n",
    "Epoch 306/500\n",
    "3/3 [==============================] - 2s 646ms/step - loss: 0.0530 - accuracy: 0.9702 - val_loss: 1.2629 - val_accuracy: 0.8000\n",
    "Epoch 307/500\n",
    "3/3 [==============================] - 2s 502ms/step - loss: 0.1208 - accuracy: 0.9405 - val_loss: 1.3380 - val_accuracy: 0.7778\n",
    "Epoch 308/500\n",
    "3/3 [==============================] - 2s 654ms/step - loss: 0.2058 - accuracy: 0.9464 - val_loss: 1.3583 - val_accuracy: 0.7778\n",
    "Epoch 309/500\n",
    "3/3 [==============================] - 2s 504ms/step - loss: 0.0690 - accuracy: 0.9702 - val_loss: 1.2619 - val_accuracy: 0.8222\n",
    "Epoch 310/500\n",
    "3/3 [==============================] - 2s 643ms/step - loss: 0.0660 - accuracy: 0.9702 - val_loss: 1.1346 - val_accuracy: 0.8222\n",
    "Epoch 311/500\n",
    "3/3 [==============================] - 2s 508ms/step - loss: 0.1365 - accuracy: 0.9524 - val_loss: 1.1260 - val_accuracy: 0.8444\n",
    "Epoch 312/500\n",
    "3/3 [==============================] - 2s 500ms/step - loss: 0.1418 - accuracy: 0.9524 - val_loss: 1.1509 - val_accuracy: 0.8000\n",
    "Epoch 313/500\n",
    "3/3 [==============================] - 2s 520ms/step - loss: 0.1215 - accuracy: 0.9524 - val_loss: 1.0724 - val_accuracy: 0.8444\n",
    "Epoch 314/500\n",
    "3/3 [==============================] - 2s 519ms/step - loss: 0.1361 - accuracy: 0.9524 - val_loss: 1.0678 - val_accuracy: 0.8222\n",
    "Epoch 315/500\n",
    "3/3 [==============================] - 2s 659ms/step - loss: 0.2214 - accuracy: 0.8929 - val_loss: 1.0403 - val_accuracy: 0.8444\n",
    "Epoch 316/500\n",
    "3/3 [==============================] - 2s 654ms/step - loss: 0.1690 - accuracy: 0.9524 - val_loss: 1.0461 - val_accuracy: 0.8444\n",
    "Epoch 317/500\n",
    "3/3 [==============================] - 2s 648ms/step - loss: 0.1199 - accuracy: 0.9464 - val_loss: 1.0002 - val_accuracy: 0.8444\n",
    "Epoch 318/500\n",
    "3/3 [==============================] - 2s 654ms/step - loss: 0.0664 - accuracy: 0.9762 - val_loss: 0.9346 - val_accuracy: 0.8444\n",
    "Epoch 319/500\n",
    "3/3 [==============================] - 2s 653ms/step - loss: 0.0520 - accuracy: 0.9940 - val_loss: 0.8992 - val_accuracy: 0.8222\n",
    "Epoch 320/500\n",
    "\n",
    "3/3 [==============================] - 2s 650ms/step - loss: 0.2154 - accuracy: 0.9464 - val_loss: 0.9495 - val_accuracy: 0.8667\n",
    "Epoch 321/500\n",
    "3/3 [==============================] - 2s 644ms/step - loss: 0.1330 - accuracy: 0.9464 - val_loss: 0.9961 - val_accuracy: 0.8667\n",
    "Epoch 322/500\n",
    "3/3 [==============================] - 2s 509ms/step - loss: 0.0864 - accuracy: 0.9702 - val_loss: 1.1242 - val_accuracy: 0.8000\n",
    "Epoch 323/500\n",
    "3/3 [==============================] - 2s 654ms/step - loss: 0.0859 - accuracy: 0.9643 - val_loss: 1.2703 - val_accuracy: 0.7778\n",
    "Epoch 324/500\n",
    "3/3 [==============================] - 2s 509ms/step - loss: 0.1066 - accuracy: 0.9762 - val_loss: 1.3793 - val_accuracy: 0.7556\n",
    "Epoch 325/500\n",
    "3/3 [==============================] - 2s 656ms/step - loss: 0.0726 - accuracy: 0.9821 - val_loss: 1.3428 - val_accuracy: 0.7556\n",
    "Epoch 326/500\n",
    "3/3 [==============================] - 2s 528ms/step - loss: 0.0778 - accuracy: 0.9821 - val_loss: 1.1844 - val_accuracy: 0.8222\n",
    "Epoch 327/500\n",
    "3/3 [==============================] - 2s 512ms/step - loss: 0.0406 - accuracy: 0.9821 - val_loss: 1.0646 - val_accuracy: 0.8444\n",
    "Epoch 328/500\n",
    "3/3 [==============================] - 2s 510ms/step - loss: 0.1109 - accuracy: 0.9643 - val_loss: 0.9933 - val_accuracy: 0.8444\n",
    "Epoch 329/500\n",
    "3/3 [==============================] - 2s 527ms/step - loss: 0.1002 - accuracy: 0.9702 - val_loss: 0.9795 - val_accuracy: 0.8889\n",
    "Epoch 330/500\n",
    "3/3 [==============================] - 2s 654ms/step - loss: 0.0845 - accuracy: 0.9821 - val_loss: 0.9963 - val_accuracy: 0.8889\n",
    "Epoch 331/500\n",
    "3/3 [==============================] - 2s 512ms/step - loss: 0.1305 - accuracy: 0.9524 - val_loss: 1.0051 - val_accuracy: 0.9111\n",
    "Epoch 332/500\n",
    "3/3 [==============================] - 2s 504ms/step - loss: 0.0812 - accuracy: 0.9762 - val_loss: 1.0195 - val_accuracy: 0.8889\n",
    "Epoch 333/500\n",
    "3/3 [==============================] - 2s 514ms/step - loss: 0.0954 - accuracy: 0.9643 - val_loss: 1.0230 - val_accuracy: 0.8889\n",
    "Epoch 334/500\n",
    "3/3 [==============================] - 2s 644ms/step - loss: 0.1156 - accuracy: 0.9643 - val_loss: 1.0170 - val_accuracy: 0.8667\n",
    "Epoch 335/500\n",
    "3/3 [==============================] - 2s 643ms/step - loss: 0.1151 - accuracy: 0.9524 - val_loss: 1.0134 - val_accuracy: 0.8667\n",
    "Epoch 336/500\n",
    "3/3 [==============================] - 2s 649ms/step - loss: 0.0344 - accuracy: 0.9881 - val_loss: 1.0243 - val_accuracy: 0.8444\n",
    "Epoch 337/500\n",
    "3/3 [==============================] - 2s 655ms/step - loss: 0.0431 - accuracy: 0.9881 - val_loss: 1.0346 - val_accuracy: 0.8444\n",
    "Epoch 338/500\n",
    "3/3 [==============================] - 2s 514ms/step - loss: 0.1606 - accuracy: 0.9226 - val_loss: 1.0613 - val_accuracy: 0.8444\n",
    "Epoch 339/500\n",
    "3/3 [==============================] - 2s 504ms/step - loss: 0.0406 - accuracy: 0.9821 - val_loss: 1.1202 - val_accuracy: 0.8444\n",
    "Epoch 340/500\n",
    "3/3 [==============================] - 2s 653ms/step - loss: 0.0420 - accuracy: 0.9821 - val_loss: 1.1694 - val_accuracy: 0.8444\n",
    "Epoch 341/500\n",
    "3/3 [==============================] - 2s 659ms/step - loss: 0.0939 - accuracy: 0.9643 - val_loss: 1.1929 - val_accuracy: 0.8444\n",
    "Epoch 342/500\n",
    "3/3 [==============================] - 2s 506ms/step - loss: 0.0117 - accuracy: 1.0000 - val_loss: 1.1994 - val_accuracy: 0.8444\n",
    "Epoch 343/500\n",
    "3/3 [==============================] - 2s 510ms/step - loss: 0.1112 - accuracy: 0.9702 - val_loss: 1.1487 - val_accuracy: 0.8444\n",
    "Epoch 344/500\n",
    "3/3 [==============================] - 2s 648ms/step - loss: 0.0510 - accuracy: 0.9762 - val_loss: 1.1155 - val_accuracy: 0.8222\n",
    "Epoch 345/500\n",
    "3/3 [==============================] - 2s 500ms/step - loss: 0.0765 - accuracy: 0.9821 - val_loss: 1.0895 - val_accuracy: 0.8444\n",
    "Epoch 346/500\n",
    "3/3 [==============================] - 2s 502ms/step - loss: 0.0981 - accuracy: 0.9643 - val_loss: 1.0453 - val_accuracy: 0.8444\n",
    "Epoch 347/500\n",
    "3/3 [==============================] - 2s 654ms/step - loss: 0.0598 - accuracy: 0.9762 - val_loss: 1.0174 - val_accuracy: 0.8444\n",
    "Epoch 348/500\n",
    "3/3 [==============================] - 2s 503ms/step - loss: 0.2089 - accuracy: 0.9464 - val_loss: 0.9709 - val_accuracy: 0.8444\n",
    "Epoch 349/500\n",
    "3/3 [==============================] - 2s 502ms/step - loss: 0.0580 - accuracy: 0.9762 - val_loss: 0.9521 - val_accuracy: 0.8889\n",
    "Epoch 350/500\n",
    "3/3 [==============================] - 2s 500ms/step - loss: 0.0797 - accuracy: 0.9702 - val_loss: 0.9664 - val_accuracy: 0.8889\n",
    "Epoch 351/500\n",
    "3/3 [==============================] - 2s 512ms/step - loss: 0.0547 - accuracy: 0.9881 - val_loss: 0.9854 - val_accuracy: 0.8889\n",
    "Epoch 352/500\n",
    "3/3 [==============================] - 2s 514ms/step - loss: 0.1093 - accuracy: 0.9643 - val_loss: 0.9868 - val_accuracy: 0.8889\n",
    "Epoch 353/500\n",
    "3/3 [==============================] - 2s 650ms/step - loss: 0.0861 - accuracy: 0.9702 - val_loss: 1.0060 - val_accuracy: 0.8889\n",
    "Epoch 354/500\n",
    "3/3 [==============================] - 2s 506ms/step - loss: 0.1244 - accuracy: 0.9583 - val_loss: 1.0222 - val_accuracy: 0.8889\n",
    "Epoch 355/500\n",
    "3/3 [==============================] - 2s 501ms/step - loss: 0.0734 - accuracy: 0.9643 - val_loss: 0.9838 - val_accuracy: 0.8667\n",
    "Epoch 356/500\n",
    "3/3 [==============================] - 2s 526ms/step - loss: 0.0738 - accuracy: 0.9762 - val_loss: 0.9470 - val_accuracy: 0.8444\n",
    "Epoch 357/500\n",
    "3/3 [==============================] - 2s 507ms/step - loss: 0.0325 - accuracy: 0.9940 - val_loss: 0.9390 - val_accuracy: 0.8444\n",
    "Epoch 358/500\n",
    "3/3 [==============================] - 2s 646ms/step - loss: 0.1691 - accuracy: 0.9345 - val_loss: 0.9271 - val_accuracy: 0.8667\n",
    "Epoch 359/500\n",
    "3/3 [==============================] - 2s 500ms/step - loss: 0.1240 - accuracy: 0.9464 - val_loss: 0.8918 - val_accuracy: 0.8667\n",
    "Epoch 360/500\n",
    "3/3 [==============================] - 2s 515ms/step - loss: 0.1250 - accuracy: 0.9643 - val_loss: 0.9001 - val_accuracy: 0.8889\n",
    "Epoch 361/500\n",
    "3/3 [==============================] - 2s 519ms/step - loss: 0.1225 - accuracy: 0.9583 - val_loss: 0.9002 - val_accuracy: 0.8667\n",
    "Epoch 362/500\n",
    "3/3 [==============================] - 2s 658ms/step - loss: 0.0730 - accuracy: 0.9881 - val_loss: 0.8667 - val_accuracy: 0.8667\n",
    "Epoch 363/500\n",
    "3/3 [==============================] - 2s 650ms/step - loss: 0.0560 - accuracy: 0.9762 - val_loss: 0.8177 - val_accuracy: 0.8889\n",
    "Epoch 364/500\n",
    "3/3 [==============================] - 2s 509ms/step - loss: 0.0736 - accuracy: 0.9643 - val_loss: 0.7916 - val_accuracy: 0.8667\n",
    "Epoch 365/500\n",
    "3/3 [==============================] - 2s 523ms/step - loss: 0.0919 - accuracy: 0.9702 - val_loss: 0.8148 - val_accuracy: 0.8667\n",
    "Epoch 366/500\n",
    "3/3 [==============================] - 2s 505ms/step - loss: 0.1026 - accuracy: 0.9583 - val_loss: 0.9244 - val_accuracy: 0.8889\n",
    "Epoch 367/500\n",
    "3/3 [==============================] - 2s 500ms/step - loss: 0.0378 - accuracy: 0.9881 - val_loss: 1.0417 - val_accuracy: 0.8444\n",
    "Epoch 368/500\n",
    "3/3 [==============================] - 2s 517ms/step - loss: 0.0847 - accuracy: 0.9583 - val_loss: 1.1407 - val_accuracy: 0.8000\n",
    "Epoch 369/500\n",
    "3/3 [==============================] - 2s 496ms/step - loss: 0.1289 - accuracy: 0.9464 - val_loss: 1.1780 - val_accuracy: 0.8222\n",
    "Epoch 370/500\n",
    "3/3 [==============================] - 2s 657ms/step - loss: 0.1280 - accuracy: 0.9583 - val_loss: 1.1456 - val_accuracy: 0.8222\n",
    "Epoch 371/500\n",
    "3/3 [==============================] - 2s 502ms/step - loss: 0.1028 - accuracy: 0.9583 - val_loss: 1.1091 - val_accuracy: 0.8444\n",
    "Epoch 372/500\n",
    "3/3 [==============================] - 2s 646ms/step - loss: 0.1399 - accuracy: 0.9464 - val_loss: 1.0851 - val_accuracy: 0.8889\n",
    "Epoch 373/500\n",
    "3/3 [==============================] - 2s 652ms/step - loss: 0.0821 - accuracy: 0.9821 - val_loss: 1.0369 - val_accuracy: 0.8889\n",
    "Epoch 374/500\n",
    "3/3 [==============================] - 2s 508ms/step - loss: 0.1053 - accuracy: 0.9643 - val_loss: 0.9703 - val_accuracy: 0.8889\n",
    "Epoch 375/500\n",
    "3/3 [==============================] - 2s 532ms/step - loss: 0.1078 - accuracy: 0.9583 - val_loss: 0.9337 - val_accuracy: 0.8889\n",
    "Epoch 376/500\n",
    "3/3 [==============================] - 2s 644ms/step - loss: 0.1309 - accuracy: 0.9524 - val_loss: 0.8961 - val_accuracy: 0.8667\n",
    "Epoch 377/500\n",
    "\n",
    "3/3 [==============================] - 2s 517ms/step - loss: 0.0911 - accuracy: 0.9702 - val_loss: 0.9010 - val_accuracy: 0.8667\n",
    "Epoch 378/500\n",
    "3/3 [==============================] - 2s 511ms/step - loss: 0.1481 - accuracy: 0.9345 - val_loss: 0.9591 - val_accuracy: 0.8667\n",
    "Epoch 379/500\n",
    "3/3 [==============================] - 2s 516ms/step - loss: 0.0853 - accuracy: 0.9643 - val_loss: 1.0450 - val_accuracy: 0.8667\n",
    "Epoch 380/500\n",
    "3/3 [==============================] - 2s 511ms/step - loss: 0.1755 - accuracy: 0.9464 - val_loss: 1.1141 - val_accuracy: 0.8667\n",
    "Epoch 381/500\n",
    "3/3 [==============================] - 2s 649ms/step - loss: 0.1182 - accuracy: 0.9583 - val_loss: 1.1269 - val_accuracy: 0.8444\n",
    "Epoch 382/500\n",
    "3/3 [==============================] - 2s 663ms/step - loss: 0.0784 - accuracy: 0.9762 - val_loss: 1.0106 - val_accuracy: 0.8667\n",
    "Epoch 383/500\n",
    "3/3 [==============================] - 2s 522ms/step - loss: 0.0920 - accuracy: 0.9643 - val_loss: 0.9740 - val_accuracy: 0.8889\n",
    "Epoch 384/500\n",
    "3/3 [==============================] - 2s 510ms/step - loss: 0.0874 - accuracy: 0.9702 - val_loss: 0.9645 - val_accuracy: 0.8889\n",
    "Epoch 385/500\n",
    "3/3 [==============================] - 2s 662ms/step - loss: 0.1422 - accuracy: 0.9583 - val_loss: 0.9219 - val_accuracy: 0.8889\n",
    "Epoch 386/500\n",
    "3/3 [==============================] - 2s 655ms/step - loss: 0.1174 - accuracy: 0.9702 - val_loss: 0.8990 - val_accuracy: 0.8889\n",
    "Epoch 387/500\n",
    "3/3 [==============================] - 2s 521ms/step - loss: 0.0914 - accuracy: 0.9762 - val_loss: 0.9045 - val_accuracy: 0.8889\n",
    "Epoch 388/500\n",
    "3/3 [==============================] - 2s 644ms/step - loss: 0.0256 - accuracy: 0.9940 - val_loss: 0.9293 - val_accuracy: 0.9111\n",
    "Epoch 389/500\n",
    "3/3 [==============================] - 2s 504ms/step - loss: 0.1319 - accuracy: 0.9702 - val_loss: 0.9128 - val_accuracy: 0.8889\n",
    "Epoch 390/500\n",
    "3/3 [==============================] - 2s 647ms/step - loss: 0.0979 - accuracy: 0.9762 - val_loss: 0.9375 - val_accuracy: 0.8444\n",
    "Epoch 391/500\n",
    "3/3 [==============================] - 2s 653ms/step - loss: 0.0774 - accuracy: 0.9524 - val_loss: 1.0250 - val_accuracy: 0.8444\n",
    "Epoch 392/500\n",
    "3/3 [==============================] - 2s 646ms/step - loss: 0.0541 - accuracy: 0.9702 - val_loss: 1.0688 - val_accuracy: 0.8667\n",
    "Epoch 393/500\n",
    "3/3 [==============================] - 2s 641ms/step - loss: 0.0363 - accuracy: 0.9881 - val_loss: 1.0460 - val_accuracy: 0.8667\n",
    "Epoch 394/500\n",
    "3/3 [==============================] - 2s 498ms/step - loss: 0.0608 - accuracy: 0.9702 - val_loss: 1.0165 - val_accuracy: 0.8667\n",
    "Epoch 395/500\n",
    "3/3 [==============================] - 2s 654ms/step - loss: 0.0801 - accuracy: 0.9762 - val_loss: 0.9743 - val_accuracy: 0.8667\n",
    "Epoch 396/500\n",
    "3/3 [==============================] - 2s 514ms/step - loss: 0.1331 - accuracy: 0.9464 - val_loss: 0.9169 - val_accuracy: 0.8667\n",
    "Epoch 397/500\n",
    "3/3 [==============================] - 2s 657ms/step - loss: 0.0756 - accuracy: 0.9643 - val_loss: 0.8522 - val_accuracy: 0.8667\n",
    "Epoch 398/500\n",
    "3/3 [==============================] - 2s 514ms/step - loss: 0.0452 - accuracy: 0.9821 - val_loss: 0.8029 - val_accuracy: 0.8667\n",
    "Epoch 399/500\n",
    "3/3 [==============================] - 2s 520ms/step - loss: 0.0482 - accuracy: 0.9702 - val_loss: 0.7825 - val_accuracy: 0.8667\n",
    "Epoch 400/500\n",
    "3/3 [==============================] - 2s 512ms/step - loss: 0.1233 - accuracy: 0.9524 - val_loss: 0.7322 - val_accuracy: 0.8889\n",
    "Epoch 401/500\n",
    "3/3 [==============================] - 2s 516ms/step - loss: 0.0671 - accuracy: 0.9762 - val_loss: 0.7092 - val_accuracy: 0.9111\n",
    "Epoch 402/500\n",
    "3/3 [==============================] - 2s 508ms/step - loss: 0.0771 - accuracy: 0.9643 - val_loss: 0.7189 - val_accuracy: 0.9111\n",
    "Epoch 403/500\n",
    "3/3 [==============================] - 2s 645ms/step - loss: 0.0635 - accuracy: 0.9762 - val_loss: 0.7306 - val_accuracy: 0.9111\n",
    "Epoch 404/500\n",
    "3/3 [==============================] - 2s 507ms/step - loss: 0.0978 - accuracy: 0.9643 - val_loss: 0.7414 - val_accuracy: 0.9111\n",
    "Epoch 405/500\n",
    "3/3 [==============================] - 2s 493ms/step - loss: 0.0440 - accuracy: 0.9940 - val_loss: 0.7718 - val_accuracy: 0.9111\n",
    "Epoch 406/500\n",
    "3/3 [==============================] - 2s 653ms/step - loss: 0.1335 - accuracy: 0.9524 - val_loss: 0.8405 - val_accuracy: 0.9111\n",
    "Epoch 407/500\n",
    "3/3 [==============================] - 2s 517ms/step - loss: 0.1049 - accuracy: 0.9583 - val_loss: 0.9298 - val_accuracy: 0.9111\n",
    "Epoch 408/500\n",
    "3/3 [==============================] - 2s 500ms/step - loss: 0.0292 - accuracy: 0.9940 - val_loss: 1.0491 - val_accuracy: 0.9111\n",
    "Epoch 409/500\n",
    "3/3 [==============================] - 2s 496ms/step - loss: 0.0544 - accuracy: 0.9762 - val_loss: 1.1526 - val_accuracy: 0.9111\n",
    "Epoch 410/500\n",
    "3/3 [==============================] - 2s 649ms/step - loss: 0.0622 - accuracy: 0.9821 - val_loss: 1.2438 - val_accuracy: 0.8444\n",
    "Epoch 411/500\n",
    "3/3 [==============================] - 2s 502ms/step - loss: 0.1041 - accuracy: 0.9643 - val_loss: 1.2871 - val_accuracy: 0.8444\n",
    "Epoch 412/500\n",
    "3/3 [==============================] - 2s 508ms/step - loss: 0.0752 - accuracy: 0.9643 - val_loss: 1.2815 - val_accuracy: 0.8667\n",
    "Epoch 413/500\n",
    "3/3 [==============================] - 2s 498ms/step - loss: 0.0323 - accuracy: 0.9881 - val_loss: 1.2225 - val_accuracy: 0.8889\n",
    "Epoch 414/500\n",
    "3/3 [==============================] - 2s 510ms/step - loss: 0.1092 - accuracy: 0.9643 - val_loss: 1.1585 - val_accuracy: 0.8889\n",
    "Epoch 415/500\n",
    "3/3 [==============================] - 2s 645ms/step - loss: 0.0602 - accuracy: 0.9821 - val_loss: 1.0721 - val_accuracy: 0.8889\n",
    "Epoch 416/500\n",
    "3/3 [==============================] - 2s 648ms/step - loss: 0.0561 - accuracy: 0.9821 - val_loss: 0.9713 - val_accuracy: 0.8889\n",
    "Epoch 417/500\n",
    "3/3 [==============================] - 2s 642ms/step - loss: 0.0687 - accuracy: 0.9643 - val_loss: 0.8886 - val_accuracy: 0.9111\n",
    "Epoch 418/500\n",
    "3/3 [==============================] - 2s 514ms/step - loss: 0.0465 - accuracy: 0.9762 - val_loss: 0.8334 - val_accuracy: 0.9111\n",
    "Epoch 419/500\n",
    "3/3 [==============================] - 2s 664ms/step - loss: 0.0402 - accuracy: 0.9881 - val_loss: 0.7791 - val_accuracy: 0.9111\n",
    "Epoch 420/500\n",
    "3/3 [==============================] - 2s 497ms/step - loss: 0.1008 - accuracy: 0.9583 - val_loss: 0.7633 - val_accuracy: 0.9111\n",
    "Epoch 421/500\n",
    "3/3 [==============================] - 2s 508ms/step - loss: 0.0629 - accuracy: 0.9762 - val_loss: 0.7918 - val_accuracy: 0.9111\n",
    "Epoch 422/500\n",
    "3/3 [==============================] - 2s 506ms/step - loss: 0.0620 - accuracy: 0.9643 - val_loss: 0.8271 - val_accuracy: 0.9111\n",
    "Epoch 423/500\n",
    "3/3 [==============================] - 2s 519ms/step - loss: 0.0730 - accuracy: 0.9702 - val_loss: 0.8322 - val_accuracy: 0.9111\n",
    "Epoch 424/500\n",
    "3/3 [==============================] - 2s 649ms/step - loss: 0.0695 - accuracy: 0.9702 - val_loss: 0.8605 - val_accuracy: 0.9111\n",
    "Epoch 425/500\n",
    "3/3 [==============================] - 2s 518ms/step - loss: 0.1118 - accuracy: 0.9524 - val_loss: 0.9284 - val_accuracy: 0.8667\n",
    "Epoch 426/500\n",
    "3/3 [==============================] - 2s 512ms/step - loss: 0.0589 - accuracy: 0.9881 - val_loss: 1.0092 - val_accuracy: 0.8667\n",
    "Epoch 427/500\n",
    "3/3 [==============================] - 2s 645ms/step - loss: 0.0640 - accuracy: 0.9762 - val_loss: 1.0680 - val_accuracy: 0.8667\n",
    "Epoch 428/500\n",
    "3/3 [==============================] - 2s 512ms/step - loss: 0.1670 - accuracy: 0.9524 - val_loss: 1.0828 - val_accuracy: 0.8667\n",
    "Epoch 429/500\n",
    "3/3 [==============================] - 2s 510ms/step - loss: 0.0551 - accuracy: 0.9821 - val_loss: 1.0643 - val_accuracy: 0.8667\n",
    "Epoch 430/500\n",
    "3/3 [==============================] - 2s 514ms/step - loss: 0.0747 - accuracy: 0.9643 - val_loss: 1.0342 - val_accuracy: 0.8667\n",
    "Epoch 431/500\n",
    "3/3 [==============================] - 2s 650ms/step - loss: 0.0329 - accuracy: 0.9940 - val_loss: 1.0048 - val_accuracy: 0.8889\n",
    "Epoch 432/500\n",
    "3/3 [==============================] - 2s 515ms/step - loss: 0.0758 - accuracy: 0.9702 - val_loss: 0.9508 - val_accuracy: 0.8889\n",
    "Epoch 433/500\n",
    "3/3 [==============================] - 2s 516ms/step - loss: 0.0945 - accuracy: 0.9583 - val_loss: 0.9148 - val_accuracy: 0.8889\n",
    "Epoch 434/500\n",
    "\n",
    "3/3 [==============================] - 2s 524ms/step - loss: 0.0955 - accuracy: 0.9643 - val_loss: 0.8795 - val_accuracy: 0.8889\n",
    "Epoch 435/500\n",
    "3/3 [==============================] - 2s 496ms/step - loss: 0.0232 - accuracy: 0.9881 - val_loss: 0.8608 - val_accuracy: 0.8889\n",
    "Epoch 436/500\n",
    "3/3 [==============================] - 2s 510ms/step - loss: 0.0982 - accuracy: 0.9702 - val_loss: 0.8457 - val_accuracy: 0.8667\n",
    "Epoch 437/500\n",
    "3/3 [==============================] - 2s 643ms/step - loss: 0.0505 - accuracy: 0.9821 - val_loss: 0.8691 - val_accuracy: 0.8444\n",
    "Epoch 438/500\n",
    "3/3 [==============================] - 2s 516ms/step - loss: 0.0826 - accuracy: 0.9762 - val_loss: 0.8700 - val_accuracy: 0.8444\n",
    "Epoch 439/500\n",
    "3/3 [==============================] - 2s 510ms/step - loss: 0.0550 - accuracy: 0.9762 - val_loss: 0.8633 - val_accuracy: 0.8444\n",
    "Epoch 440/500\n",
    "3/3 [==============================] - 2s 501ms/step - loss: 0.0585 - accuracy: 0.9881 - val_loss: 0.8571 - val_accuracy: 0.8667\n",
    "Epoch 441/500\n",
    "3/3 [==============================] - 2s 511ms/step - loss: 0.0671 - accuracy: 0.9643 - val_loss: 0.8388 - val_accuracy: 0.8889\n",
    "Epoch 442/500\n",
    "3/3 [==============================] - 2s 504ms/step - loss: 0.0233 - accuracy: 1.0000 - val_loss: 0.8201 - val_accuracy: 0.8889\n",
    "Epoch 443/500\n",
    "3/3 [==============================] - 2s 648ms/step - loss: 0.0454 - accuracy: 0.9762 - val_loss: 0.8012 - val_accuracy: 0.9111\n",
    "Epoch 444/500\n",
    "3/3 [==============================] - 2s 648ms/step - loss: 0.0604 - accuracy: 0.9762 - val_loss: 0.7807 - val_accuracy: 0.9111\n",
    "Epoch 445/500\n",
    "3/3 [==============================] - 2s 504ms/step - loss: 0.0397 - accuracy: 0.9881 - val_loss: 0.7733 - val_accuracy: 0.9111\n",
    "Epoch 446/500\n",
    "3/3 [==============================] - 2s 649ms/step - loss: 0.0845 - accuracy: 0.9762 - val_loss: 0.7923 - val_accuracy: 0.9111\n",
    "Epoch 447/500\n",
    "3/3 [==============================] - 2s 504ms/step - loss: 0.0879 - accuracy: 0.9881 - val_loss: 0.7960 - val_accuracy: 0.9111\n",
    "Epoch 448/500\n",
    "3/3 [==============================] - 2s 516ms/step - loss: 0.0516 - accuracy: 0.9881 - val_loss: 0.7920 - val_accuracy: 0.9111\n",
    "Epoch 449/500\n",
    "3/3 [==============================] - 2s 651ms/step - loss: 0.0499 - accuracy: 0.9881 - val_loss: 0.7854 - val_accuracy: 0.9111\n",
    "Epoch 450/500\n",
    "3/3 [==============================] - 2s 501ms/step - loss: 0.1137 - accuracy: 0.9583 - val_loss: 0.7619 - val_accuracy: 0.8889\n",
    "Epoch 451/500\n",
    "3/3 [==============================] - 2s 648ms/step - loss: 0.0434 - accuracy: 0.9821 - val_loss: 0.8464 - val_accuracy: 0.8889\n",
    "Epoch 452/500\n",
    "3/3 [==============================] - 2s 650ms/step - loss: 0.0975 - accuracy: 0.9762 - val_loss: 0.9132 - val_accuracy: 0.8889\n",
    "Epoch 453/500\n",
    "3/3 [==============================] - 2s 524ms/step - loss: 0.0555 - accuracy: 0.9881 - val_loss: 0.9540 - val_accuracy: 0.8889\n",
    "Epoch 454/500\n",
    "3/3 [==============================] - 2s 501ms/step - loss: 0.0696 - accuracy: 0.9702 - val_loss: 0.9980 - val_accuracy: 0.8889\n",
    "Epoch 455/500\n",
    "3/3 [==============================] - 2s 500ms/step - loss: 0.0630 - accuracy: 0.9821 - val_loss: 1.0655 - val_accuracy: 0.8889\n",
    "Epoch 456/500\n",
    "3/3 [==============================] - 2s 502ms/step - loss: 0.0959 - accuracy: 0.9643 - val_loss: 1.1411 - val_accuracy: 0.8667\n",
    "Epoch 457/500\n",
    "3/3 [==============================] - 2s 652ms/step - loss: 0.1284 - accuracy: 0.9583 - val_loss: 1.2324 - val_accuracy: 0.8667\n",
    "Epoch 458/500\n",
    "3/3 [==============================] - 2s 648ms/step - loss: 0.0656 - accuracy: 0.9821 - val_loss: 1.2837 - val_accuracy: 0.8667\n",
    "Epoch 459/500\n",
    "3/3 [==============================] - 2s 503ms/step - loss: 0.0650 - accuracy: 0.9762 - val_loss: 1.3045 - val_accuracy: 0.8667\n",
    "Epoch 460/500\n",
    "3/3 [==============================] - 2s 505ms/step - loss: 0.0621 - accuracy: 0.9821 - val_loss: 1.2889 - val_accuracy: 0.8667\n",
    "Epoch 461/500\n",
    "3/3 [==============================] - 2s 649ms/step - loss: 0.0442 - accuracy: 0.9821 - val_loss: 1.2749 - val_accuracy: 0.8667\n",
    "Epoch 462/500\n",
    "3/3 [==============================] - 2s 518ms/step - loss: 0.0606 - accuracy: 0.9702 - val_loss: 1.2461 - val_accuracy: 0.8667\n",
    "Epoch 463/500\n",
    "3/3 [==============================] - 2s 503ms/step - loss: 0.0695 - accuracy: 0.9821 - val_loss: 1.2799 - val_accuracy: 0.8667\n",
    "Epoch 464/500\n",
    "3/3 [==============================] - 2s 509ms/step - loss: 0.0573 - accuracy: 0.9762 - val_loss: 1.2458 - val_accuracy: 0.8667\n",
    "Epoch 465/500\n",
    "3/3 [==============================] - 2s 513ms/step - loss: 0.0328 - accuracy: 0.9881 - val_loss: 1.1971 - val_accuracy: 0.8667\n",
    "Epoch 466/500\n",
    "3/3 [==============================] - 2s 498ms/step - loss: 0.0480 - accuracy: 0.9940 - val_loss: 1.1912 - val_accuracy: 0.8667\n",
    "Epoch 467/500\n",
    "3/3 [==============================] - 2s 503ms/step - loss: 0.0431 - accuracy: 0.9881 - val_loss: 1.2128 - val_accuracy: 0.8667\n",
    "Epoch 468/500\n",
    "3/3 [==============================] - 2s 514ms/step - loss: 0.0546 - accuracy: 0.9762 - val_loss: 1.1397 - val_accuracy: 0.8667\n",
    "Epoch 469/500\n",
    "3/3 [==============================] - 2s 501ms/step - loss: 0.1484 - accuracy: 0.9524 - val_loss: 1.0853 - val_accuracy: 0.8667\n",
    "Epoch 470/500\n",
    "3/3 [==============================] - 2s 526ms/step - loss: 0.1363 - accuracy: 0.9643 - val_loss: 1.1000 - val_accuracy: 0.8667\n",
    "Epoch 471/500\n",
    "3/3 [==============================] - 2s 661ms/step - loss: 0.0545 - accuracy: 0.9762 - val_loss: 1.0894 - val_accuracy: 0.8667\n",
    "Epoch 472/500\n",
    "3/3 [==============================] - 2s 652ms/step - loss: 0.0730 - accuracy: 0.9643 - val_loss: 1.1118 - val_accuracy: 0.8667\n",
    "Epoch 473/500\n",
    "3/3 [==============================] - 2s 508ms/step - loss: 0.0312 - accuracy: 0.9940 - val_loss: 1.1152 - val_accuracy: 0.8667\n",
    "Epoch 474/500\n",
    "3/3 [==============================] - 2s 649ms/step - loss: 0.0601 - accuracy: 0.9821 - val_loss: 1.1527 - val_accuracy: 0.8667\n",
    "Epoch 475/500\n",
    "3/3 [==============================] - 2s 645ms/step - loss: 0.0617 - accuracy: 0.9762 - val_loss: 1.1961 - val_accuracy: 0.8667\n",
    "Epoch 476/500\n",
    "3/3 [==============================] - 2s 502ms/step - loss: 0.0559 - accuracy: 0.9821 - val_loss: 1.2158 - val_accuracy: 0.8667\n",
    "Epoch 477/500\n",
    "3/3 [==============================] - 2s 521ms/step - loss: 0.0625 - accuracy: 0.9821 - val_loss: 1.2902 - val_accuracy: 0.8889\n",
    "Epoch 478/500\n",
    "3/3 [==============================] - 2s 515ms/step - loss: 0.0759 - accuracy: 0.9762 - val_loss: 1.3500 - val_accuracy: 0.8444\n",
    "Epoch 479/500\n",
    "3/3 [==============================] - 2s 501ms/step - loss: 0.0224 - accuracy: 0.9940 - val_loss: 1.3776 - val_accuracy: 0.8444\n",
    "Epoch 480/500\n",
    "3/3 [==============================] - 2s 659ms/step - loss: 0.0291 - accuracy: 0.9881 - val_loss: 1.3871 - val_accuracy: 0.8444\n",
    "Epoch 481/500\n",
    "3/3 [==============================] - 2s 524ms/step - loss: 0.0302 - accuracy: 0.9940 - val_loss: 1.3805 - val_accuracy: 0.8444\n",
    "Epoch 482/500\n",
    "3/3 [==============================] - 2s 648ms/step - loss: 0.0379 - accuracy: 0.9940 - val_loss: 1.3633 - val_accuracy: 0.8667\n",
    "Epoch 483/500\n",
    "3/3 [==============================] - 2s 507ms/step - loss: 0.0679 - accuracy: 0.9762 - val_loss: 1.2773 - val_accuracy: 0.8667\n",
    "Epoch 484/500\n",
    "3/3 [==============================] - 2s 510ms/step - loss: 0.0922 - accuracy: 0.9762 - val_loss: 1.2027 - val_accuracy: 0.8667\n",
    "Epoch 485/500\n",
    "3/3 [==============================] - 2s 509ms/step - loss: 0.0278 - accuracy: 0.9940 - val_loss: 1.0967 - val_accuracy: 0.8889\n",
    "Epoch 486/500\n",
    "3/3 [==============================] - 2s 653ms/step - loss: 0.0740 - accuracy: 0.9762 - val_loss: 1.1322 - val_accuracy: 0.8667\n",
    "Epoch 487/500\n",
    "3/3 [==============================] - 2s 526ms/step - loss: 0.0252 - accuracy: 0.9940 - val_loss: 1.1685 - val_accuracy: 0.8444\n",
    "Epoch 488/500\n",
    "3/3 [==============================] - 2s 525ms/step - loss: 0.0703 - accuracy: 0.9762 - val_loss: 1.2304 - val_accuracy: 0.8444\n",
    "Epoch 489/500\n",
    "3/3 [==============================] - 2s 499ms/step - loss: 0.0280 - accuracy: 0.9940 - val_loss: 1.2163 - val_accuracy: 0.8667\n",
    "Epoch 490/500\n",
    "3/3 [==============================] - 2s 504ms/step - loss: 0.0456 - accuracy: 0.9940 - val_loss: 1.1901 - val_accuracy: 0.8889\n",
    "Epoch 491/500\n",
    "\n",
    "3/3 [==============================] - 2s 648ms/step - loss: 0.0934 - accuracy: 0.9702 - val_loss: 1.1848 - val_accuracy: 0.8667\n",
    "Epoch 492/500\n",
    "3/3 [==============================] - 2s 653ms/step - loss: 0.0491 - accuracy: 0.9821 - val_loss: 1.1943 - val_accuracy: 0.8667\n",
    "Epoch 493/500\n",
    "3/3 [==============================] - 2s 507ms/step - loss: 0.1076 - accuracy: 0.9583 - val_loss: 1.1900 - val_accuracy: 0.8667\n",
    "Epoch 494/500\n",
    "3/3 [==============================] - 2s 663ms/step - loss: 0.0401 - accuracy: 0.9940 - val_loss: 1.1983 - val_accuracy: 0.8667\n",
    "Epoch 495/500\n",
    "3/3 [==============================] - 2s 516ms/step - loss: 0.0458 - accuracy: 0.9821 - val_loss: 1.2210 - val_accuracy: 0.8667\n",
    "Epoch 496/500\n",
    "3/3 [==============================] - 2s 513ms/step - loss: 0.0067 - accuracy: 1.0000 - val_loss: 1.2303 - val_accuracy: 0.8667\n",
    "Epoch 497/500\n",
    "3/3 [==============================] - 2s 508ms/step - loss: 0.0519 - accuracy: 0.9881 - val_loss: 1.2451 - val_accuracy: 0.8667\n",
    "Epoch 498/500\n",
    "3/3 [==============================] - 2s 513ms/step - loss: 0.0836 - accuracy: 0.9702 - val_loss: 1.2395 - val_accuracy: 0.8667\n",
    "Epoch 499/500\n",
    "3/3 [==============================] - 2s 499ms/step - loss: 0.0422 - accuracy: 0.9821 - val_loss: 1.2059 - val_accuracy: 0.8667\n",
    "Epoch 500/500\n",
    "3/3 [==============================] - 2s 653ms/step - loss: 0.0650 - accuracy: 0.9762 - val_loss: 1.2521 - val_accuracy: 0.8889\n",
    "{'loss': [2.95908260345459, 2.1094470024108887, 2.138078451156616, 1.9270095825195312, 1.7741403579711914, 1.733173131942749, 1.6602792739868164, 1.4563144445419312, 1.366180658340454, 1.5510913133621216, 1.4025843143463135, 1.3299307823181152, 1.392945408821106, 1.143754005432129, 1.143189549446106, 1.0838916301727295, 1.0908807516098022, 1.07453191280365, 0.9453669190406799, 0.9348494410514832, 1.0141942501068115, 0.7818953990936279, 0.8863324522972107, 0.8642049431800842, 0.8093708753585815, 0.7292441129684448, 0.643237292766571, 0.6736966371536255, 0.5983654856681824, 0.6591202616691589, 0.7380944490432739, 0.5630293488502502, 0.5224015116691589, 0.41594183444976807, 0.565088152885437, 0.551234245300293, 0.4456098973751068, 0.6252957582473755, 0.47323617339134216, 0.5554986596107483, 0.40131205320358276, 0.534026026725769, 0.41552454233169556, 0.4648570120334625, 0.46338263154029846, 0.43239185214042664, 0.39196476340293884, 0.44170841574668884, 0.34196487069129944, 0.3669523596763611, 0.4620266556739807, 0.403145432472229, 0.4289311468601227, 0.3713006377220154, 0.2931680679321289, 0.34874919056892395, 0.3672231435775757, 0.27308619022369385, 0.4957294464111328, 0.2460702657699585, 0.42501458525657654, 0.24079792201519012, 0.32491424679756165, 0.22495296597480774, 0.31262385845184326, 0.30855706334114075, 0.33378204703330994, 0.24212701618671417, 0.2960590720176697, 0.3047158718109131, 0.27355676889419556, 0.29401957988739014, 0.3093714118003845, 0.34460344910621643, 0.3274112343788147, 0.31848853826522827, 0.2245599925518036, 0.21789520978927612, 0.2978796362876892, 0.29020532965660095, 0.2755669057369232, 0.2078675776720047, 0.21197643876075745, 0.2137085199356079, 0.23853136599063873, 0.2322765737771988, 0.24512368440628052, 0.2595347464084625, 0.19564294815063477, 0.2912288308143616, 0.24360300600528717, 0.25573989748954773, 0.2738356292247772, 0.20500242710113525, 0.21336542069911957, 0.1883661150932312, 0.2854744493961334, 0.16367566585540771, 0.21464911103248596, 0.3652145564556122, 0.2611657679080963, 0.21353551745414734, 0.3109561502933502, 0.24345389008522034, 0.23958882689476013, 0.1998898684978485, 0.23475803434848785, 0.18366050720214844, 0.24145187437534332, 0.34524038434028625, 0.16039562225341797, 0.25662094354629517, 0.13651682436466217, 0.24828720092773438, 0.12889328598976135, 0.20116017758846283, 0.27542099356651306, 0.23872892558574677, 0.2808765470981598, 0.17942872643470764, 0.21238917112350464, 0.14035974442958832, 0.1491348147392273, 0.1842092126607895, 0.13580267131328583, 0.31276991963386536, 0.22079670429229736, 0.2224271446466446, 0.3018577992916107, 0.12637338042259216, 0.19956029951572418, 0.21368660032749176, 0.26595714688301086, 0.18699288368225098, 0.12877964973449707, 0.3574887216091156, 0.22239430248737335, 0.2543523609638214, 0.3036765456199646, 0.2051912099123001, 0.202616348862648, 0.21008774638175964, 0.20579467713832855, 0.19690917432308197, 0.17005737125873566, 0.18508312106132507, 0.19959571957588196, 0.18827955424785614, 0.14264215528964996, 0.16686412692070007, 0.095218725502491, 0.11835356056690216, 0.18323828279972076, 0.15758159756660461, 0.1603994369506836, 0.18547287583351135, 0.1200568825006485, 0.20171837508678436, 0.1215922012925148, 0.15578272938728333, 0.1538151353597641, 0.1556367427110672, 0.10635671019554138, 0.18246415257453918, 0.18374283611774445, 0.1566699743270874, 0.10925596207380295, 0.2159247100353241, 0.1551910638809204, 0.1762683093547821, 0.13411568105220795, 0.06233592331409454, 0.13395388424396515, 0.18247529864311218, 0.09431488811969757, 0.10102421790361404, 0.22720634937286377, 0.12799635529518127, 0.20281794667243958, 0.10907820612192154, 0.09706319123506546, 0.14971381425857544, 0.2113369107246399, 0.10045153647661209, 0.0986241027712822, 0.15381599962711334, 0.11025441437959671, 0.11148077994585037, 0.07207901775836945, 0.11688760668039322, 0.0804164707660675, 0.1826961189508438, 0.09884515404701233, 0.08266409486532211, 0.10438334196805954, 0.07103455066680908, 0.09379124641418457, 0.08031731843948364, 0.1656089425086975, 0.07599607855081558, 0.08116573840379715, 0.1157970279455185, 0.128836989402771, 0.07513492554426193, 0.10154717415571213, 0.11872478574514389, 0.0843316987156868, 0.05929339677095413, 0.10132219642400742, 0.08508000522851944, 0.14785364270210266, 0.1095181405544281, 0.07212929427623749, 0.059802811592817307, 0.08071137964725494, 0.11125443875789642, 0.09696422517299652, 0.11312204599380493, 0.13438042998313904, 0.16185295581817627, 0.18380038440227509, 0.07263992726802826, 0.06190939620137215, 0.0993572399020195, 0.1258668750524521, 0.0982600599527359, 0.08936747163534164, 0.09581340849399567, 0.14864374697208405, 0.07186172902584076, 0.051202014088630676, 0.12815797328948975, 0.12419039011001587, 0.11332112550735474, 0.16551533341407776, 0.06015113368630409, 0.0705580785870552, 0.1524682343006134, 0.08936374634504318, 0.058858610689640045, 0.24388858675956726, 0.14711572229862213, 0.1621294915676117, 0.08076342195272446, 0.08466484397649765, 0.11449640244245529, 0.10432940721511841, 0.08341284841299057, 0.15822197496891022, 0.14860013127326965, 0.06388720870018005, 0.0806996077299118, 0.10421044379472733, 0.08598748594522476, 0.0850176215171814, 0.10910378396511078, 0.10534332692623138, 0.056275300681591034, 0.10614477097988129, 0.07715415209531784, 0.11385127156972885, 0.07392147928476334, 0.05716323107481003, 0.11049844324588776, 0.11227691918611526, 0.10391534864902496, 0.0767655000090599, 0.11421821266412735, 0.06778327375650406, 0.11322160065174103, 0.09861880540847778, 0.09297869354486465, 0.11263114213943481, 0.10299976915121078, 0.10267732292413712, 0.11025895178318024, 0.07456842064857483, 0.07916098088026047, 0.060892753303050995, 0.07660853862762451, 0.06540761888027191, 0.09594429284334183, 0.10157346725463867, 0.13713496923446655, 0.12023098021745682, 0.11612793058156967, 0.06992074102163315, 0.039410170167684555, 0.17766588926315308, 0.10813134908676147, 0.13011333346366882, 0.08398103713989258, 0.14075471460819244, 0.06728492677211761, 0.1645125150680542, 0.06370624154806137, 0.06272011995315552, 0.14882349967956543, 0.23782162368297577, 0.15381290018558502, 0.09093737602233887, 0.12175505608320236, 0.14086711406707764, 0.10158068686723709, 0.16355803608894348, 0.05299215018749237, 0.12079963088035583, 0.20576131343841553, 0.06895256042480469, 0.0660431981086731, 0.13645049929618835, 0.14177441596984863, 0.12152671813964844, 0.13606049120426178, 0.22136016190052032, 0.1689702421426773, 0.11988568305969238, 0.06636746227741241, 0.052030257880687714, 0.21541477739810944, 0.13298767805099487, 0.08639532327651978, 0.08590497076511383, 0.1065763533115387, 0.0726175457239151, 0.07776705175638199, 0.040553443133831024, 0.1108522042632103, 0.1002461314201355, 0.08454546332359314, 0.13045646250247955, 0.08123829215765, 0.09535706788301468, 0.11556443572044373, 0.11511534452438354, 0.03435734286904335, 0.043123308569192886, 0.16057708859443665, 0.04059590771794319, 0.04197028651833534, 0.0938723087310791, 0.011684374883770943, 0.11122170090675354, 0.051031000912189484, 0.07648440450429916, 0.09807177633047104, 0.05982125550508499, 0.20891354978084564, 0.057965438812971115, 0.07965404540300369, 0.05474890395998955, 0.10932978242635727, 0.08613346517086029, 0.12436336278915405, 0.07341061532497406, 0.07380788773298264, 0.03249158337712288, 0.16911320388317108, 0.12403849512338638, 0.12504549324512482, 0.12246263772249222, 0.07296117395162582, 0.05601854622364044, 0.07360347360372543, 0.09192922711372375, 0.10264880210161209, 0.037776120007038116, 0.08468630164861679, 0.1288585662841797, 0.1279572695493698, 0.10282745957374573, 0.13994017243385315, 0.08211839944124222, 0.10527392476797104, 0.10782589018344879, 0.13090437650680542, 0.09112308919429779, 0.14812949299812317, 0.0852692723274231, 0.17551466822624207, 0.11822396516799927, 0.07841581851243973, 0.09203355014324188, 0.08736079186201096, 0.14218971133232117, 0.11744683235883713, 0.09140186011791229, 0.025556785985827446, 0.13191506266593933, 0.09792139381170273, 0.07742859423160553, 0.05410830304026604, 0.03631886467337608, 0.060819465667009354, 0.08009444177150726, 0.13313595950603485, 0.07557638734579086, 0.04516604542732239, 0.04823935776948929, 0.1233028918504715, 0.06707890331745148, 0.07709580659866333, 0.06348107755184174, 0.09783965349197388, 0.044040385633707047, 0.13346610963344574, 0.10490621626377106, 0.02916644886136055, 0.05443098023533821, 0.06224414333701134, 0.10411756485700607, 0.0751730352640152, 0.03226795792579651, 0.10920776426792145, 0.06022340804338455, 0.05605154484510422, 0.06869486719369888, 0.04652781039476395, 0.04015521705150604, 0.10080701857805252, 0.06290418654680252, 0.06195123493671417, 0.07296691834926605, 0.06947004050016403, 0.11184276640415192, 0.05892715975642204, 0.06401930749416351, 0.16701945662498474, 0.0551120899617672, 0.07468317449092865, 0.03291159123182297, 0.07578136771917343, 0.09448760002851486, 0.09552574157714844, 0.023189431056380272, 0.0981866717338562, 0.05054883658885956, 0.08257970958948135, 0.05496218800544739, 0.058485180139541626, 0.06706339120864868, 0.023250039666891098, 0.04536771774291992, 0.060423579066991806, 0.03969216346740723, 0.08448781073093414, 0.08790547400712967, 0.05163456127047539, 0.04986875504255295, 0.11367058753967285, 0.04344968497753143, 0.09753405302762985, 0.05553852766752243, 0.06961822509765625, 0.06297139078378677, 0.09586840867996216, 0.12843720614910126, 0.06559697538614273, 0.0650096982717514, 0.06205931305885315, 0.04423904791474342, 0.06058434024453163, 0.06947969645261765, 0.057289354503154755, 0.03275807946920395, 0.04801180213689804, 0.04313118755817413, 0.05455248802900314, 0.1484105885028839, 0.13625061511993408, 0.05449290573596954, 0.07298554480075836, 0.03121786378324032, 0.060146264731884, 0.06165216118097305, 0.055930245667696, 0.06254442781209946, 0.07594047486782074, 0.022407522425055504, 0.029086817055940628, 0.03018076717853546, 0.03791545704007149, 0.06787879019975662, 0.0922311469912529, 0.027808764949440956, 0.07399690896272659, 0.02521769516170025, 0.07027426362037659, 0.027963487431406975, 0.04563774913549423, 0.09343741089105606, 0.04906551167368889, 0.10762207955121994, 0.04014069214463234, 0.0458093099296093, 0.0066744303330779076, 0.05194137245416641, 0.0836050733923912, 0.042216140776872635, 0.06496267020702362], 'accuracy': [0.1607142835855484, 0.3273809552192688, 0.2916666567325592, 0.3452380895614624, 0.386904776096344, 0.4583333432674408, 0.4107142984867096, 0.5059523582458496, 0.494047611951828, 0.4464285671710968, 0.5059523582458496, 0.5357142686843872, 0.4821428656578064, 0.5535714030265808, 0.523809552192688, 0.5535714030265808, 0.5654761791229248, 0.5833333134651184, 0.6845238208770752, 0.6726190447807312, 0.625, 0.7321428656578064, 0.6904761791229248, 0.7083333134651184, 0.726190447807312, 0.738095223903656, 0.7440476417541504, 0.75, 0.7916666865348816, 0.773809552192688, 0.738095223903656, 0.7678571343421936, 0.7976190447807312, 0.8392857313156128, 0.7916666865348816, 0.8035714030265808, 0.851190447807312, 0.7797619104385376, 0.7916666865348816, 0.7976190447807312, 0.8452380895614624, 0.7916666865348816, 0.8333333134651184, 0.851190447807312, 0.8214285969734192, 0.8214285969734192, 0.8392857313156128, 0.8571428656578064, 0.8928571343421936, 0.8452380895614624, 0.8809523582458496, 0.8214285969734192, 0.8571428656578064, 0.863095223903656, 0.8809523582458496, 0.8928571343421936, 0.8214285969734192, 0.8928571343421936, 0.8571428656578064, 0.898809552192688, 0.8035714030265808, 0.9107142686843872, 0.886904776096344, 0.9226190447807312, 0.8928571343421936, 0.886904776096344, 0.863095223903656, 0.898809552192688, 0.875, 0.886904776096344, 0.886904776096344, 0.8690476417541504, 0.898809552192688, 0.863095223903656, 0.898809552192688, 0.8928571343421936, 0.9226190447807312, 0.9226190447807312, 0.875, 0.9226190447807312, 0.898809552192688, 0.9345238208770752, 0.9166666865348816, 0.9166666865348816, 0.9107142686843872, 0.9047619104385376, 0.9226190447807312, 0.898809552192688, 0.9345238208770752, 0.8928571343421936, 0.9404761791229248, 0.9107142686843872, 0.9107142686843872, 0.898809552192688, 0.9166666865348816, 0.9166666865348816, 0.8809523582458496, 0.9285714030265808, 0.9226190447807312, 0.8928571343421936, 0.9166666865348816, 0.9226190447807312, 0.863095223903656, 0.9226190447807312, 0.9107142686843872, 0.9226190447807312, 0.9107142686843872, 0.9285714030265808, 0.898809552192688, 0.8928571343421936, 0.9226190447807312, 0.898809552192688, 0.9523809552192688, 0.9226190447807312, 0.9642857313156128, 0.9226190447807312, 0.9226190447807312, 0.898809552192688, 0.9285714030265808, 0.9166666865348816, 0.9285714030265808, 0.9404761791229248, 0.9404761791229248, 0.9345238208770752, 0.9583333134651184, 0.8928571343421936, 0.9285714030265808, 0.9285714030265808, 0.8809523582458496, 0.9702380895614624, 0.9464285969734192, 0.9047619104385376, 0.9107142686843872, 0.9226190447807312, 0.9583333134651184, 0.898809552192688, 0.9226190447807312, 0.9166666865348816, 0.875, 0.9464285969734192, 0.9107142686843872, 0.9285714030265808, 0.9226190447807312, 0.9166666865348816, 0.9166666865348816, 0.9345238208770752, 0.9166666865348816, 0.9404761791229248, 0.9345238208770752, 0.9523809552192688, 0.9642857313156128, 0.9583333134651184, 0.9285714030265808, 0.9285714030265808, 0.9404761791229248, 0.9166666865348816, 0.9702380895614624, 0.9583333134651184, 0.9404761791229248, 0.9523809552192688, 0.9523809552192688, 0.9642857313156128, 0.9702380895614624, 0.9345238208770752, 0.9464285969734192, 0.9345238208770752, 0.9642857313156128, 0.9285714030265808, 0.9345238208770752, 0.9464285969734192, 0.9523809552192688, 0.976190447807312, 0.9642857313156128, 0.9166666865348816, 0.9642857313156128, 0.9702380895614624, 0.9345238208770752, 0.9464285969734192, 0.9345238208770752, 0.976190447807312, 0.9523809552192688, 0.9523809552192688, 0.9226190447807312, 0.9583333134651184, 0.9642857313156128, 0.9404761791229248, 0.9702380895614624, 0.9345238208770752, 0.976190447807312, 0.9702380895614624, 0.9702380895614624, 0.9464285969734192, 0.9583333134651184, 0.9583333134651184, 0.976190447807312, 0.9702380895614624, 0.9583333134651184, 0.9642857313156128, 0.9464285969734192, 0.9702380895614624, 0.9523809552192688, 0.9642857313156128, 0.9404761791229248, 0.9821428656578064, 0.9523809552192688, 0.9523809552192688, 0.976190447807312, 0.988095223903656, 0.9642857313156128, 0.9702380895614624, 0.9583333134651184, 0.9642857313156128, 0.976190447807312, 0.9821428656578064, 0.9702380895614624, 0.9523809552192688, 0.9642857313156128, 0.9583333134651184, 0.9404761791229248, 0.9523809552192688, 0.9464285969734192, 0.9642857313156128, 0.976190447807312, 0.976190447807312, 0.9702380895614624, 0.9583333134651184, 0.9702380895614624, 0.9583333134651184, 0.9464285969734192, 0.9702380895614624, 0.988095223903656, 0.9642857313156128, 0.9404761791229248, 0.9702380895614624, 0.9404761791229248, 0.976190447807312, 0.9702380895614624, 0.9583333134651184, 0.9702380895614624, 0.988095223903656, 0.9285714030265808, 0.9583333134651184, 0.9404761791229248, 0.9642857313156128, 0.9642857313156128, 0.9642857313156128, 0.9642857313156128, 0.976190447807312, 0.9404761791229248, 0.9642857313156128, 0.976190447807312, 0.9642857313156128, 0.9702380895614624, 0.9642857313156128, 0.9702380895614624, 0.9642857313156128, 0.9642857313156128, 0.988095223903656, 0.9642857313156128, 0.9642857313156128, 0.9583333134651184, 0.976190447807312, 0.9821428656578064, 0.9523809552192688, 0.9583333134651184, 0.9583333134651184, 0.976190447807312, 0.9583333134651184, 0.9702380895614624, 0.9642857313156128, 0.9702380895614624, 0.976190447807312, 0.9583333134651184, 0.9523809552192688, 0.9642857313156128, 0.9702380895614624, 0.988095223903656, 0.976190447807312, 0.9702380895614624, 0.976190447807312, 0.988095223903656, 0.9702380895614624, 0.9702380895614624, 0.9702380895614624, 0.976190447807312, 0.9702380895614624, 0.9583333134651184, 0.988095223903656, 0.9464285969734192, 0.9702380895614624, 0.9404761791229248, 0.9702380895614624, 0.9523809552192688, 0.976190447807312, 0.9285714030265808, 0.9821428656578064, 0.988095223903656, 0.9464285969734192, 0.9107142686843872, 0.9583333134651184, 0.9702380895614624, 0.9583333134651184, 0.9464285969734192, 0.9702380895614624, 0.9404761791229248, 0.9702380895614624, 0.9404761791229248, 0.9464285969734192, 0.9702380895614624, 0.9702380895614624, 0.9523809552192688, 0.9523809552192688, 0.9523809552192688, 0.9523809552192688, 0.8928571343421936, 0.9523809552192688, 0.9464285969734192, 0.976190447807312, 0.9940476417541504, 0.9464285969734192, 0.9464285969734192, 0.9702380895614624, 0.9642857313156128, 0.976190447807312, 0.9821428656578064, 0.9821428656578064, 0.9821428656578064, 0.9642857313156128, 0.9702380895614624, 0.9821428656578064, 0.9523809552192688, 0.976190447807312, 0.9642857313156128, 0.9642857313156128, 0.9523809552192688, 0.988095223903656, 0.988095223903656, 0.9226190447807312, 0.9821428656578064, 0.9821428656578064, 0.9642857313156128, 1.0, 0.9702380895614624, 0.976190447807312, 0.9821428656578064, 0.9642857313156128, 0.976190447807312, 0.9464285969734192, 0.976190447807312, 0.9702380895614624, 0.988095223903656, 0.9642857313156128, 0.9702380895614624, 0.9583333134651184, 0.9642857313156128, 0.976190447807312, 0.9940476417541504, 0.9345238208770752, 0.9464285969734192, 0.9642857313156128, 0.9583333134651184, 0.988095223903656, 0.976190447807312, 0.9642857313156128, 0.9702380895614624, 0.9583333134651184, 0.988095223903656, 0.9583333134651184, 0.9464285969734192, 0.9583333134651184, 0.9583333134651184, 0.9464285969734192, 0.9821428656578064, 0.9642857313156128, 0.9583333134651184, 0.9523809552192688, 0.9702380895614624, 0.9345238208770752, 0.9642857313156128, 0.9464285969734192, 0.9583333134651184, 0.976190447807312, 0.9642857313156128, 0.9702380895614624, 0.9583333134651184, 0.9702380895614624, 0.976190447807312, 0.9940476417541504, 0.9702380895614624, 0.976190447807312, 0.9523809552192688, 0.9702380895614624, 0.988095223903656, 0.9702380895614624, 0.976190447807312, 0.9464285969734192, 0.9642857313156128, 0.9821428656578064, 0.9702380895614624, 0.9523809552192688, 0.976190447807312, 0.9642857313156128, 0.976190447807312, 0.9642857313156128, 0.9940476417541504, 0.9523809552192688, 0.9583333134651184, 0.9940476417541504, 0.976190447807312, 0.9821428656578064, 0.9642857313156128, 0.9642857313156128, 0.988095223903656, 0.9642857313156128, 0.9821428656578064, 0.9821428656578064, 0.9642857313156128, 0.976190447807312, 0.988095223903656, 0.9583333134651184, 0.976190447807312, 0.9642857313156128, 0.9702380895614624, 0.9702380895614624, 0.9523809552192688, 0.988095223903656, 0.976190447807312, 0.9523809552192688, 0.9821428656578064, 0.9642857313156128, 0.9940476417541504, 0.9702380895614624, 0.9583333134651184, 0.9642857313156128, 0.988095223903656, 0.9702380895614624, 0.9821428656578064, 0.976190447807312, 0.976190447807312, 0.988095223903656, 0.9642857313156128, 1.0, 0.976190447807312, 0.976190447807312, 0.988095223903656, 0.976190447807312, 0.988095223903656, 0.988095223903656, 0.988095223903656, 0.9583333134651184, 0.9821428656578064, 0.976190447807312, 0.988095223903656, 0.9702380895614624, 0.9821428656578064, 0.9642857313156128, 0.9583333134651184, 0.9821428656578064, 0.976190447807312, 0.9821428656578064, 0.9821428656578064, 0.9702380895614624, 0.9821428656578064, 0.976190447807312, 0.988095223903656, 0.9940476417541504, 0.988095223903656, 0.976190447807312, 0.9523809552192688, 0.9642857313156128, 0.976190447807312, 0.9642857313156128, 0.9940476417541504, 0.9821428656578064, 0.976190447807312, 0.9821428656578064, 0.9821428656578064, 0.976190447807312, 0.9940476417541504, 0.988095223903656, 0.9940476417541504, 0.9940476417541504, 0.976190447807312, 0.976190447807312, 0.9940476417541504, 0.976190447807312, 0.9940476417541504, 0.976190447807312, 0.9940476417541504, 0.9940476417541504, 0.9702380895614624, 0.9821428656578064, 0.9583333134651184, 0.9940476417541504, 0.9821428656578064, 1.0, 0.988095223903656, 0.9702380895614624, 0.9821428656578064, 0.976190447807312], 'val_loss': [2.337362051010132, 2.9484989643096924, 3.3269848823547363, 3.5543832778930664, 4.7871994972229, 6.427802085876465, 6.875142574310303, 6.662641525268555, 6.202365398406982, 6.451361656188965, 7.545226573944092, 7.613083839416504, 7.09066915512085, 6.47061014175415, 6.159397602081299, 5.817906379699707, 5.697127342224121, 5.408501625061035, 4.807544708251953, 4.091978073120117, 3.6588494777679443, 3.426290273666382, 3.3581364154815674, 3.3747518062591553, 3.2029733657836914, 3.3548834323883057, 3.676459312438965, 3.7777535915374756, 3.7234463691711426, 3.5274930000305176, 2.928403615951538, 2.678149700164795, 2.7227909564971924, 2.738535165786743, 3.2920172214508057, 4.652506351470947, 5.333517074584961, 4.226428031921387, 3.2454326152801514, 2.5499954223632812, 2.1074700355529785, 1.9406955242156982, 1.975865364074707, 1.9863200187683105, 1.9085344076156616, 1.8043639659881592, 1.7201025485992432, 1.7768410444259644, 1.866887092590332, 1.828763484954834, 1.613802194595337, 1.421373724937439, 1.2836202383041382, 1.1691641807556152, 1.1155563592910767, 1.1421260833740234, 1.1841446161270142, 1.2640607357025146, 1.2276034355163574, 1.1202659606933594, 0.9177986979484558, 0.8166405558586121, 0.8127270340919495, 0.8983515501022339, 0.9692408442497253, 1.0460314750671387, 1.1398816108703613, 1.0808672904968262, 0.9842047691345215, 0.8731116652488708, 0.7847884893417358, 0.7545297145843506, 0.7749603390693665, 0.806835412979126, 0.8931449055671692, 1.4074499607086182, 1.6744204759597778, 1.8886386156082153, 1.9724477529525757, 1.8842179775238037, 1.6334784030914307, 1.4144859313964844, 1.1923296451568604, 0.9911007285118103, 0.7918015122413635, 0.6808077096939087, 0.6378013491630554, 0.6488251090049744, 0.6950916051864624, 0.6190918684005737, 0.655022919178009, 0.767395555973053, 0.8128077983856201, 0.8176888227462769, 0.8144480586051941, 0.7991721034049988, 0.8432245850563049, 0.9309417605400085, 0.9234727621078491, 1.0249401330947876, 1.1157667636871338, 1.1973668336868286, 1.1227847337722778, 0.9906090497970581, 0.8468812108039856, 0.7883244752883911, 0.7592978477478027, 0.7680513262748718, 0.7865720391273499, 0.7938464879989624, 0.783319354057312, 0.788104236125946, 0.7988173365592957, 0.792818009853363, 0.8251724243164062, 0.8502969741821289, 0.9251899123191833, 0.8773018717765808, 0.7983942031860352, 0.7354322075843811, 5.53761100769043, 20.121299743652344, 14.180015563964844, 10.638304710388184, 10.153412818908691, 8.969377517700195, 6.384521484375, 4.278139114379883, 2.508026361465454, 2.0888216495513916, 2.2071492671966553, 2.178370237350464, 1.9325398206710815, 1.5740630626678467, 1.3322731256484985, 1.2389938831329346, 1.1638903617858887, 1.2670526504516602, 1.31983482837677, 1.349894404411316, 1.2727152109146118, 1.1060702800750732, 0.9403108358383179, 0.8670828342437744, 0.8529046773910522, 0.8357517719268799, 0.8209591507911682, 0.9346787333488464, 1.0046708583831787, 1.055780291557312, 1.022026538848877, 0.9884965419769287, 0.907689094543457, 0.8657647967338562, 0.8757117390632629, 0.9150952100753784, 0.9790568947792053, 1.0205578804016113, 1.045722484588623, 1.0109808444976807, 0.9740749597549438, 0.9353408813476562, 0.921489953994751, 0.9410532712936401, 0.9997275471687317, 1.0350451469421387, 1.0613293647766113, 1.0123827457427979, 0.9845438599586487, 1.016605257987976, 1.129687786102295, 1.222272515296936, 1.2876923084259033, 1.0967957973480225, 1.0224249362945557, 0.9200754165649414, 0.9087032675743103, 0.9291438460350037, 0.8946868777275085, 0.9556816816329956, 1.000712275505066, 1.0334445238113403, 1.085397720336914, 1.101690649986267, 1.0722219944000244, 1.004201054573059, 0.972564697265625, 0.9995757937431335, 1.0203449726104736, 1.0565299987792969, 1.0505024194717407, 1.0777149200439453, 1.067716121673584, 0.999011754989624, 0.9358351826667786, 0.903090238571167, 0.8981634378433228, 0.8688971400260925, 0.7973088622093201, 0.7156437635421753, 0.6700883507728577, 0.613431990146637, 0.5617713332176208, 0.5654569268226624, 0.5839942693710327, 0.6496861577033997, 0.7334883213043213, 0.8204290866851807, 0.891013503074646, 0.9126109480857849, 0.9811180830001831, 1.082694172859192, 1.1992168426513672, 1.1833219528198242, 83.54351806640625, 45.62351989746094, 26.734119415283203, 15.669730186462402, 9.774227142333984, 6.844722270965576, 5.110479831695557, 4.191381931304932, 3.6117608547210693, 3.1338679790496826, 2.8179612159729004, 2.5283408164978027, 2.1999504566192627, 1.9457910060882568, 1.6881883144378662, 1.579293966293335, 1.5355541706085205, 1.4355206489562988, 1.3408267498016357, 1.3117504119873047, 1.244768738746643, 1.1327592134475708, 1.0753538608551025, 1.0468922853469849, 1.0275932550430298, 1.0529475212097168, 1.107851505279541, 1.103761076927185, 1.0583444833755493, 0.9987238645553589, 0.9041029810905457, 0.917910099029541, 0.8949249386787415, 0.8978897333145142, 0.904612123966217, 0.891707718372345, 0.8956989049911499, 0.9358366131782532, 0.9752295017242432, 0.991470992565155, 1.0862081050872803, 1.3125464916229248, 1.4115930795669556, 1.318982481956482, 1.0724691152572632, 0.9123430848121643, 0.7317467331886292, 0.658782422542572, 0.6311448216438293, 0.6059969663619995, 0.620524525642395, 0.6310352087020874, 0.6625164747238159, 0.7342567443847656, 0.7947564125061035, 0.7712321877479553, 0.7325848937034607, 0.7290593385696411, 0.8718105554580688, 1.0593175888061523, 1.216312289237976, 1.2476574182510376, 1.2171474695205688, 1.202945590019226, 1.2006008625030518, 1.2480661869049072, 1.2405951023101807, 1.3009415864944458, 1.3042536973953247, 1.2375752925872803, 1.2421995401382446, 1.244821310043335, 1.2793419361114502, 1.2823430299758911, 1.2577316761016846, 1.1854068040847778, 1.1307644844055176, 1.1706467866897583, 1.220345377922058, 1.252368450164795, 1.1636425256729126, 1.0881125926971436, 1.0771230459213257, 1.1195368766784668, 1.099173665046692, 1.0328786373138428, 0.9863108396530151, 1.0695033073425293, 1.1087257862091064, 1.1394495964050293, 1.1797069311141968, 1.2629122734069824, 1.3379895687103271, 1.3582782745361328, 1.2618861198425293, 1.1345813274383545, 1.1260020732879639, 1.150871753692627, 1.0723772048950195, 1.0677796602249146, 1.040317416191101, 1.0460561513900757, 1.0001741647720337, 0.934604287147522, 0.8991532325744629, 0.9494737982749939, 0.9960981607437134, 1.1242127418518066, 1.2702693939208984, 1.37931489944458, 1.3427850008010864, 1.1844100952148438, 1.0646425485610962, 0.9933367967605591, 0.9795261025428772, 0.9963431358337402, 1.0051097869873047, 1.0194979906082153, 1.022989273071289, 1.0169780254364014, 1.013365387916565, 1.024323582649231, 1.034561276435852, 1.0613343715667725, 1.1202278137207031, 1.169429063796997, 1.1928606033325195, 1.1994390487670898, 1.1486811637878418, 1.115469217300415, 1.0895155668258667, 1.0452688932418823, 1.0173919200897217, 0.9708729982376099, 0.9521257281303406, 0.966428816318512, 0.985370397567749, 0.9868148565292358, 1.0060209035873413, 1.0221532583236694, 0.9838309288024902, 0.9469926357269287, 0.9389522075653076, 0.9271009564399719, 0.8917754292488098, 0.9001493453979492, 0.9001898765563965, 0.8667050004005432, 0.8176705241203308, 0.7916392087936401, 0.8147985935211182, 0.9243690371513367, 1.041691780090332, 1.140722632408142, 1.1780263185501099, 1.1455788612365723, 1.1090960502624512, 1.0850509405136108, 1.0368711948394775, 0.9703266620635986, 0.9337330460548401, 0.8961203694343567, 0.9009523987770081, 0.9590975642204285, 1.0450167655944824, 1.1140631437301636, 1.126890778541565, 1.01055908203125, 0.9740353226661682, 0.964479923248291, 0.921872615814209, 0.8990203738212585, 0.9045080542564392, 0.9293462038040161, 0.9127904176712036, 0.937481164932251, 1.0249861478805542, 1.0687544345855713, 1.0460362434387207, 1.0165257453918457, 0.9743021726608276, 0.9169034957885742, 0.8522232174873352, 0.8028574585914612, 0.7824552655220032, 0.7321624755859375, 0.709179162979126, 0.7189042568206787, 0.7306302189826965, 0.7413835525512695, 0.7718359231948853, 0.8404682874679565, 0.9298384189605713, 1.0490671396255493, 1.1525834798812866, 1.2437998056411743, 1.2870824337005615, 1.281539797782898, 1.2224748134613037, 1.1585296392440796, 1.0721077919006348, 0.9712545871734619, 0.8886042237281799, 0.833372175693512, 0.7790682315826416, 0.7633200883865356, 0.7918173670768738, 0.8270675539970398, 0.8321598768234253, 0.8604971170425415, 0.9283581376075745, 1.0091999769210815, 1.0679776668548584, 1.0828074216842651, 1.0643495321273804, 1.0342379808425903, 1.0048394203186035, 0.9507988095283508, 0.9148461222648621, 0.8794543147087097, 0.8608143925666809, 0.8456722497940063, 0.8690623641014099, 0.8699833154678345, 0.8633190393447876, 0.8570864200592041, 0.8388222455978394, 0.8200610876083374, 0.8012286424636841, 0.7806835770606995, 0.7732943892478943, 0.7922610640525818, 0.7960439324378967, 0.7919709086418152, 0.7853819131851196, 0.7619261145591736, 0.8464046716690063, 0.9132344722747803, 0.9539571404457092, 0.9980167150497437, 1.065457820892334, 1.1410844326019287, 1.2323766946792603, 1.2837326526641846, 1.3044569492340088, 1.2889118194580078, 1.274863362312317, 1.246117353439331, 1.279880166053772, 1.245845913887024, 1.1970967054367065, 1.1911826133728027, 1.2127941846847534, 1.1396700143814087, 1.0853002071380615, 1.10002863407135, 1.0893723964691162, 1.1117572784423828, 1.1152360439300537, 1.1526663303375244, 1.1960530281066895, 1.2158383131027222, 1.2901662588119507, 1.3500316143035889, 1.3776053190231323, 1.3870710134506226, 1.3805022239685059, 1.363278865814209, 1.277253270149231, 1.2026770114898682, 1.0966898202896118, 1.1322495937347412, 1.1684746742248535, 1.2304202318191528, 1.216282606124878, 1.1901003122329712, 1.1847655773162842, 1.194303274154663, 1.1900131702423096, 1.1983139514923096, 1.220996618270874, 1.2303053140640259, 1.245065689086914, 1.2395107746124268, 1.2058600187301636, 1.252066731452942], 'val_accuracy': [0.13333334028720856, 0.13333334028720856, 0.15555556118488312, 0.31111112236976624, 0.15555556118488312, 0.15555556118488312, 0.15555556118488312, 0.15555556118488312, 0.15555556118488312, 0.15555556118488312, 0.15555556118488312, 0.15555556118488312, 0.15555556118488312, 0.15555556118488312, 0.15555556118488312, 0.15555556118488312, 0.15555556118488312, 0.17777778208255768, 0.20000000298023224, 0.20000000298023224, 0.20000000298023224, 0.20000000298023224, 0.2222222238779068, 0.24444444477558136, 0.2888889014720917, 0.4000000059604645, 0.4000000059604645, 0.42222222685813904, 0.31111112236976624, 0.31111112236976624, 0.4888888895511627, 0.4888888895511627, 0.5111111402511597, 0.5111111402511597, 0.4000000059604645, 0.15555556118488312, 0.15555556118488312, 0.20000000298023224, 0.2666666805744171, 0.3777777850627899, 0.46666666865348816, 0.4888888895511627, 0.5111111402511597, 0.5333333611488342, 0.5777778029441833, 0.5777778029441833, 0.5777778029441833, 0.5777778029441833, 0.5333333611488342, 0.5333333611488342, 0.5777778029441833, 0.6222222447395325, 0.644444465637207, 0.6888889074325562, 0.7555555701255798, 0.6888889074325562, 0.7111111283302307, 0.6222222447395325, 0.644444465637207, 0.7111111283302307, 0.7777777910232544, 0.7777777910232544, 0.800000011920929, 0.800000011920929, 0.7555555701255798, 0.7555555701255798, 0.7111111283302307, 0.7333333492279053, 0.8222222328186035, 0.8222222328186035, 0.8222222328186035, 0.8444444537162781, 0.8222222328186035, 0.7777777910232544, 0.7777777910232544, 0.6888889074325562, 0.6666666865348816, 0.6666666865348816, 0.6666666865348816, 0.644444465637207, 0.6666666865348816, 0.7111111283302307, 0.7333333492279053, 0.7777777910232544, 0.8444444537162781, 0.8666666746139526, 0.8666666746139526, 0.8444444537162781, 0.8444444537162781, 0.8666666746139526, 0.8444444537162781, 0.8222222328186035, 0.8222222328186035, 0.8222222328186035, 0.8444444537162781, 0.8444444537162781, 0.800000011920929, 0.7555555701255798, 0.8222222328186035, 0.8222222328186035, 0.800000011920929, 0.7555555701255798, 0.7777777910232544, 0.8222222328186035, 0.8444444537162781, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8888888955116272, 0.9111111164093018, 0.9111111164093018, 0.9111111164093018, 0.9111111164093018, 0.9111111164093018, 0.8666666746139526, 0.8666666746139526, 0.8888888955116272, 0.8888888955116272, 0.3333333432674408, 0.17777778208255768, 0.20000000298023224, 0.2222222238779068, 0.20000000298023224, 0.2222222238779068, 0.2888889014720917, 0.35555556416511536, 0.5333333611488342, 0.6666666865348816, 0.6666666865348816, 0.7333333492279053, 0.800000011920929, 0.7555555701255798, 0.7555555701255798, 0.7777777910232544, 0.800000011920929, 0.800000011920929, 0.8222222328186035, 0.7777777910232544, 0.8222222328186035, 0.8444444537162781, 0.8444444537162781, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8444444537162781, 0.8444444537162781, 0.8444444537162781, 0.8444444537162781, 0.8444444537162781, 0.8444444537162781, 0.8444444537162781, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8666666746139526, 0.9111111164093018, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8666666746139526, 0.8444444537162781, 0.8666666746139526, 0.8444444537162781, 0.8444444537162781, 0.800000011920929, 0.8444444537162781, 0.8222222328186035, 0.8444444537162781, 0.8444444537162781, 0.8444444537162781, 0.800000011920929, 0.800000011920929, 0.800000011920929, 0.800000011920929, 0.800000011920929, 0.800000011920929, 0.800000011920929, 0.8222222328186035, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8666666746139526, 0.8222222328186035, 0.800000011920929, 0.8222222328186035, 0.8888888955116272, 0.8666666746139526, 0.8666666746139526, 0.8888888955116272, 0.8666666746139526, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.9111111164093018, 0.9111111164093018, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.9111111164093018, 0.8888888955116272, 0.8888888955116272, 0.8444444537162781, 0.8666666746139526, 0.7555555701255798, 0.7555555701255798, 0.15555556118488312, 0.15555556118488312, 0.15555556118488312, 0.2222222238779068, 0.3777777850627899, 0.4888888895511627, 0.6222222447395325, 0.6222222447395325, 0.6666666865348816, 0.7111111283302307, 0.7111111283302307, 0.7333333492279053, 0.7333333492279053, 0.7333333492279053, 0.7333333492279053, 0.7777777910232544, 0.7777777910232544, 0.7777777910232544, 0.7777777910232544, 0.7777777910232544, 0.7777777910232544, 0.8444444537162781, 0.8444444537162781, 0.8444444537162781, 0.8444444537162781, 0.8222222328186035, 0.800000011920929, 0.800000011920929, 0.8222222328186035, 0.8666666746139526, 0.8888888955116272, 0.8666666746139526, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8666666746139526, 0.8222222328186035, 0.8222222328186035, 0.8444444537162781, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.9111111164093018, 0.9111111164093018, 0.9111111164093018, 0.8888888955116272, 0.9111111164093018, 0.9111111164093018, 0.8888888955116272, 0.8888888955116272, 0.8444444537162781, 0.8222222328186035, 0.8444444537162781, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8444444537162781, 0.8444444537162781, 0.8222222328186035, 0.8444444537162781, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8888888955116272, 0.8444444537162781, 0.8666666746139526, 0.8666666746139526, 0.8444444537162781, 0.8666666746139526, 0.8444444537162781, 0.8666666746139526, 0.8666666746139526, 0.8444444537162781, 0.8444444537162781, 0.8666666746139526, 0.8666666746139526, 0.8444444537162781, 0.8666666746139526, 0.8222222328186035, 0.800000011920929, 0.7777777910232544, 0.7777777910232544, 0.8222222328186035, 0.8222222328186035, 0.8444444537162781, 0.800000011920929, 0.8444444537162781, 0.8222222328186035, 0.8444444537162781, 0.8444444537162781, 0.8444444537162781, 0.8444444537162781, 0.8222222328186035, 0.8666666746139526, 0.8666666746139526, 0.800000011920929, 0.7777777910232544, 0.7555555701255798, 0.7555555701255798, 0.8222222328186035, 0.8444444537162781, 0.8444444537162781, 0.8888888955116272, 0.8888888955116272, 0.9111111164093018, 0.8888888955116272, 0.8888888955116272, 0.8666666746139526, 0.8666666746139526, 0.8444444537162781, 0.8444444537162781, 0.8444444537162781, 0.8444444537162781, 0.8444444537162781, 0.8444444537162781, 0.8444444537162781, 0.8444444537162781, 0.8222222328186035, 0.8444444537162781, 0.8444444537162781, 0.8444444537162781, 0.8444444537162781, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8666666746139526, 0.8444444537162781, 0.8444444537162781, 0.8666666746139526, 0.8666666746139526, 0.8888888955116272, 0.8666666746139526, 0.8666666746139526, 0.8888888955116272, 0.8666666746139526, 0.8666666746139526, 0.8888888955116272, 0.8444444537162781, 0.800000011920929, 0.8222222328186035, 0.8222222328186035, 0.8444444537162781, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8444444537162781, 0.8666666746139526, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.9111111164093018, 0.8888888955116272, 0.8444444537162781, 0.8444444537162781, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8888888955116272, 0.9111111164093018, 0.9111111164093018, 0.9111111164093018, 0.9111111164093018, 0.9111111164093018, 0.9111111164093018, 0.9111111164093018, 0.9111111164093018, 0.9111111164093018, 0.8444444537162781, 0.8444444537162781, 0.8666666746139526, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.9111111164093018, 0.9111111164093018, 0.9111111164093018, 0.9111111164093018, 0.9111111164093018, 0.9111111164093018, 0.9111111164093018, 0.9111111164093018, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8666666746139526, 0.8444444537162781, 0.8444444537162781, 0.8444444537162781, 0.8666666746139526, 0.8888888955116272, 0.8888888955116272, 0.9111111164093018, 0.9111111164093018, 0.9111111164093018, 0.9111111164093018, 0.9111111164093018, 0.9111111164093018, 0.9111111164093018, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8888888955116272, 0.8444444537162781, 0.8444444537162781, 0.8444444537162781, 0.8444444537162781, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8888888955116272, 0.8666666746139526, 0.8444444537162781, 0.8444444537162781, 0.8666666746139526, 0.8888888955116272, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8888888955116272]}\n",
    "\n",
    "#exp2\n",
    "\n",
    "import tensorflow\n",
    "\n",
    "from keras.layers import Dense,Flatten\n",
    "\n",
    "from keras.applications.vgg16 import VGG16\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "from keras import Sequential\n",
    "\n",
    "from keras.layers import Dense,Conv2D,MaxPooling2D,Flatten,BatchNormalization,Dropout\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam,SGD,RMSprop\n",
    "\n",
    "from tensorflow.keras.layers import Dense,Input,Dropout,GlobalAveragePooling2D,Flatten,Conv2D,BatchNormalization,Activation,MaxPooling2D\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Flatten, BatchNormalization, Dropout\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from tensorflow.keras.applications.vgg19 import VGG19\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Importing Deep Learning Libraries\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "from keras import regularizers\n",
    "\n",
    "from keras.preprocessing import image\n",
    "\n",
    "​\n",
    "\n",
    "#HISTOGRAM CODE\n",
    "\n",
    "#histogram code\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "\n",
    "​\n",
    "\n",
    "emotions = [\"happy\", \"sadness\", \"anger\", \"disgust\", \"neutral\", \"fear\", \"surprise\"]\n",
    "\n",
    "​\n",
    "\n",
    "folder_path = \"Jaffetrainvalidation/train\"\n",
    "\n",
    "# Counting the number of images per emotion\n",
    "\n",
    "counts = [len(os.listdir(os.path.join(folder_path, emotion))) for emotion in emotions]\n",
    "\n",
    "​\n",
    "\n",
    "# Plotting the bar chart\n",
    "\n",
    "colors = ['red', 'yellow', 'black', 'blue', 'orange', 'green', 'pink']\n",
    "\n",
    "plt.bar(emotions, height=counts, color=colors)\n",
    "\n",
    "plt.ylabel('Number')\n",
    "\n",
    "plt.xlabel('Emotions')\n",
    "\n",
    "​\n",
    "\n",
    "​\n",
    "\n",
    "#plt.savefig('hostgoarm.png')\n",
    "\n",
    "#plt.show()\n",
    "\n",
    "​\n",
    "\n",
    "​\n",
    "\n",
    "# Data generators\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "​\n",
    "\n",
    "# Data augmentation for training set\n",
    "\n",
    "datagen_train = ImageDataGenerator(\n",
    "\n",
    "    rescale=1./255,\n",
    "\n",
    "    rotation_range=15,\n",
    "\n",
    "    width_shift_range=0.1,\n",
    "\n",
    "    height_shift_range=0.1,\n",
    "\n",
    "    shear_range=0.2,\n",
    "\n",
    "    zoom_range=0.2,\n",
    "\n",
    "    horizontal_flip=True,\n",
    "\n",
    "    fill_mode='nearest'\n",
    "\n",
    ")\n",
    "\n",
    "​\n",
    "\n",
    "# Normalization for validation set (no augmentation)\n",
    "\n",
    "datagen_val = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "​\n",
    "\n",
    "​\n",
    "\n",
    "​\n",
    "\n",
    "train_ds = datagen_train.flow_from_directory(\"Jaffetrainvalidation/train\",\n",
    "\n",
    "                                             target_size=(256, 256),\n",
    "\n",
    "                                             color_mode=\"rgb\",\n",
    "\n",
    "                                             batch_size=batch_size,\n",
    "\n",
    "                                             class_mode='categorical',\n",
    "\n",
    "                                             shuffle=True)\n",
    "\n",
    "​\n",
    "\n",
    "test_ds = datagen_val.flow_from_directory(\"Jaffetrainvalidation/validation\",\n",
    "\n",
    "                                         target_size=(256, 256),\n",
    "\n",
    "                                         color_mode=\"rgb\",\n",
    "\n",
    "                                         batch_size=batch_size,\n",
    "\n",
    "                                         class_mode='categorical',\n",
    "\n",
    "                                         shuffle=False)\n",
    "\n",
    "print('Train and Validation sets have been created.')\n",
    "\n",
    "​\n",
    "\n",
    "​\n",
    "\n",
    "#model vgg19\n",
    "\n",
    "​\n",
    "\n",
    "from keras.applications.vgg19 import VGG19\n",
    "\n",
    "conv_base = VGG19(weights='imagenet', include_top=False, input_shape=(256, 256, 3))\n",
    "\n",
    "conv_base.summary()\n",
    "\n",
    "​\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(conv_base)\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(256,activation = 'relu'))\n",
    "\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dropout(0.30))\n",
    "\n",
    "# Second fully connected layer  \n",
    "\n",
    "model.add(Dense(512,activation = 'relu'))\n",
    "\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dropout(0.30))\n",
    "\n",
    "​\n",
    "\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "​\n",
    "\n",
    "# model.summary()\n",
    "\n",
    "# Visualize the model.\n",
    "\n",
    "#plot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True)\n",
    "\n",
    "​\n",
    "\n",
    "conv_base.trainable = False\n",
    "\n",
    "​\n",
    "\n",
    "model.compile(optimizer = Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "​\n",
    "\n",
    "print('CNN model has been created you can proceed to train you data with this model.')\n",
    "\n",
    "​\n",
    "\n",
    "​\n",
    "\n",
    "​\n",
    "\n",
    "​\n",
    "\n",
    "# Training the model\n",
    "\n",
    "epochs = 500\n",
    "\n",
    "​\n",
    "\n",
    "history = model.fit(x=train_ds,\n",
    "\n",
    "                    epochs=epochs,\n",
    "\n",
    "                    validation_data=test_ds)\n",
    "\n",
    "​\n",
    "\n",
    "# Print training history\n",
    "\n",
    "print(history.history)\n",
    "\n",
    "​\n",
    "\n",
    "# Plotting training history\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "​\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "\n",
    "plt.title('Loss')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "​\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "\n",
    "plt.title('Accuracy')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "Found 168 images belonging to 7 classes.\n",
    "Found 45 images belonging to 7 classes.\n",
    "Train and Validation sets have been created.\n",
    "Model: \"vgg19\"\n",
    "_________________________________________________________________\n",
    " Layer (type)                Output Shape              Param #   \n",
    "=================================================================\n",
    " input_3 (InputLayer)        [(None, 256, 256, 3)]     0         \n",
    "                                                                 \n",
    " block1_conv1 (Conv2D)       (None, 256, 256, 64)      1792      \n",
    "                                                                 \n",
    " block1_conv2 (Conv2D)       (None, 256, 256, 64)      36928     \n",
    "                                                                 \n",
    " block1_pool (MaxPooling2D)  (None, 128, 128, 64)      0         \n",
    "                                                                 \n",
    " block2_conv1 (Conv2D)       (None, 128, 128, 128)     73856     \n",
    "                                                                 \n",
    " block2_conv2 (Conv2D)       (None, 128, 128, 128)     147584    \n",
    "                                                                 \n",
    " block2_pool (MaxPooling2D)  (None, 64, 64, 128)       0         \n",
    "                                                                 \n",
    " block3_conv1 (Conv2D)       (None, 64, 64, 256)       295168    \n",
    "                                                                 \n",
    " block3_conv2 (Conv2D)       (None, 64, 64, 256)       590080    \n",
    "                                                                 \n",
    " block3_conv3 (Conv2D)       (None, 64, 64, 256)       590080    \n",
    "                                                                 \n",
    " block3_conv4 (Conv2D)       (None, 64, 64, 256)       590080    \n",
    "                                                                 \n",
    " block3_pool (MaxPooling2D)  (None, 32, 32, 256)       0         \n",
    "                                                                 \n",
    " block4_conv1 (Conv2D)       (None, 32, 32, 512)       1180160   \n",
    "                                                                 \n",
    " block4_conv2 (Conv2D)       (None, 32, 32, 512)       2359808   \n",
    "                                                                 \n",
    " block4_conv3 (Conv2D)       (None, 32, 32, 512)       2359808   \n",
    "                                                                 \n",
    " block4_conv4 (Conv2D)       (None, 32, 32, 512)       2359808   \n",
    "                                                                 \n",
    " block4_pool (MaxPooling2D)  (None, 16, 16, 512)       0         \n",
    "                                                                 \n",
    " block5_conv1 (Conv2D)       (None, 16, 16, 512)       2359808   \n",
    "                                                                 \n",
    " block5_conv2 (Conv2D)       (None, 16, 16, 512)       2359808   \n",
    "                                                                 \n",
    " block5_conv3 (Conv2D)       (None, 16, 16, 512)       2359808   \n",
    "                                                                 \n",
    " block5_conv4 (Conv2D)       (None, 16, 16, 512)       2359808   \n",
    "                                                                 \n",
    " block5_pool (MaxPooling2D)  (None, 8, 8, 512)         0         \n",
    "                                                                 \n",
    "=================================================================\n",
    "Total params: 20024384 (76.39 MB)\n",
    "Trainable params: 20024384 (76.39 MB)\n",
    "Non-trainable params: 0 (0.00 Byte)\n",
    "_________________________________________________________________\n",
    "CNN model has been created you can proceed to train you data with this model.\n",
    "Epoch 1/500\n",
    "3/3 [==============================] - 3s 561ms/step - loss: 2.8224 - accuracy: 0.1726 - val_loss: 4.7611 - val_accuracy: 0.2444\n",
    "Epoch 2/500\n",
    "3/3 [==============================] - 2s 517ms/step - loss: 2.3075 - accuracy: 0.3393 - val_loss: 5.7170 - val_accuracy: 0.2000\n",
    "Epoch 3/500\n",
    "3/3 [==============================] - 2s 520ms/step - loss: 1.9946 - accuracy: 0.2857 - val_loss: 7.9262 - val_accuracy: 0.1778\n",
    "Epoch 4/500\n",
    "3/3 [==============================] - 2s 505ms/step - loss: 1.8552 - accuracy: 0.3274 - val_loss: 10.1117 - val_accuracy: 0.1778\n",
    "Epoch 5/500\n",
    "3/3 [==============================] - 2s 512ms/step - loss: 1.8314 - accuracy: 0.3929 - val_loss: 12.1257 - val_accuracy: 0.1556\n",
    "Epoch 6/500\n",
    "3/3 [==============================] - 2s 518ms/step - loss: 2.0271 - accuracy: 0.3036 - val_loss: 12.2712 - val_accuracy: 0.2222\n",
    "Epoch 7/500\n",
    "3/3 [==============================] - 2s 514ms/step - loss: 1.6070 - accuracy: 0.4107 - val_loss: 12.1107 - val_accuracy: 0.1556\n",
    "Epoch 8/500\n",
    "3/3 [==============================] - 2s 497ms/step - loss: 1.8270 - accuracy: 0.3929 - val_loss: 12.1840 - val_accuracy: 0.1333\n",
    "Epoch 9/500\n",
    "3/3 [==============================] - 2s 504ms/step - loss: 1.5394 - accuracy: 0.4345 - val_loss: 11.9665 - val_accuracy: 0.1333\n",
    "Epoch 10/500\n",
    "3/3 [==============================] - 2s 639ms/step - loss: 1.5558 - accuracy: 0.4226 - val_loss: 11.5705 - val_accuracy: 0.1333\n",
    "Epoch 11/500\n",
    "3/3 [==============================] - 2s 644ms/step - loss: 1.4093 - accuracy: 0.4881 - val_loss: 11.0497 - val_accuracy: 0.1556\n",
    "Epoch 12/500\n",
    "3/3 [==============================] - 2s 501ms/step - loss: 1.3383 - accuracy: 0.5179 - val_loss: 11.1158 - val_accuracy: 0.1778\n",
    "Epoch 13/500\n",
    "3/3 [==============================] - 2s 505ms/step - loss: 1.2347 - accuracy: 0.5357 - val_loss: 11.4968 - val_accuracy: 0.1556\n",
    "Epoch 14/500\n",
    "3/3 [==============================] - 2s 515ms/step - loss: 1.2084 - accuracy: 0.5774 - val_loss: 11.8275 - val_accuracy: 0.1333\n",
    "Epoch 15/500\n",
    "3/3 [==============================] - 2s 514ms/step - loss: 1.1639 - accuracy: 0.5655 - val_loss: 11.8982 - val_accuracy: 0.1333\n",
    "Epoch 16/500\n",
    "3/3 [==============================] - 2s 502ms/step - loss: 1.3439 - accuracy: 0.5238 - val_loss: 11.8728 - val_accuracy: 0.1333\n",
    "Epoch 17/500\n",
    "3/3 [==============================] - 2s 656ms/step - loss: 1.1308 - accuracy: 0.5774 - val_loss: 11.7873 - val_accuracy: 0.1333\n",
    "Epoch 18/500\n",
    "3/3 [==============================] - 2s 504ms/step - loss: 1.0882 - accuracy: 0.6369 - val_loss: 10.8697 - val_accuracy: 0.1333\n",
    "Epoch 19/500\n",
    "3/3 [==============================] - 2s 515ms/step - loss: 0.8120 - accuracy: 0.7083 - val_loss: 10.0630 - val_accuracy: 0.1556\n",
    "Epoch 20/500\n",
    "3/3 [==============================] - 2s 505ms/step - loss: 0.9574 - accuracy: 0.6548 - val_loss: 9.4871 - val_accuracy: 0.1778\n",
    "Epoch 21/500\n",
    "3/3 [==============================] - 2s 506ms/step - loss: 0.8671 - accuracy: 0.6726 - val_loss: 9.5549 - val_accuracy: 0.1778\n",
    "Epoch 22/500\n",
    "3/3 [==============================] - 2s 515ms/step - loss: 0.9490 - accuracy: 0.6369 - val_loss: 11.4748 - val_accuracy: 0.1556\n",
    "Epoch 23/500\n",
    "3/3 [==============================] - 2s 513ms/step - loss: 0.7988 - accuracy: 0.6726 - val_loss: 12.3740 - val_accuracy: 0.1333\n",
    "Epoch 24/500\n",
    "3/3 [==============================] - 2s 644ms/step - loss: 0.8560 - accuracy: 0.7024 - val_loss: 12.5525 - val_accuracy: 0.1333\n",
    "Epoch 25/500\n",
    "3/3 [==============================] - 2s 643ms/step - loss: 0.7381 - accuracy: 0.6964 - val_loss: 11.3656 - val_accuracy: 0.1556\n",
    "Epoch 26/500\n",
    "3/3 [==============================] - 2s 653ms/step - loss: 0.7872 - accuracy: 0.7143 - val_loss: 9.8563 - val_accuracy: 0.1556\n",
    "Epoch 27/500\n",
    "3/3 [==============================] - 2s 648ms/step - loss: 0.6460 - accuracy: 0.7857 - val_loss: 8.9852 - val_accuracy: 0.2222\n",
    "Epoch 28/500\n",
    "3/3 [==============================] - 2s 511ms/step - loss: 0.7752 - accuracy: 0.7024 - val_loss: 8.2793 - val_accuracy: 0.2667\n",
    "Epoch 29/500\n",
    "3/3 [==============================] - 2s 516ms/step - loss: 0.7371 - accuracy: 0.7143 - val_loss: 7.5918 - val_accuracy: 0.2222\n",
    "Epoch 30/500\n",
    "3/3 [==============================] - 2s 502ms/step - loss: 0.6784 - accuracy: 0.7024 - val_loss: 7.3793 - val_accuracy: 0.2667\n",
    "Epoch 31/500\n",
    "3/3 [==============================] - 2s 498ms/step - loss: 0.6030 - accuracy: 0.8036 - val_loss: 7.1998 - val_accuracy: 0.2667\n",
    "Epoch 32/500\n",
    "3/3 [==============================] - 2s 518ms/step - loss: 0.6758 - accuracy: 0.7143 - val_loss: 6.6317 - val_accuracy: 0.2667\n",
    "Epoch 33/500\n",
    "\n",
    "3/3 [==============================] - 2s 499ms/step - loss: 0.5511 - accuracy: 0.8036 - val_loss: 5.9684 - val_accuracy: 0.2667\n",
    "Epoch 34/500\n",
    "3/3 [==============================] - 2s 518ms/step - loss: 0.6516 - accuracy: 0.7500 - val_loss: 5.4746 - val_accuracy: 0.2667\n",
    "Epoch 35/500\n",
    "3/3 [==============================] - 2s 501ms/step - loss: 0.5127 - accuracy: 0.8274 - val_loss: 5.2741 - val_accuracy: 0.2889\n",
    "Epoch 36/500\n",
    "3/3 [==============================] - 2s 640ms/step - loss: 0.5399 - accuracy: 0.7917 - val_loss: 4.9151 - val_accuracy: 0.2889\n",
    "Epoch 37/500\n",
    "3/3 [==============================] - 2s 515ms/step - loss: 0.5875 - accuracy: 0.7917 - val_loss: 4.6620 - val_accuracy: 0.2667\n",
    "Epoch 38/500\n",
    "3/3 [==============================] - 2s 653ms/step - loss: 0.4652 - accuracy: 0.8274 - val_loss: 4.5982 - val_accuracy: 0.2667\n",
    "Epoch 39/500\n",
    "3/3 [==============================] - 2s 504ms/step - loss: 0.3968 - accuracy: 0.8571 - val_loss: 4.4803 - val_accuracy: 0.2444\n",
    "Epoch 40/500\n",
    "3/3 [==============================] - 2s 524ms/step - loss: 0.5165 - accuracy: 0.8393 - val_loss: 4.5227 - val_accuracy: 0.2222\n",
    "Epoch 41/500\n",
    "3/3 [==============================] - 2s 500ms/step - loss: 0.3917 - accuracy: 0.8393 - val_loss: 4.6347 - val_accuracy: 0.2667\n",
    "Epoch 42/500\n",
    "3/3 [==============================] - 2s 654ms/step - loss: 0.3386 - accuracy: 0.8690 - val_loss: 4.5441 - val_accuracy: 0.2889\n",
    "Epoch 43/500\n",
    "3/3 [==============================] - 2s 514ms/step - loss: 0.3552 - accuracy: 0.8869 - val_loss: 4.5349 - val_accuracy: 0.2889\n",
    "Epoch 44/500\n",
    "3/3 [==============================] - 2s 500ms/step - loss: 0.4558 - accuracy: 0.8512 - val_loss: 4.2854 - val_accuracy: 0.2889\n",
    "Epoch 45/500\n",
    "3/3 [==============================] - 2s 511ms/step - loss: 0.5934 - accuracy: 0.7917 - val_loss: 3.7940 - val_accuracy: 0.2889\n",
    "Epoch 46/500\n",
    "3/3 [==============================] - 2s 503ms/step - loss: 0.3779 - accuracy: 0.8512 - val_loss: 3.4934 - val_accuracy: 0.3111\n",
    "Epoch 47/500\n",
    "3/3 [==============================] - 2s 507ms/step - loss: 0.3486 - accuracy: 0.8750 - val_loss: 3.3432 - val_accuracy: 0.3111\n",
    "Epoch 48/500\n",
    "3/3 [==============================] - 2s 655ms/step - loss: 0.4599 - accuracy: 0.8155 - val_loss: 3.4153 - val_accuracy: 0.3556\n",
    "Epoch 49/500\n",
    "3/3 [==============================] - 2s 654ms/step - loss: 0.4008 - accuracy: 0.8512 - val_loss: 3.6979 - val_accuracy: 0.2889\n",
    "Epoch 50/500\n",
    "3/3 [==============================] - 2s 525ms/step - loss: 0.3926 - accuracy: 0.8571 - val_loss: 3.9372 - val_accuracy: 0.2889\n",
    "Epoch 51/500\n",
    "3/3 [==============================] - 2s 502ms/step - loss: 0.2824 - accuracy: 0.8929 - val_loss: 3.9974 - val_accuracy: 0.3111\n",
    "Epoch 52/500\n",
    "3/3 [==============================] - 2s 520ms/step - loss: 0.3739 - accuracy: 0.8571 - val_loss: 4.1952 - val_accuracy: 0.2889\n",
    "Epoch 53/500\n",
    "3/3 [==============================] - 2s 508ms/step - loss: 0.3198 - accuracy: 0.8750 - val_loss: 4.1401 - val_accuracy: 0.3111\n",
    "Epoch 54/500\n",
    "3/3 [==============================] - 2s 516ms/step - loss: 0.2595 - accuracy: 0.9048 - val_loss: 4.0859 - val_accuracy: 0.3111\n",
    "Epoch 55/500\n",
    "3/3 [==============================] - 2s 509ms/step - loss: 0.3937 - accuracy: 0.8750 - val_loss: 3.6433 - val_accuracy: 0.3778\n",
    "Epoch 56/500\n",
    "3/3 [==============================] - 2s 522ms/step - loss: 0.2809 - accuracy: 0.8810 - val_loss: 2.8883 - val_accuracy: 0.4222\n",
    "Epoch 57/500\n",
    "3/3 [==============================] - 2s 644ms/step - loss: 0.5070 - accuracy: 0.8214 - val_loss: 2.2377 - val_accuracy: 0.5556\n",
    "Epoch 58/500\n",
    "3/3 [==============================] - 2s 655ms/step - loss: 0.4850 - accuracy: 0.8333 - val_loss: 1.9370 - val_accuracy: 0.5333\n",
    "Epoch 59/500\n",
    "3/3 [==============================] - 2s 660ms/step - loss: 0.3659 - accuracy: 0.8571 - val_loss: 1.8029 - val_accuracy: 0.5556\n",
    "Epoch 60/500\n",
    "3/3 [==============================] - 2s 504ms/step - loss: 0.4087 - accuracy: 0.8750 - val_loss: 1.7609 - val_accuracy: 0.5111\n",
    "Epoch 61/500\n",
    "3/3 [==============================] - 2s 504ms/step - loss: 0.2984 - accuracy: 0.8929 - val_loss: 1.7220 - val_accuracy: 0.4889\n",
    "Epoch 62/500\n",
    "3/3 [==============================] - 2s 655ms/step - loss: 0.3539 - accuracy: 0.8452 - val_loss: 1.8135 - val_accuracy: 0.4667\n",
    "Epoch 63/500\n",
    "3/3 [==============================] - 2s 648ms/step - loss: 0.2553 - accuracy: 0.9107 - val_loss: 1.9209 - val_accuracy: 0.4889\n",
    "Epoch 64/500\n",
    "3/3 [==============================] - 2s 533ms/step - loss: 0.3399 - accuracy: 0.8690 - val_loss: 1.9767 - val_accuracy: 0.4667\n",
    "Epoch 65/500\n",
    "3/3 [==============================] - 2s 659ms/step - loss: 0.3561 - accuracy: 0.8750 - val_loss: 2.1191 - val_accuracy: 0.4667\n",
    "Epoch 66/500\n",
    "3/3 [==============================] - 2s 515ms/step - loss: 0.3394 - accuracy: 0.8988 - val_loss: 2.2096 - val_accuracy: 0.5111\n",
    "Epoch 67/500\n",
    "3/3 [==============================] - 2s 520ms/step - loss: 0.3282 - accuracy: 0.8631 - val_loss: 2.3346 - val_accuracy: 0.4444\n",
    "Epoch 68/500\n",
    "3/3 [==============================] - 2s 649ms/step - loss: 0.3722 - accuracy: 0.8631 - val_loss: 6.9871 - val_accuracy: 0.1556\n",
    "Epoch 69/500\n",
    "3/3 [==============================] - 2s 501ms/step - loss: 0.2879 - accuracy: 0.8869 - val_loss: 12.1476 - val_accuracy: 0.1333\n",
    "Epoch 70/500\n",
    "3/3 [==============================] - 2s 512ms/step - loss: 0.3616 - accuracy: 0.8333 - val_loss: 8.8943 - val_accuracy: 0.1778\n",
    "Epoch 71/500\n",
    "3/3 [==============================] - 2s 660ms/step - loss: 0.3695 - accuracy: 0.8512 - val_loss: 5.4242 - val_accuracy: 0.2667\n",
    "Epoch 72/500\n",
    "3/3 [==============================] - 2s 506ms/step - loss: 0.2912 - accuracy: 0.9107 - val_loss: 3.4127 - val_accuracy: 0.3556\n",
    "Epoch 73/500\n",
    "3/3 [==============================] - 2s 514ms/step - loss: 0.2935 - accuracy: 0.9107 - val_loss: 2.6249 - val_accuracy: 0.4444\n",
    "Epoch 74/500\n",
    "3/3 [==============================] - 2s 509ms/step - loss: 0.3372 - accuracy: 0.9048 - val_loss: 2.0145 - val_accuracy: 0.4667\n",
    "Epoch 75/500\n",
    "3/3 [==============================] - 2s 653ms/step - loss: 0.3578 - accuracy: 0.8810 - val_loss: 1.4673 - val_accuracy: 0.6222\n",
    "Epoch 76/500\n",
    "3/3 [==============================] - 2s 646ms/step - loss: 0.2602 - accuracy: 0.9226 - val_loss: 1.2092 - val_accuracy: 0.6667\n",
    "Epoch 77/500\n",
    "3/3 [==============================] - 2s 499ms/step - loss: 0.2584 - accuracy: 0.9048 - val_loss: 1.0820 - val_accuracy: 0.7111\n",
    "Epoch 78/500\n",
    "3/3 [==============================] - 2s 511ms/step - loss: 0.2116 - accuracy: 0.9226 - val_loss: 0.9912 - val_accuracy: 0.7333\n",
    "Epoch 79/500\n",
    "3/3 [==============================] - 2s 499ms/step - loss: 0.2878 - accuracy: 0.8869 - val_loss: 0.9617 - val_accuracy: 0.7333\n",
    "Epoch 80/500\n",
    "3/3 [==============================] - 2s 650ms/step - loss: 0.3300 - accuracy: 0.8690 - val_loss: 1.0294 - val_accuracy: 0.7111\n",
    "Epoch 81/500\n",
    "3/3 [==============================] - 2s 514ms/step - loss: 0.2446 - accuracy: 0.9226 - val_loss: 3.4067 - val_accuracy: 0.5333\n",
    "Epoch 82/500\n",
    "3/3 [==============================] - 2s 655ms/step - loss: 0.2551 - accuracy: 0.8988 - val_loss: 3.6351 - val_accuracy: 0.5333\n",
    "Epoch 83/500\n",
    "3/3 [==============================] - 2s 516ms/step - loss: 0.2195 - accuracy: 0.9107 - val_loss: 3.0908 - val_accuracy: 0.6000\n",
    "Epoch 84/500\n",
    "3/3 [==============================] - 2s 525ms/step - loss: 0.2275 - accuracy: 0.9048 - val_loss: 2.7384 - val_accuracy: 0.6222\n",
    "Epoch 85/500\n",
    "3/3 [==============================] - 2s 517ms/step - loss: 0.1839 - accuracy: 0.9167 - val_loss: 2.4600 - val_accuracy: 0.6000\n",
    "Epoch 86/500\n",
    "3/3 [==============================] - 2s 498ms/step - loss: 0.2504 - accuracy: 0.9048 - val_loss: 2.1309 - val_accuracy: 0.6444\n",
    "Epoch 87/500\n",
    "3/3 [==============================] - 2s 519ms/step - loss: 0.2711 - accuracy: 0.8929 - val_loss: 1.8073 - val_accuracy: 0.6222\n",
    "Epoch 88/500\n",
    "3/3 [==============================] - 2s 513ms/step - loss: 0.1884 - accuracy: 0.9405 - val_loss: 1.6461 - val_accuracy: 0.5778\n",
    "Epoch 89/500\n",
    "3/3 [==============================] - 2s 647ms/step - loss: 0.1913 - accuracy: 0.9226 - val_loss: 1.4652 - val_accuracy: 0.6000\n",
    "Epoch 90/500\n",
    "3/3 [==============================] - 2s 497ms/step - loss: 0.2072 - accuracy: 0.9226 - val_loss: 1.2968 - val_accuracy: 0.6667\n",
    "\n",
    "Epoch 91/500\n",
    "3/3 [==============================] - 2s 505ms/step - loss: 0.2675 - accuracy: 0.8988 - val_loss: 1.0589 - val_accuracy: 0.7111\n",
    "Epoch 92/500\n",
    "3/3 [==============================] - 2s 493ms/step - loss: 0.3151 - accuracy: 0.8750 - val_loss: 0.8410 - val_accuracy: 0.7333\n",
    "Epoch 93/500\n",
    "3/3 [==============================] - 2s 658ms/step - loss: 0.2364 - accuracy: 0.9405 - val_loss: 0.7694 - val_accuracy: 0.7778\n",
    "Epoch 94/500\n",
    "3/3 [==============================] - 2s 511ms/step - loss: 0.2337 - accuracy: 0.9167 - val_loss: 0.7666 - val_accuracy: 0.7778\n",
    "Epoch 95/500\n",
    "3/3 [==============================] - 2s 654ms/step - loss: 0.2865 - accuracy: 0.9107 - val_loss: 0.9215 - val_accuracy: 0.7556\n",
    "Epoch 96/500\n",
    "3/3 [==============================] - 2s 651ms/step - loss: 0.3207 - accuracy: 0.9107 - val_loss: 1.0357 - val_accuracy: 0.7333\n",
    "Epoch 97/500\n",
    "3/3 [==============================] - 2s 649ms/step - loss: 0.1991 - accuracy: 0.9167 - val_loss: 0.9717 - val_accuracy: 0.7778\n",
    "Epoch 98/500\n",
    "3/3 [==============================] - 2s 496ms/step - loss: 0.1759 - accuracy: 0.9345 - val_loss: 0.8708 - val_accuracy: 0.8222\n",
    "Epoch 99/500\n",
    "3/3 [==============================] - 2s 504ms/step - loss: 0.1493 - accuracy: 0.9345 - val_loss: 0.8077 - val_accuracy: 0.8222\n",
    "Epoch 100/500\n",
    "3/3 [==============================] - 2s 512ms/step - loss: 0.2006 - accuracy: 0.9167 - val_loss: 0.7567 - val_accuracy: 0.8222\n",
    "Epoch 101/500\n",
    "3/3 [==============================] - 2s 650ms/step - loss: 0.1438 - accuracy: 0.9464 - val_loss: 0.7149 - val_accuracy: 0.8444\n",
    "Epoch 102/500\n",
    "3/3 [==============================] - 2s 514ms/step - loss: 0.1974 - accuracy: 0.9167 - val_loss: 0.7378 - val_accuracy: 0.8444\n",
    "Epoch 103/500\n",
    "3/3 [==============================] - 2s 515ms/step - loss: 0.1922 - accuracy: 0.9226 - val_loss: 0.8151 - val_accuracy: 0.8222\n",
    "Epoch 104/500\n",
    "3/3 [==============================] - 2s 650ms/step - loss: 0.1877 - accuracy: 0.9345 - val_loss: 0.8900 - val_accuracy: 0.7778\n",
    "Epoch 105/500\n",
    "3/3 [==============================] - 2s 494ms/step - loss: 0.3096 - accuracy: 0.8929 - val_loss: 1.0466 - val_accuracy: 0.7556\n",
    "Epoch 106/500\n",
    "3/3 [==============================] - 2s 517ms/step - loss: 0.2231 - accuracy: 0.9107 - val_loss: 1.2527 - val_accuracy: 0.7778\n",
    "Epoch 107/500\n",
    "3/3 [==============================] - 2s 658ms/step - loss: 0.2781 - accuracy: 0.8988 - val_loss: 1.3261 - val_accuracy: 0.7778\n",
    "Epoch 108/500\n",
    "3/3 [==============================] - 2s 501ms/step - loss: 0.1417 - accuracy: 0.9464 - val_loss: 1.2212 - val_accuracy: 0.8000\n",
    "Epoch 109/500\n",
    "3/3 [==============================] - 2s 526ms/step - loss: 0.2487 - accuracy: 0.9167 - val_loss: 0.9812 - val_accuracy: 0.8222\n",
    "Epoch 110/500\n",
    "3/3 [==============================] - 2s 648ms/step - loss: 0.2694 - accuracy: 0.9107 - val_loss: 0.8249 - val_accuracy: 0.8889\n",
    "Epoch 111/500\n",
    "3/3 [==============================] - 2s 516ms/step - loss: 0.1850 - accuracy: 0.9286 - val_loss: 0.7458 - val_accuracy: 0.8889\n",
    "Epoch 112/500\n",
    "3/3 [==============================] - 2s 499ms/step - loss: 0.2690 - accuracy: 0.9048 - val_loss: 0.7164 - val_accuracy: 0.8667\n",
    "Epoch 113/500\n",
    "3/3 [==============================] - 2s 648ms/step - loss: 0.1380 - accuracy: 0.9643 - val_loss: 0.7531 - val_accuracy: 0.8667\n",
    "Epoch 114/500\n",
    "3/3 [==============================] - 2s 521ms/step - loss: 0.1422 - accuracy: 0.9464 - val_loss: 0.8378 - val_accuracy: 0.8667\n",
    "Epoch 115/500\n",
    "3/3 [==============================] - 2s 509ms/step - loss: 0.2040 - accuracy: 0.9048 - val_loss: 0.9434 - val_accuracy: 0.8444\n",
    "Epoch 116/500\n",
    "3/3 [==============================] - 2s 503ms/step - loss: 0.1540 - accuracy: 0.9583 - val_loss: 1.0400 - val_accuracy: 0.8000\n",
    "Epoch 117/500\n",
    "3/3 [==============================] - 2s 521ms/step - loss: 0.2161 - accuracy: 0.9167 - val_loss: 0.9726 - val_accuracy: 0.8444\n",
    "Epoch 118/500\n",
    "3/3 [==============================] - 2s 645ms/step - loss: 0.1697 - accuracy: 0.9583 - val_loss: 0.9255 - val_accuracy: 0.8222\n",
    "Epoch 119/500\n",
    "3/3 [==============================] - 2s 507ms/step - loss: 0.1732 - accuracy: 0.9226 - val_loss: 0.9195 - val_accuracy: 0.8222\n",
    "Epoch 120/500\n",
    "3/3 [==============================] - 2s 517ms/step - loss: 0.1643 - accuracy: 0.9405 - val_loss: 0.8936 - val_accuracy: 0.8222\n",
    "Epoch 121/500\n",
    "3/3 [==============================] - 2s 495ms/step - loss: 0.1545 - accuracy: 0.9226 - val_loss: 0.9192 - val_accuracy: 0.8222\n",
    "Epoch 122/500\n",
    "3/3 [==============================] - 2s 507ms/step - loss: 0.1772 - accuracy: 0.9405 - val_loss: 0.9800 - val_accuracy: 0.8222\n",
    "Epoch 123/500\n",
    "3/3 [==============================] - 2s 645ms/step - loss: 0.1832 - accuracy: 0.9167 - val_loss: 0.9787 - val_accuracy: 0.8000\n",
    "Epoch 124/500\n",
    "3/3 [==============================] - 2s 654ms/step - loss: 0.2370 - accuracy: 0.9345 - val_loss: 4.4093 - val_accuracy: 0.4667\n",
    "Epoch 125/500\n",
    "3/3 [==============================] - 2s 652ms/step - loss: 0.1829 - accuracy: 0.9464 - val_loss: 8.1502 - val_accuracy: 0.4222\n",
    "Epoch 126/500\n",
    "3/3 [==============================] - 2s 660ms/step - loss: 0.1131 - accuracy: 0.9583 - val_loss: 2.7161 - val_accuracy: 0.5556\n",
    "Epoch 127/500\n",
    "3/3 [==============================] - 2s 513ms/step - loss: 0.1300 - accuracy: 0.9524 - val_loss: 5.2292 - val_accuracy: 0.5111\n",
    "Epoch 128/500\n",
    "3/3 [==============================] - 2s 504ms/step - loss: 0.1858 - accuracy: 0.9405 - val_loss: 6.4363 - val_accuracy: 0.3778\n",
    "Epoch 129/500\n",
    "3/3 [==============================] - 2s 509ms/step - loss: 0.1699 - accuracy: 0.9286 - val_loss: 2.5184 - val_accuracy: 0.6444\n",
    "Epoch 130/500\n",
    "3/3 [==============================] - 2s 500ms/step - loss: 0.1922 - accuracy: 0.9464 - val_loss: 2.2690 - val_accuracy: 0.6222\n",
    "Epoch 131/500\n",
    "3/3 [==============================] - 2s 498ms/step - loss: 0.0983 - accuracy: 0.9702 - val_loss: 2.1305 - val_accuracy: 0.6444\n",
    "Epoch 132/500\n",
    "3/3 [==============================] - 2s 641ms/step - loss: 0.1748 - accuracy: 0.9524 - val_loss: 2.2447 - val_accuracy: 0.6667\n",
    "Epoch 133/500\n",
    "3/3 [==============================] - 2s 649ms/step - loss: 0.1960 - accuracy: 0.9405 - val_loss: 2.3275 - val_accuracy: 0.6667\n",
    "Epoch 134/500\n",
    "3/3 [==============================] - 2s 502ms/step - loss: 0.1563 - accuracy: 0.9405 - val_loss: 2.4279 - val_accuracy: 0.6444\n",
    "Epoch 135/500\n",
    "3/3 [==============================] - 2s 659ms/step - loss: 0.2254 - accuracy: 0.9286 - val_loss: 2.2866 - val_accuracy: 0.6444\n",
    "Epoch 136/500\n",
    "3/3 [==============================] - 2s 494ms/step - loss: 0.1738 - accuracy: 0.9524 - val_loss: 1.9334 - val_accuracy: 0.7333\n",
    "Epoch 137/500\n",
    "3/3 [==============================] - 2s 506ms/step - loss: 0.1409 - accuracy: 0.9405 - val_loss: 1.5240 - val_accuracy: 0.7556\n",
    "Epoch 138/500\n",
    "3/3 [==============================] - 2s 510ms/step - loss: 0.1418 - accuracy: 0.9464 - val_loss: 1.3269 - val_accuracy: 0.7778\n",
    "Epoch 139/500\n",
    "3/3 [==============================] - 2s 507ms/step - loss: 0.1270 - accuracy: 0.9405 - val_loss: 1.1958 - val_accuracy: 0.7778\n",
    "Epoch 140/500\n",
    "3/3 [==============================] - 2s 525ms/step - loss: 0.1639 - accuracy: 0.9345 - val_loss: 1.0286 - val_accuracy: 0.8222\n",
    "Epoch 141/500\n",
    "3/3 [==============================] - 2s 505ms/step - loss: 0.2031 - accuracy: 0.9286 - val_loss: 0.8766 - val_accuracy: 0.8667\n",
    "Epoch 142/500\n",
    "3/3 [==============================] - 2s 508ms/step - loss: 0.1542 - accuracy: 0.9345 - val_loss: 0.8059 - val_accuracy: 0.8667\n",
    "Epoch 143/500\n",
    "3/3 [==============================] - 2s 511ms/step - loss: 0.1462 - accuracy: 0.9345 - val_loss: 0.8436 - val_accuracy: 0.8444\n",
    "Epoch 144/500\n",
    "3/3 [==============================] - 2s 495ms/step - loss: 0.1795 - accuracy: 0.9524 - val_loss: 0.9148 - val_accuracy: 0.8667\n",
    "Epoch 145/500\n",
    "3/3 [==============================] - 2s 520ms/step - loss: 0.2123 - accuracy: 0.9167 - val_loss: 0.9989 - val_accuracy: 0.8444\n",
    "Epoch 146/500\n",
    "3/3 [==============================] - 2s 648ms/step - loss: 0.1904 - accuracy: 0.9464 - val_loss: 1.0179 - val_accuracy: 0.8444\n",
    "Epoch 147/500\n",
    "3/3 [==============================] - 2s 501ms/step - loss: 0.1849 - accuracy: 0.9286 - val_loss: 1.0084 - val_accuracy: 0.8222\n",
    "Epoch 148/500\n",
    "\n",
    "3/3 [==============================] - 2s 493ms/step - loss: 0.1528 - accuracy: 0.9524 - val_loss: 1.0308 - val_accuracy: 0.7778\n",
    "Epoch 149/500\n",
    "3/3 [==============================] - 2s 646ms/step - loss: 0.2038 - accuracy: 0.9286 - val_loss: 0.9637 - val_accuracy: 0.8222\n",
    "Epoch 150/500\n",
    "3/3 [==============================] - 2s 493ms/step - loss: 0.1786 - accuracy: 0.9464 - val_loss: 0.8904 - val_accuracy: 0.8222\n",
    "Epoch 151/500\n",
    "3/3 [==============================] - 2s 653ms/step - loss: 0.2023 - accuracy: 0.9226 - val_loss: 0.8902 - val_accuracy: 0.8444\n",
    "Epoch 152/500\n",
    "3/3 [==============================] - 2s 517ms/step - loss: 0.1189 - accuracy: 0.9762 - val_loss: 0.9542 - val_accuracy: 0.8222\n",
    "Epoch 153/500\n",
    "3/3 [==============================] - 2s 516ms/step - loss: 0.1631 - accuracy: 0.9405 - val_loss: 0.9494 - val_accuracy: 0.8444\n",
    "Epoch 154/500\n",
    "3/3 [==============================] - 2s 643ms/step - loss: 0.2219 - accuracy: 0.9286 - val_loss: 0.7944 - val_accuracy: 0.8889\n",
    "Epoch 155/500\n",
    "3/3 [==============================] - 2s 506ms/step - loss: 0.1231 - accuracy: 0.9464 - val_loss: 0.6978 - val_accuracy: 0.8667\n",
    "Epoch 156/500\n",
    "3/3 [==============================] - 2s 514ms/step - loss: 0.1553 - accuracy: 0.9583 - val_loss: 0.6997 - val_accuracy: 0.8667\n",
    "Epoch 157/500\n",
    "3/3 [==============================] - 2s 522ms/step - loss: 0.0907 - accuracy: 0.9583 - val_loss: 0.7195 - val_accuracy: 0.8667\n",
    "Epoch 158/500\n",
    "3/3 [==============================] - 2s 511ms/step - loss: 0.1240 - accuracy: 0.9405 - val_loss: 0.7679 - val_accuracy: 0.8667\n",
    "Epoch 159/500\n",
    "3/3 [==============================] - 2s 520ms/step - loss: 0.1482 - accuracy: 0.9464 - val_loss: 0.8262 - val_accuracy: 0.8667\n",
    "Epoch 160/500\n",
    "3/3 [==============================] - 2s 499ms/step - loss: 0.1783 - accuracy: 0.9405 - val_loss: 0.8632 - val_accuracy: 0.8667\n",
    "Epoch 161/500\n",
    "3/3 [==============================] - 2s 649ms/step - loss: 0.1249 - accuracy: 0.9464 - val_loss: 0.7889 - val_accuracy: 0.8667\n",
    "Epoch 162/500\n",
    "3/3 [==============================] - 2s 657ms/step - loss: 0.1168 - accuracy: 0.9583 - val_loss: 0.7127 - val_accuracy: 0.8889\n",
    "Epoch 163/500\n",
    "3/3 [==============================] - 2s 646ms/step - loss: 0.1022 - accuracy: 0.9524 - val_loss: 0.6250 - val_accuracy: 0.8889\n",
    "Epoch 164/500\n",
    "3/3 [==============================] - 2s 514ms/step - loss: 0.0928 - accuracy: 0.9643 - val_loss: 0.5442 - val_accuracy: 0.9111\n",
    "Epoch 165/500\n",
    "3/3 [==============================] - 2s 654ms/step - loss: 0.1594 - accuracy: 0.9405 - val_loss: 0.5494 - val_accuracy: 0.9111\n",
    "Epoch 166/500\n",
    "3/3 [==============================] - 2s 503ms/step - loss: 0.0926 - accuracy: 0.9702 - val_loss: 0.5923 - val_accuracy: 0.8889\n",
    "Epoch 167/500\n",
    "3/3 [==============================] - 2s 509ms/step - loss: 0.1366 - accuracy: 0.9524 - val_loss: 0.6605 - val_accuracy: 0.8667\n",
    "Epoch 168/500\n",
    "3/3 [==============================] - 2s 503ms/step - loss: 0.1656 - accuracy: 0.9524 - val_loss: 0.6795 - val_accuracy: 0.8667\n",
    "Epoch 169/500\n",
    "3/3 [==============================] - 2s 520ms/step - loss: 0.1730 - accuracy: 0.9702 - val_loss: 0.6689 - val_accuracy: 0.8667\n",
    "Epoch 170/500\n",
    "3/3 [==============================] - 2s 528ms/step - loss: 0.1641 - accuracy: 0.9405 - val_loss: 0.6572 - val_accuracy: 0.8667\n",
    "Epoch 171/500\n",
    "3/3 [==============================] - 2s 534ms/step - loss: 0.1151 - accuracy: 0.9643 - val_loss: 0.6090 - val_accuracy: 0.8667\n",
    "Epoch 172/500\n",
    "3/3 [==============================] - 2s 509ms/step - loss: 0.1479 - accuracy: 0.9405 - val_loss: 0.6278 - val_accuracy: 0.8667\n",
    "Epoch 173/500\n",
    "3/3 [==============================] - 2s 508ms/step - loss: 0.0622 - accuracy: 0.9940 - val_loss: 0.6715 - val_accuracy: 0.8667\n",
    "Epoch 174/500\n",
    "3/3 [==============================] - 2s 526ms/step - loss: 0.1942 - accuracy: 0.9286 - val_loss: 0.7349 - val_accuracy: 0.8889\n",
    "Epoch 175/500\n",
    "3/3 [==============================] - 2s 539ms/step - loss: 0.2143 - accuracy: 0.9405 - val_loss: 0.8008 - val_accuracy: 0.8667\n",
    "Epoch 176/500\n",
    "3/3 [==============================] - 2s 659ms/step - loss: 0.1317 - accuracy: 0.9286 - val_loss: 0.8602 - val_accuracy: 0.8444\n",
    "Epoch 177/500\n",
    "3/3 [==============================] - 2s 563ms/step - loss: 0.0749 - accuracy: 0.9762 - val_loss: 0.9425 - val_accuracy: 0.8444\n",
    "Epoch 178/500\n",
    "3/3 [==============================] - 2s 521ms/step - loss: 0.1204 - accuracy: 0.9583 - val_loss: 1.0015 - val_accuracy: 0.8222\n",
    "Epoch 179/500\n",
    "3/3 [==============================] - 2s 507ms/step - loss: 0.0831 - accuracy: 0.9643 - val_loss: 1.0493 - val_accuracy: 0.8222\n",
    "Epoch 180/500\n",
    "3/3 [==============================] - 2s 521ms/step - loss: 0.1525 - accuracy: 0.9643 - val_loss: 1.1439 - val_accuracy: 0.7778\n",
    "Epoch 181/500\n",
    "3/3 [==============================] - 2s 509ms/step - loss: 0.1378 - accuracy: 0.9345 - val_loss: 1.1746 - val_accuracy: 0.7778\n",
    "Epoch 182/500\n",
    "3/3 [==============================] - 2s 653ms/step - loss: 0.1015 - accuracy: 0.9702 - val_loss: 1.0580 - val_accuracy: 0.8222\n",
    "Epoch 183/500\n",
    "3/3 [==============================] - 2s 502ms/step - loss: 0.1117 - accuracy: 0.9583 - val_loss: 1.0158 - val_accuracy: 0.8444\n",
    "Epoch 184/500\n",
    "3/3 [==============================] - 2s 503ms/step - loss: 0.1789 - accuracy: 0.9345 - val_loss: 0.9910 - val_accuracy: 0.8667\n",
    "Epoch 185/500\n",
    "3/3 [==============================] - 2s 507ms/step - loss: 0.0731 - accuracy: 0.9821 - val_loss: 1.0698 - val_accuracy: 0.8444\n",
    "Epoch 186/500\n",
    "3/3 [==============================] - 2s 506ms/step - loss: 0.1602 - accuracy: 0.9345 - val_loss: 1.0958 - val_accuracy: 0.8444\n",
    "Epoch 187/500\n",
    "3/3 [==============================] - 2s 501ms/step - loss: 0.1261 - accuracy: 0.9643 - val_loss: 1.0380 - val_accuracy: 0.8222\n",
    "Epoch 188/500\n",
    "3/3 [==============================] - 2s 519ms/step - loss: 0.1075 - accuracy: 0.9643 - val_loss: 1.0210 - val_accuracy: 0.8667\n",
    "Epoch 189/500\n",
    "3/3 [==============================] - 2s 655ms/step - loss: 0.1649 - accuracy: 0.9405 - val_loss: 1.0310 - val_accuracy: 0.8667\n",
    "Epoch 190/500\n",
    "3/3 [==============================] - 2s 504ms/step - loss: 0.0956 - accuracy: 0.9643 - val_loss: 1.1171 - val_accuracy: 0.8444\n",
    "Epoch 191/500\n",
    "3/3 [==============================] - 2s 511ms/step - loss: 0.1927 - accuracy: 0.9226 - val_loss: 1.1016 - val_accuracy: 0.8667\n",
    "Epoch 192/500\n",
    "3/3 [==============================] - 2s 522ms/step - loss: 0.1024 - accuracy: 0.9524 - val_loss: 1.0269 - val_accuracy: 0.8667\n",
    "Epoch 193/500\n",
    "3/3 [==============================] - 2s 648ms/step - loss: 0.1238 - accuracy: 0.9583 - val_loss: 1.0023 - val_accuracy: 0.8667\n",
    "Epoch 194/500\n",
    "3/3 [==============================] - 2s 657ms/step - loss: 0.0985 - accuracy: 0.9643 - val_loss: 1.0176 - val_accuracy: 0.8444\n",
    "Epoch 195/500\n",
    "3/3 [==============================] - 2s 508ms/step - loss: 0.2024 - accuracy: 0.9286 - val_loss: 1.0569 - val_accuracy: 0.8444\n",
    "Epoch 196/500\n",
    "3/3 [==============================] - 2s 500ms/step - loss: 0.0976 - accuracy: 0.9702 - val_loss: 1.1333 - val_accuracy: 0.8444\n",
    "Epoch 197/500\n",
    "3/3 [==============================] - 2s 656ms/step - loss: 0.1610 - accuracy: 0.9464 - val_loss: 1.2607 - val_accuracy: 0.7778\n",
    "Epoch 198/500\n",
    "3/3 [==============================] - 2s 661ms/step - loss: 0.1200 - accuracy: 0.9583 - val_loss: 1.3339 - val_accuracy: 0.7556\n",
    "Epoch 199/500\n",
    "3/3 [==============================] - 2s 516ms/step - loss: 0.0833 - accuracy: 0.9643 - val_loss: 1.3079 - val_accuracy: 0.7556\n",
    "Epoch 200/500\n",
    "3/3 [==============================] - 2s 504ms/step - loss: 0.1564 - accuracy: 0.9583 - val_loss: 1.1764 - val_accuracy: 0.8000\n",
    "Epoch 201/500\n",
    "3/3 [==============================] - 2s 654ms/step - loss: 0.2536 - accuracy: 0.9167 - val_loss: 1.2044 - val_accuracy: 0.8222\n",
    "Epoch 202/500\n",
    "3/3 [==============================] - 2s 661ms/step - loss: 0.1535 - accuracy: 0.9464 - val_loss: 1.4289 - val_accuracy: 0.8000\n",
    "Epoch 203/500\n",
    "3/3 [==============================] - 2s 514ms/step - loss: 0.1401 - accuracy: 0.9762 - val_loss: 1.6151 - val_accuracy: 0.7778\n",
    "Epoch 204/500\n",
    "3/3 [==============================] - 2s 658ms/step - loss: 0.0941 - accuracy: 0.9643 - val_loss: 1.7913 - val_accuracy: 0.7333\n",
    "Epoch 205/500\n",
    "\n",
    "3/3 [==============================] - 2s 655ms/step - loss: 0.1809 - accuracy: 0.9286 - val_loss: 1.9537 - val_accuracy: 0.6889\n",
    "Epoch 206/500\n",
    "3/3 [==============================] - 2s 522ms/step - loss: 0.1495 - accuracy: 0.9405 - val_loss: 2.0849 - val_accuracy: 0.6667\n",
    "Epoch 207/500\n",
    "3/3 [==============================] - 2s 656ms/step - loss: 0.1009 - accuracy: 0.9524 - val_loss: 2.1428 - val_accuracy: 0.6667\n",
    "Epoch 208/500\n",
    "3/3 [==============================] - 2s 521ms/step - loss: 0.1799 - accuracy: 0.9226 - val_loss: 2.0125 - val_accuracy: 0.6667\n",
    "Epoch 209/500\n",
    "3/3 [==============================] - 2s 519ms/step - loss: 0.1944 - accuracy: 0.9167 - val_loss: 2.0949 - val_accuracy: 0.6889\n",
    "Epoch 210/500\n",
    "3/3 [==============================] - 2s 660ms/step - loss: 0.1096 - accuracy: 0.9702 - val_loss: 2.3250 - val_accuracy: 0.6667\n",
    "Epoch 211/500\n",
    "3/3 [==============================] - 2s 657ms/step - loss: 0.1741 - accuracy: 0.9464 - val_loss: 2.3272 - val_accuracy: 0.6889\n",
    "Epoch 212/500\n",
    "3/3 [==============================] - 2s 649ms/step - loss: 0.1646 - accuracy: 0.9345 - val_loss: 2.0606 - val_accuracy: 0.6889\n",
    "Epoch 213/500\n",
    "3/3 [==============================] - 2s 665ms/step - loss: 0.0927 - accuracy: 0.9583 - val_loss: 1.9459 - val_accuracy: 0.7333\n",
    "Epoch 214/500\n",
    "3/3 [==============================] - 2s 519ms/step - loss: 0.1554 - accuracy: 0.9464 - val_loss: 1.6947 - val_accuracy: 0.8222\n",
    "Epoch 215/500\n",
    "3/3 [==============================] - 2s 514ms/step - loss: 0.0783 - accuracy: 0.9702 - val_loss: 1.5999 - val_accuracy: 0.8222\n",
    "Epoch 216/500\n",
    "3/3 [==============================] - 2s 662ms/step - loss: 0.0912 - accuracy: 0.9643 - val_loss: 1.5954 - val_accuracy: 0.8000\n",
    "Epoch 217/500\n",
    "3/3 [==============================] - 2s 507ms/step - loss: 0.1051 - accuracy: 0.9464 - val_loss: 1.6069 - val_accuracy: 0.8000\n",
    "Epoch 218/500\n",
    "3/3 [==============================] - 2s 504ms/step - loss: 0.1053 - accuracy: 0.9583 - val_loss: 1.7668 - val_accuracy: 0.7778\n",
    "Epoch 219/500\n",
    "3/3 [==============================] - 2s 515ms/step - loss: 0.0332 - accuracy: 0.9940 - val_loss: 1.9787 - val_accuracy: 0.7556\n",
    "Epoch 220/500\n",
    "3/3 [==============================] - 2s 511ms/step - loss: 0.1476 - accuracy: 0.9464 - val_loss: 2.0234 - val_accuracy: 0.7556\n",
    "Epoch 221/500\n",
    "3/3 [==============================] - 2s 498ms/step - loss: 0.1047 - accuracy: 0.9702 - val_loss: 1.9409 - val_accuracy: 0.7556\n",
    "Epoch 222/500\n",
    "3/3 [==============================] - 2s 515ms/step - loss: 0.1713 - accuracy: 0.9345 - val_loss: 1.6919 - val_accuracy: 0.8222\n",
    "Epoch 223/500\n",
    "3/3 [==============================] - 2s 505ms/step - loss: 0.1444 - accuracy: 0.9583 - val_loss: 1.4947 - val_accuracy: 0.8222\n",
    "Epoch 224/500\n",
    "3/3 [==============================] - 2s 521ms/step - loss: 0.1101 - accuracy: 0.9643 - val_loss: 1.4076 - val_accuracy: 0.8222\n",
    "Epoch 225/500\n",
    "3/3 [==============================] - 2s 650ms/step - loss: 0.0583 - accuracy: 0.9762 - val_loss: 1.3873 - val_accuracy: 0.8222\n",
    "Epoch 226/500\n",
    "3/3 [==============================] - 2s 660ms/step - loss: 0.1158 - accuracy: 0.9524 - val_loss: 1.2671 - val_accuracy: 0.8444\n",
    "Epoch 227/500\n",
    "3/3 [==============================] - 2s 660ms/step - loss: 0.1159 - accuracy: 0.9583 - val_loss: 1.1662 - val_accuracy: 0.8667\n",
    "Epoch 228/500\n",
    "3/3 [==============================] - 2s 655ms/step - loss: 0.1253 - accuracy: 0.9583 - val_loss: 1.1887 - val_accuracy: 0.8667\n",
    "Epoch 229/500\n",
    "3/3 [==============================] - 2s 654ms/step - loss: 0.1863 - accuracy: 0.9524 - val_loss: 1.1674 - val_accuracy: 0.8889\n",
    "Epoch 230/500\n",
    "3/3 [==============================] - 2s 500ms/step - loss: 0.1294 - accuracy: 0.9524 - val_loss: 1.1668 - val_accuracy: 0.8889\n",
    "Epoch 231/500\n",
    "3/3 [==============================] - 2s 648ms/step - loss: 0.1068 - accuracy: 0.9643 - val_loss: 1.1493 - val_accuracy: 0.8667\n",
    "Epoch 232/500\n",
    "3/3 [==============================] - 2s 510ms/step - loss: 0.0832 - accuracy: 0.9821 - val_loss: 1.1236 - val_accuracy: 0.8667\n",
    "Epoch 233/500\n",
    "3/3 [==============================] - 2s 652ms/step - loss: 0.0712 - accuracy: 0.9643 - val_loss: 1.1083 - val_accuracy: 0.8667\n",
    "Epoch 234/500\n",
    "3/3 [==============================] - 2s 647ms/step - loss: 0.1056 - accuracy: 0.9524 - val_loss: 1.0378 - val_accuracy: 0.8667\n",
    "Epoch 235/500\n",
    "3/3 [==============================] - 2s 514ms/step - loss: 0.1129 - accuracy: 0.9583 - val_loss: 0.9854 - val_accuracy: 0.8667\n",
    "Epoch 236/500\n",
    "3/3 [==============================] - 2s 659ms/step - loss: 0.1421 - accuracy: 0.9524 - val_loss: 0.9520 - val_accuracy: 0.8444\n",
    "Epoch 237/500\n",
    "3/3 [==============================] - 2s 651ms/step - loss: 0.1349 - accuracy: 0.9524 - val_loss: 0.7995 - val_accuracy: 0.8222\n",
    "Epoch 238/500\n",
    "3/3 [==============================] - 2s 507ms/step - loss: 0.0971 - accuracy: 0.9643 - val_loss: 0.6786 - val_accuracy: 0.8444\n",
    "Epoch 239/500\n",
    "3/3 [==============================] - 2s 518ms/step - loss: 0.0963 - accuracy: 0.9762 - val_loss: 0.6085 - val_accuracy: 0.8667\n",
    "Epoch 240/500\n",
    "3/3 [==============================] - 2s 509ms/step - loss: 0.1529 - accuracy: 0.9464 - val_loss: 0.5780 - val_accuracy: 0.8667\n",
    "Epoch 241/500\n",
    "3/3 [==============================] - 2s 495ms/step - loss: 0.0716 - accuracy: 0.9821 - val_loss: 0.5664 - val_accuracy: 0.8667\n",
    "Epoch 242/500\n",
    "3/3 [==============================] - 2s 647ms/step - loss: 0.1487 - accuracy: 0.9524 - val_loss: 0.5818 - val_accuracy: 0.8667\n",
    "Epoch 243/500\n",
    "3/3 [==============================] - 2s 517ms/step - loss: 0.1553 - accuracy: 0.9464 - val_loss: 0.7003 - val_accuracy: 0.8667\n",
    "Epoch 244/500\n",
    "3/3 [==============================] - 2s 516ms/step - loss: 0.1251 - accuracy: 0.9583 - val_loss: 0.8952 - val_accuracy: 0.8444\n",
    "Epoch 245/500\n",
    "3/3 [==============================] - 2s 652ms/step - loss: 0.1922 - accuracy: 0.9345 - val_loss: 1.0643 - val_accuracy: 0.8667\n",
    "Epoch 246/500\n",
    "3/3 [==============================] - 2s 504ms/step - loss: 0.1342 - accuracy: 0.9583 - val_loss: 1.2037 - val_accuracy: 0.8444\n",
    "Epoch 247/500\n",
    "3/3 [==============================] - 2s 514ms/step - loss: 0.0826 - accuracy: 0.9643 - val_loss: 1.2267 - val_accuracy: 0.8444\n",
    "Epoch 248/500\n",
    "3/3 [==============================] - 2s 519ms/step - loss: 0.0728 - accuracy: 0.9643 - val_loss: 21.1996 - val_accuracy: 0.2444\n",
    "Epoch 249/500\n",
    "3/3 [==============================] - 2s 510ms/step - loss: 0.0720 - accuracy: 0.9821 - val_loss: 7.7876 - val_accuracy: 0.4667\n",
    "Epoch 250/500\n",
    "3/3 [==============================] - 2s 662ms/step - loss: 0.0534 - accuracy: 0.9762 - val_loss: 4.1621 - val_accuracy: 0.5333\n",
    "Epoch 251/500\n",
    "3/3 [==============================] - 2s 497ms/step - loss: 0.0673 - accuracy: 0.9881 - val_loss: 3.3244 - val_accuracy: 0.5778\n",
    "Epoch 252/500\n",
    "3/3 [==============================] - 2s 511ms/step - loss: 0.1784 - accuracy: 0.9405 - val_loss: 2.9168 - val_accuracy: 0.5778\n",
    "Epoch 253/500\n",
    "3/3 [==============================] - 2s 515ms/step - loss: 0.2626 - accuracy: 0.9107 - val_loss: 2.4742 - val_accuracy: 0.6444\n",
    "Epoch 254/500\n",
    "3/3 [==============================] - 2s 514ms/step - loss: 0.1138 - accuracy: 0.9643 - val_loss: 2.0533 - val_accuracy: 0.7111\n",
    "Epoch 255/500\n",
    "3/3 [==============================] - 2s 505ms/step - loss: 0.1073 - accuracy: 0.9583 - val_loss: 1.9169 - val_accuracy: 0.7556\n",
    "Epoch 256/500\n",
    "3/3 [==============================] - 2s 657ms/step - loss: 0.1775 - accuracy: 0.9226 - val_loss: 1.6437 - val_accuracy: 0.7778\n",
    "Epoch 257/500\n",
    "3/3 [==============================] - 2s 507ms/step - loss: 0.1663 - accuracy: 0.9405 - val_loss: 1.3619 - val_accuracy: 0.8000\n",
    "Epoch 258/500\n",
    "3/3 [==============================] - 2s 656ms/step - loss: 0.0920 - accuracy: 0.9643 - val_loss: 1.1320 - val_accuracy: 0.8222\n",
    "Epoch 259/500\n",
    "3/3 [==============================] - 2s 656ms/step - loss: 0.1203 - accuracy: 0.9405 - val_loss: 1.0639 - val_accuracy: 0.8444\n",
    "Epoch 260/500\n",
    "3/3 [==============================] - 2s 649ms/step - loss: 0.0742 - accuracy: 0.9702 - val_loss: 1.0941 - val_accuracy: 0.8444\n",
    "Epoch 261/500\n",
    "3/3 [==============================] - 2s 658ms/step - loss: 0.0961 - accuracy: 0.9643 - val_loss: 1.1521 - val_accuracy: 0.8444\n",
    "Epoch 262/500\n",
    "\n",
    "3/3 [==============================] - 2s 647ms/step - loss: 0.1500 - accuracy: 0.9524 - val_loss: 1.2152 - val_accuracy: 0.8444\n",
    "Epoch 263/500\n",
    "3/3 [==============================] - 2s 498ms/step - loss: 0.0785 - accuracy: 0.9702 - val_loss: 1.2790 - val_accuracy: 0.8444\n",
    "Epoch 264/500\n",
    "3/3 [==============================] - 2s 516ms/step - loss: 0.0997 - accuracy: 0.9702 - val_loss: 1.3220 - val_accuracy: 0.8222\n",
    "Epoch 265/500\n",
    "3/3 [==============================] - 2s 521ms/step - loss: 0.2258 - accuracy: 0.9405 - val_loss: 1.4096 - val_accuracy: 0.8000\n",
    "Epoch 266/500\n",
    "3/3 [==============================] - 2s 525ms/step - loss: 0.1123 - accuracy: 0.9524 - val_loss: 1.4178 - val_accuracy: 0.8000\n",
    "Epoch 267/500\n",
    "3/3 [==============================] - 2s 656ms/step - loss: 0.1196 - accuracy: 0.9583 - val_loss: 1.3474 - val_accuracy: 0.8222\n",
    "Epoch 268/500\n",
    "3/3 [==============================] - 2s 528ms/step - loss: 0.0841 - accuracy: 0.9643 - val_loss: 1.2825 - val_accuracy: 0.8444\n",
    "Epoch 269/500\n",
    "3/3 [==============================] - 2s 662ms/step - loss: 0.0946 - accuracy: 0.9762 - val_loss: 1.2996 - val_accuracy: 0.8667\n",
    "Epoch 270/500\n",
    "3/3 [==============================] - 2s 506ms/step - loss: 0.0948 - accuracy: 0.9702 - val_loss: 1.2604 - val_accuracy: 0.8444\n",
    "Epoch 271/500\n",
    "3/3 [==============================] - 2s 514ms/step - loss: 0.1035 - accuracy: 0.9524 - val_loss: 1.2453 - val_accuracy: 0.8444\n",
    "Epoch 272/500\n",
    "3/3 [==============================] - 2s 655ms/step - loss: 0.0633 - accuracy: 0.9821 - val_loss: 1.2000 - val_accuracy: 0.8222\n",
    "Epoch 273/500\n",
    "3/3 [==============================] - 2s 517ms/step - loss: 0.1502 - accuracy: 0.9405 - val_loss: 1.1526 - val_accuracy: 0.8222\n",
    "Epoch 274/500\n",
    "3/3 [==============================] - 2s 501ms/step - loss: 0.0704 - accuracy: 0.9702 - val_loss: 1.1581 - val_accuracy: 0.8222\n",
    "Epoch 275/500\n",
    "3/3 [==============================] - 2s 649ms/step - loss: 0.0958 - accuracy: 0.9405 - val_loss: 1.2633 - val_accuracy: 0.8222\n",
    "Epoch 276/500\n",
    "3/3 [==============================] - 2s 662ms/step - loss: 0.1683 - accuracy: 0.9524 - val_loss: 1.2576 - val_accuracy: 0.7778\n",
    "Epoch 277/500\n",
    "3/3 [==============================] - 2s 502ms/step - loss: 0.0800 - accuracy: 0.9702 - val_loss: 1.2151 - val_accuracy: 0.7778\n",
    "Epoch 278/500\n",
    "3/3 [==============================] - 2s 656ms/step - loss: 0.0925 - accuracy: 0.9524 - val_loss: 1.2054 - val_accuracy: 0.7778\n",
    "Epoch 279/500\n",
    "3/3 [==============================] - 2s 496ms/step - loss: 0.1176 - accuracy: 0.9524 - val_loss: 1.1484 - val_accuracy: 0.7778\n",
    "Epoch 280/500\n",
    "3/3 [==============================] - 2s 518ms/step - loss: 0.0689 - accuracy: 0.9881 - val_loss: 1.1312 - val_accuracy: 0.8000\n",
    "Epoch 281/500\n",
    "3/3 [==============================] - 2s 657ms/step - loss: 0.0789 - accuracy: 0.9643 - val_loss: 1.1701 - val_accuracy: 0.7778\n",
    "Epoch 282/500\n",
    "3/3 [==============================] - 2s 510ms/step - loss: 0.0628 - accuracy: 0.9821 - val_loss: 1.1941 - val_accuracy: 0.7778\n",
    "Epoch 283/500\n",
    "3/3 [==============================] - 2s 497ms/step - loss: 0.0991 - accuracy: 0.9762 - val_loss: 1.0799 - val_accuracy: 0.8667\n",
    "Epoch 284/500\n",
    "3/3 [==============================] - 2s 513ms/step - loss: 0.0518 - accuracy: 0.9940 - val_loss: 0.9943 - val_accuracy: 0.8667\n",
    "Epoch 285/500\n",
    "3/3 [==============================] - 2s 656ms/step - loss: 0.0830 - accuracy: 0.9702 - val_loss: 0.9277 - val_accuracy: 0.8667\n",
    "Epoch 286/500\n",
    "3/3 [==============================] - 2s 650ms/step - loss: 0.0982 - accuracy: 0.9583 - val_loss: 0.8515 - val_accuracy: 0.8667\n",
    "Epoch 287/500\n",
    "3/3 [==============================] - 2s 653ms/step - loss: 0.1495 - accuracy: 0.9464 - val_loss: 0.8961 - val_accuracy: 0.8667\n",
    "Epoch 288/500\n",
    "3/3 [==============================] - 2s 501ms/step - loss: 0.1173 - accuracy: 0.9762 - val_loss: 0.9857 - val_accuracy: 0.8667\n",
    "Epoch 289/500\n",
    "3/3 [==============================] - 2s 514ms/step - loss: 0.0554 - accuracy: 0.9821 - val_loss: 1.0516 - val_accuracy: 0.8444\n",
    "Epoch 290/500\n",
    "3/3 [==============================] - 2s 512ms/step - loss: 0.1096 - accuracy: 0.9524 - val_loss: 1.0712 - val_accuracy: 0.8444\n",
    "Epoch 291/500\n",
    "3/3 [==============================] - 2s 495ms/step - loss: 0.0791 - accuracy: 0.9643 - val_loss: 1.0653 - val_accuracy: 0.8444\n",
    "Epoch 292/500\n",
    "3/3 [==============================] - 2s 500ms/step - loss: 0.1042 - accuracy: 0.9702 - val_loss: 1.0503 - val_accuracy: 0.8667\n",
    "Epoch 293/500\n",
    "3/3 [==============================] - 2s 655ms/step - loss: 0.0969 - accuracy: 0.9643 - val_loss: 1.0011 - val_accuracy: 0.8667\n",
    "Epoch 294/500\n",
    "3/3 [==============================] - 2s 648ms/step - loss: 0.0743 - accuracy: 0.9762 - val_loss: 0.9134 - val_accuracy: 0.8444\n",
    "Epoch 295/500\n",
    "3/3 [==============================] - 2s 511ms/step - loss: 0.0860 - accuracy: 0.9643 - val_loss: 0.8931 - val_accuracy: 0.8667\n",
    "Epoch 296/500\n",
    "3/3 [==============================] - 2s 506ms/step - loss: 0.0703 - accuracy: 0.9702 - val_loss: 0.8971 - val_accuracy: 0.8889\n",
    "Epoch 297/500\n",
    "3/3 [==============================] - 2s 504ms/step - loss: 0.0584 - accuracy: 0.9821 - val_loss: 0.8744 - val_accuracy: 0.8889\n",
    "Epoch 298/500\n",
    "3/3 [==============================] - 2s 653ms/step - loss: 0.2321 - accuracy: 0.9167 - val_loss: 0.7977 - val_accuracy: 0.8667\n",
    "Epoch 299/500\n",
    "3/3 [==============================] - 2s 515ms/step - loss: 0.1587 - accuracy: 0.9583 - val_loss: 0.7340 - val_accuracy: 0.8667\n",
    "Epoch 300/500\n",
    "3/3 [==============================] - 2s 518ms/step - loss: 0.1446 - accuracy: 0.9524 - val_loss: 0.7402 - val_accuracy: 0.8667\n",
    "Epoch 301/500\n",
    "3/3 [==============================] - 2s 642ms/step - loss: 0.1157 - accuracy: 0.9643 - val_loss: 0.7201 - val_accuracy: 0.8667\n",
    "Epoch 302/500\n",
    "3/3 [==============================] - 2s 523ms/step - loss: 0.0302 - accuracy: 0.9940 - val_loss: 0.6820 - val_accuracy: 0.8889\n",
    "Epoch 303/500\n",
    "3/3 [==============================] - 2s 511ms/step - loss: 0.0886 - accuracy: 0.9643 - val_loss: 0.6593 - val_accuracy: 0.8889\n",
    "Epoch 304/500\n",
    "3/3 [==============================] - 2s 516ms/step - loss: 0.1275 - accuracy: 0.9702 - val_loss: 0.6706 - val_accuracy: 0.8889\n",
    "Epoch 305/500\n",
    "3/3 [==============================] - 2s 500ms/step - loss: 0.1662 - accuracy: 0.9286 - val_loss: 0.7696 - val_accuracy: 0.8889\n",
    "Epoch 306/500\n",
    "3/3 [==============================] - 2s 651ms/step - loss: 0.0351 - accuracy: 0.9881 - val_loss: 0.8583 - val_accuracy: 0.9111\n",
    "Epoch 307/500\n",
    "3/3 [==============================] - 2s 641ms/step - loss: 0.1007 - accuracy: 0.9583 - val_loss: 0.9132 - val_accuracy: 0.8889\n",
    "Epoch 308/500\n",
    "3/3 [==============================] - 2s 501ms/step - loss: 0.0708 - accuracy: 0.9583 - val_loss: 0.9174 - val_accuracy: 0.8889\n",
    "Epoch 309/500\n",
    "3/3 [==============================] - 2s 508ms/step - loss: 0.0498 - accuracy: 0.9821 - val_loss: 0.8908 - val_accuracy: 0.8889\n",
    "Epoch 310/500\n",
    "3/3 [==============================] - 2s 658ms/step - loss: 0.0999 - accuracy: 0.9643 - val_loss: 0.8933 - val_accuracy: 0.8444\n",
    "Epoch 311/500\n",
    "3/3 [==============================] - 2s 454ms/step - loss: 0.1114 - accuracy: 0.9702 - val_loss: 0.9061 - val_accuracy: 0.8222\n",
    "Epoch 312/500\n",
    "3/3 [==============================] - 2s 452ms/step - loss: 0.0978 - accuracy: 0.9524 - val_loss: 0.9346 - val_accuracy: 0.8222\n",
    "Epoch 313/500\n",
    "3/3 [==============================] - 2s 643ms/step - loss: 0.1074 - accuracy: 0.9583 - val_loss: 0.8679 - val_accuracy: 0.8444\n",
    "Epoch 314/500\n",
    "3/3 [==============================] - 2s 487ms/step - loss: 0.1665 - accuracy: 0.9464 - val_loss: 0.7588 - val_accuracy: 0.8444\n",
    "Epoch 315/500\n",
    "3/3 [==============================] - 2s 450ms/step - loss: 0.1103 - accuracy: 0.9583 - val_loss: 0.6677 - val_accuracy: 0.8444\n",
    "Epoch 316/500\n",
    "3/3 [==============================] - 2s 453ms/step - loss: 0.0788 - accuracy: 0.9821 - val_loss: 0.6533 - val_accuracy: 0.8444\n",
    "Epoch 317/500\n",
    "3/3 [==============================] - 2s 453ms/step - loss: 0.0768 - accuracy: 0.9643 - val_loss: 0.6134 - val_accuracy: 0.8667\n",
    "Epoch 318/500\n",
    "3/3 [==============================] - 2s 474ms/step - loss: 0.0553 - accuracy: 0.9762 - val_loss: 0.5829 - val_accuracy: 0.8889\n",
    "Epoch 319/500\n",
    "\n",
    "3/3 [==============================] - 2s 455ms/step - loss: 0.0668 - accuracy: 0.9821 - val_loss: 0.6338 - val_accuracy: 0.8889\n",
    "Epoch 320/500\n",
    "3/3 [==============================] - 2s 611ms/step - loss: 0.1017 - accuracy: 0.9821 - val_loss: 0.7270 - val_accuracy: 0.8889\n",
    "Epoch 321/500\n",
    "3/3 [==============================] - 2s 473ms/step - loss: 0.1315 - accuracy: 0.9464 - val_loss: 0.8584 - val_accuracy: 0.8667\n",
    "Epoch 322/500\n",
    "3/3 [==============================] - 2s 478ms/step - loss: 0.1144 - accuracy: 0.9583 - val_loss: 0.9841 - val_accuracy: 0.8444\n",
    "Epoch 323/500\n",
    "3/3 [==============================] - 2s 461ms/step - loss: 0.0731 - accuracy: 0.9702 - val_loss: 1.0829 - val_accuracy: 0.8444\n",
    "Epoch 324/500\n",
    "3/3 [==============================] - 2s 452ms/step - loss: 0.0873 - accuracy: 0.9702 - val_loss: 1.1724 - val_accuracy: 0.8444\n",
    "Epoch 325/500\n",
    "3/3 [==============================] - 2s 470ms/step - loss: 0.0833 - accuracy: 0.9702 - val_loss: 1.3405 - val_accuracy: 0.8444\n",
    "Epoch 326/500\n",
    "3/3 [==============================] - 2s 445ms/step - loss: 0.1090 - accuracy: 0.9524 - val_loss: 1.4248 - val_accuracy: 0.8222\n",
    "Epoch 327/500\n",
    "3/3 [==============================] - 2s 482ms/step - loss: 0.0455 - accuracy: 0.9762 - val_loss: 1.4903 - val_accuracy: 0.8222\n",
    "Epoch 328/500\n",
    "3/3 [==============================] - 2s 455ms/step - loss: 0.0561 - accuracy: 0.9821 - val_loss: 1.4626 - val_accuracy: 0.8222\n",
    "Epoch 329/500\n",
    "3/3 [==============================] - 2s 611ms/step - loss: 0.0750 - accuracy: 0.9702 - val_loss: 1.2935 - val_accuracy: 0.8444\n",
    "Epoch 330/500\n",
    "3/3 [==============================] - 2s 618ms/step - loss: 0.0861 - accuracy: 0.9702 - val_loss: 1.2167 - val_accuracy: 0.8444\n",
    "Epoch 331/500\n",
    "3/3 [==============================] - 2s 471ms/step - loss: 0.0259 - accuracy: 1.0000 - val_loss: 1.1881 - val_accuracy: 0.8667\n",
    "Epoch 332/500\n",
    "3/3 [==============================] - 2s 601ms/step - loss: 0.1587 - accuracy: 0.9464 - val_loss: 1.2605 - val_accuracy: 0.8444\n",
    "Epoch 333/500\n",
    "3/3 [==============================] - 2s 474ms/step - loss: 0.1151 - accuracy: 0.9464 - val_loss: 1.3410 - val_accuracy: 0.8444\n",
    "Epoch 334/500\n",
    "3/3 [==============================] - 2s 460ms/step - loss: 0.0743 - accuracy: 0.9702 - val_loss: 1.3497 - val_accuracy: 0.8444\n",
    "Epoch 335/500\n",
    "3/3 [==============================] - 2s 463ms/step - loss: 0.0759 - accuracy: 0.9702 - val_loss: 1.3431 - val_accuracy: 0.8667\n",
    "Epoch 336/500\n",
    "3/3 [==============================] - 2s 474ms/step - loss: 0.0542 - accuracy: 0.9881 - val_loss: 1.3148 - val_accuracy: 0.8889\n",
    "Epoch 337/500\n",
    "3/3 [==============================] - 2s 467ms/step - loss: 0.0449 - accuracy: 0.9821 - val_loss: 1.2915 - val_accuracy: 0.8889\n",
    "Epoch 338/500\n",
    "3/3 [==============================] - 2s 608ms/step - loss: 0.0568 - accuracy: 0.9762 - val_loss: 1.2482 - val_accuracy: 0.8889\n",
    "Epoch 339/500\n",
    "3/3 [==============================] - 2s 447ms/step - loss: 0.0985 - accuracy: 0.9762 - val_loss: 1.2253 - val_accuracy: 0.9111\n",
    "Epoch 340/500\n",
    "3/3 [==============================] - 2s 607ms/step - loss: 0.1395 - accuracy: 0.9405 - val_loss: 1.1707 - val_accuracy: 0.9111\n",
    "Epoch 341/500\n",
    "3/3 [==============================] - 2s 612ms/step - loss: 0.0655 - accuracy: 0.9762 - val_loss: 1.1234 - val_accuracy: 0.9111\n",
    "Epoch 342/500\n",
    "3/3 [==============================] - 2s 610ms/step - loss: 0.1071 - accuracy: 0.9821 - val_loss: 1.0855 - val_accuracy: 0.9111\n",
    "Epoch 343/500\n",
    "3/3 [==============================] - 2s 452ms/step - loss: 0.0359 - accuracy: 0.9821 - val_loss: 1.1053 - val_accuracy: 0.9111\n",
    "Epoch 344/500\n",
    "3/3 [==============================] - 2s 447ms/step - loss: 0.0956 - accuracy: 0.9643 - val_loss: 1.1222 - val_accuracy: 0.9111\n",
    "Epoch 345/500\n",
    "3/3 [==============================] - 2s 453ms/step - loss: 0.0642 - accuracy: 0.9643 - val_loss: 1.1179 - val_accuracy: 0.9111\n",
    "Epoch 346/500\n",
    "3/3 [==============================] - 2s 461ms/step - loss: 0.0414 - accuracy: 0.9881 - val_loss: 1.1003 - val_accuracy: 0.9111\n",
    "Epoch 347/500\n",
    "3/3 [==============================] - 2s 467ms/step - loss: 0.1132 - accuracy: 0.9643 - val_loss: 1.1382 - val_accuracy: 0.9111\n",
    "Epoch 348/500\n",
    "3/3 [==============================] - 2s 609ms/step - loss: 0.0452 - accuracy: 0.9821 - val_loss: 1.2177 - val_accuracy: 0.9111\n",
    "Epoch 349/500\n",
    "3/3 [==============================] - 2s 612ms/step - loss: 0.0928 - accuracy: 0.9643 - val_loss: 1.2441 - val_accuracy: 0.8889\n",
    "Epoch 350/500\n",
    "3/3 [==============================] - 2s 451ms/step - loss: 0.0719 - accuracy: 0.9881 - val_loss: 1.2581 - val_accuracy: 0.8889\n",
    "Epoch 351/500\n",
    "3/3 [==============================] - 2s 607ms/step - loss: 0.0739 - accuracy: 0.9821 - val_loss: 1.2608 - val_accuracy: 0.8889\n",
    "Epoch 352/500\n",
    "3/3 [==============================] - 2s 453ms/step - loss: 0.0775 - accuracy: 0.9821 - val_loss: 1.2691 - val_accuracy: 0.9111\n",
    "Epoch 353/500\n",
    "3/3 [==============================] - 2s 453ms/step - loss: 0.0719 - accuracy: 0.9702 - val_loss: 1.2947 - val_accuracy: 0.9111\n",
    "Epoch 354/500\n",
    "3/3 [==============================] - 2s 451ms/step - loss: 0.0720 - accuracy: 0.9702 - val_loss: 1.2966 - val_accuracy: 0.9111\n",
    "Epoch 355/500\n",
    "3/3 [==============================] - 2s 462ms/step - loss: 0.0528 - accuracy: 0.9940 - val_loss: 1.2899 - val_accuracy: 0.8667\n",
    "Epoch 356/500\n",
    "3/3 [==============================] - 2s 611ms/step - loss: 0.0732 - accuracy: 0.9821 - val_loss: 1.2773 - val_accuracy: 0.8667\n",
    "Epoch 357/500\n",
    "3/3 [==============================] - 2s 449ms/step - loss: 0.0517 - accuracy: 0.9881 - val_loss: 1.2560 - val_accuracy: 0.8889\n",
    "Epoch 358/500\n",
    "3/3 [==============================] - 2s 610ms/step - loss: 0.0795 - accuracy: 0.9643 - val_loss: 1.2251 - val_accuracy: 0.8667\n",
    "Epoch 359/500\n",
    "3/3 [==============================] - 2s 469ms/step - loss: 0.1253 - accuracy: 0.9643 - val_loss: 1.2135 - val_accuracy: 0.8889\n",
    "Epoch 360/500\n",
    "3/3 [==============================] - 2s 606ms/step - loss: 0.0785 - accuracy: 0.9583 - val_loss: 1.2166 - val_accuracy: 0.8667\n",
    "Epoch 361/500\n",
    "3/3 [==============================] - 2s 471ms/step - loss: 0.0714 - accuracy: 0.9643 - val_loss: 1.2089 - val_accuracy: 0.8444\n",
    "Epoch 362/500\n",
    "3/3 [==============================] - 2s 469ms/step - loss: 0.1027 - accuracy: 0.9583 - val_loss: 1.1678 - val_accuracy: 0.8667\n",
    "Epoch 363/500\n",
    "3/3 [==============================] - 2s 607ms/step - loss: 0.0913 - accuracy: 0.9643 - val_loss: 1.0731 - val_accuracy: 0.8667\n",
    "Epoch 364/500\n",
    "3/3 [==============================] - 2s 604ms/step - loss: 0.0679 - accuracy: 0.9702 - val_loss: 0.9965 - val_accuracy: 0.8889\n",
    "Epoch 365/500\n",
    "3/3 [==============================] - 2s 610ms/step - loss: 0.0566 - accuracy: 0.9821 - val_loss: 1.0297 - val_accuracy: 0.8889\n",
    "Epoch 366/500\n",
    "3/3 [==============================] - 2s 464ms/step - loss: 0.0631 - accuracy: 0.9821 - val_loss: 1.0555 - val_accuracy: 0.8667\n",
    "Epoch 367/500\n",
    "3/3 [==============================] - 2s 605ms/step - loss: 0.0338 - accuracy: 0.9821 - val_loss: 1.0725 - val_accuracy: 0.8667\n",
    "Epoch 368/500\n",
    "3/3 [==============================] - 2s 469ms/step - loss: 0.0882 - accuracy: 0.9821 - val_loss: 1.1703 - val_accuracy: 0.8667\n",
    "Epoch 369/500\n",
    "3/3 [==============================] - 2s 448ms/step - loss: 0.1577 - accuracy: 0.9345 - val_loss: 1.3098 - val_accuracy: 0.8667\n",
    "Epoch 370/500\n",
    "3/3 [==============================] - 2s 453ms/step - loss: 0.0875 - accuracy: 0.9643 - val_loss: 1.5215 - val_accuracy: 0.8222\n",
    "Epoch 371/500\n",
    "3/3 [==============================] - 2s 608ms/step - loss: 0.0406 - accuracy: 0.9881 - val_loss: 1.6763 - val_accuracy: 0.8222\n",
    "Epoch 372/500\n",
    "3/3 [==============================] - 2s 448ms/step - loss: 0.0802 - accuracy: 0.9762 - val_loss: 1.6585 - val_accuracy: 0.8222\n",
    "Epoch 373/500\n",
    "3/3 [==============================] - 2s 475ms/step - loss: 0.1242 - accuracy: 0.9464 - val_loss: 1.6129 - val_accuracy: 0.8222\n",
    "Epoch 374/500\n",
    "3/3 [==============================] - 2s 477ms/step - loss: 0.1100 - accuracy: 0.9762 - val_loss: 1.5785 - val_accuracy: 0.8222\n",
    "Epoch 375/500\n",
    "3/3 [==============================] - 2s 607ms/step - loss: 0.1361 - accuracy: 0.9524 - val_loss: 1.5545 - val_accuracy: 0.8444\n",
    "Epoch 376/500\n",
    "\n",
    "3/3 [==============================] - 2s 467ms/step - loss: 0.0567 - accuracy: 0.9821 - val_loss: 1.4843 - val_accuracy: 0.8444\n",
    "Epoch 377/500\n",
    "3/3 [==============================] - 2s 467ms/step - loss: 0.0808 - accuracy: 0.9702 - val_loss: 1.3739 - val_accuracy: 0.8444\n",
    "Epoch 378/500\n",
    "3/3 [==============================] - 2s 608ms/step - loss: 0.1295 - accuracy: 0.9464 - val_loss: 1.2661 - val_accuracy: 0.8667\n",
    "Epoch 379/500\n",
    "3/3 [==============================] - 2s 613ms/step - loss: 0.0742 - accuracy: 0.9821 - val_loss: 1.1913 - val_accuracy: 0.8667\n",
    "Epoch 380/500\n",
    "3/3 [==============================] - 2s 451ms/step - loss: 0.0661 - accuracy: 0.9643 - val_loss: 1.1702 - val_accuracy: 0.8667\n",
    "Epoch 381/500\n",
    "3/3 [==============================] - 2s 451ms/step - loss: 0.0575 - accuracy: 0.9762 - val_loss: 1.1843 - val_accuracy: 0.8444\n",
    "Epoch 382/500\n",
    "3/3 [==============================] - 2s 449ms/step - loss: 0.1136 - accuracy: 0.9583 - val_loss: 1.1562 - val_accuracy: 0.8444\n",
    "Epoch 383/500\n",
    "3/3 [==============================] - 2s 602ms/step - loss: 0.1415 - accuracy: 0.9524 - val_loss: 1.1713 - val_accuracy: 0.8000\n",
    "Epoch 384/500\n",
    "3/3 [==============================] - 2s 480ms/step - loss: 0.1737 - accuracy: 0.9286 - val_loss: 1.3310 - val_accuracy: 0.8000\n",
    "Epoch 385/500\n",
    "3/3 [==============================] - 2s 451ms/step - loss: 0.1246 - accuracy: 0.9524 - val_loss: 1.4564 - val_accuracy: 0.8000\n",
    "Epoch 386/500\n",
    "3/3 [==============================] - 2s 453ms/step - loss: 0.1109 - accuracy: 0.9643 - val_loss: 1.3837 - val_accuracy: 0.8222\n",
    "Epoch 387/500\n",
    "3/3 [==============================] - 2s 465ms/step - loss: 0.1584 - accuracy: 0.9345 - val_loss: 1.0794 - val_accuracy: 0.8667\n",
    "Epoch 388/500\n",
    "3/3 [==============================] - 2s 473ms/step - loss: 0.0917 - accuracy: 0.9702 - val_loss: 0.9646 - val_accuracy: 0.8889\n",
    "Epoch 389/500\n",
    "3/3 [==============================] - 2s 452ms/step - loss: 0.0735 - accuracy: 0.9702 - val_loss: 0.8988 - val_accuracy: 0.8889\n",
    "Epoch 390/500\n",
    "3/3 [==============================] - 2s 603ms/step - loss: 0.0968 - accuracy: 0.9524 - val_loss: 0.8432 - val_accuracy: 0.9111\n",
    "Epoch 391/500\n",
    "3/3 [==============================] - 2s 471ms/step - loss: 0.0460 - accuracy: 0.9821 - val_loss: 0.8295 - val_accuracy: 0.9111\n",
    "Epoch 392/500\n",
    "3/3 [==============================] - 2s 608ms/step - loss: 0.0868 - accuracy: 0.9702 - val_loss: 0.8338 - val_accuracy: 0.9111\n",
    "Epoch 393/500\n",
    "3/3 [==============================] - 2s 474ms/step - loss: 0.0636 - accuracy: 0.9583 - val_loss: 0.8728 - val_accuracy: 0.9111\n",
    "Epoch 394/500\n",
    "3/3 [==============================] - 2s 469ms/step - loss: 0.0256 - accuracy: 0.9940 - val_loss: 0.9075 - val_accuracy: 0.9111\n",
    "Epoch 395/500\n",
    "3/3 [==============================] - 2s 615ms/step - loss: 0.1052 - accuracy: 0.9702 - val_loss: 0.9093 - val_accuracy: 0.9111\n",
    "Epoch 396/500\n",
    "3/3 [==============================] - 2s 471ms/step - loss: 0.0556 - accuracy: 0.9762 - val_loss: 0.8992 - val_accuracy: 0.9111\n",
    "Epoch 397/500\n",
    "3/3 [==============================] - 2s 610ms/step - loss: 0.0857 - accuracy: 0.9702 - val_loss: 0.9054 - val_accuracy: 0.9111\n",
    "Epoch 398/500\n",
    "3/3 [==============================] - 2s 454ms/step - loss: 0.0986 - accuracy: 0.9524 - val_loss: 0.9482 - val_accuracy: 0.9111\n",
    "Epoch 399/500\n",
    "3/3 [==============================] - 2s 467ms/step - loss: 0.0690 - accuracy: 0.9762 - val_loss: 1.0058 - val_accuracy: 0.9111\n",
    "Epoch 400/500\n",
    "3/3 [==============================] - 2s 603ms/step - loss: 0.0340 - accuracy: 0.9881 - val_loss: 1.0565 - val_accuracy: 0.9111\n",
    "Epoch 401/500\n",
    "3/3 [==============================] - 2s 469ms/step - loss: 0.0638 - accuracy: 0.9643 - val_loss: 1.0983 - val_accuracy: 0.8889\n",
    "Epoch 402/500\n",
    "3/3 [==============================] - 2s 471ms/step - loss: 0.1360 - accuracy: 0.9583 - val_loss: 1.0901 - val_accuracy: 0.8889\n",
    "Epoch 403/500\n",
    "3/3 [==============================] - 2s 609ms/step - loss: 0.0549 - accuracy: 0.9762 - val_loss: 1.0701 - val_accuracy: 0.8889\n",
    "Epoch 404/500\n",
    "3/3 [==============================] - 2s 449ms/step - loss: 0.0565 - accuracy: 0.9762 - val_loss: 1.0568 - val_accuracy: 0.8889\n",
    "Epoch 405/500\n",
    "3/3 [==============================] - 2s 472ms/step - loss: 0.1553 - accuracy: 0.9405 - val_loss: 1.0278 - val_accuracy: 0.8889\n",
    "Epoch 406/500\n",
    "3/3 [==============================] - 2s 599ms/step - loss: 0.1111 - accuracy: 0.9524 - val_loss: 1.0611 - val_accuracy: 0.8667\n",
    "Epoch 407/500\n",
    "3/3 [==============================] - 2s 469ms/step - loss: 0.1685 - accuracy: 0.9524 - val_loss: 1.1721 - val_accuracy: 0.8667\n",
    "Epoch 408/500\n",
    "3/3 [==============================] - 2s 447ms/step - loss: 0.0976 - accuracy: 0.9643 - val_loss: 1.1937 - val_accuracy: 0.8667\n",
    "Epoch 409/500\n",
    "3/3 [==============================] - 2s 602ms/step - loss: 0.1701 - accuracy: 0.9524 - val_loss: 1.2947 - val_accuracy: 0.8000\n",
    "Epoch 410/500\n",
    "3/3 [==============================] - 2s 608ms/step - loss: 0.0946 - accuracy: 0.9583 - val_loss: 1.4025 - val_accuracy: 0.8222\n",
    "Epoch 411/500\n",
    "3/3 [==============================] - 2s 616ms/step - loss: 0.1357 - accuracy: 0.9524 - val_loss: 1.5176 - val_accuracy: 0.8000\n",
    "Epoch 412/500\n",
    "3/3 [==============================] - 2s 469ms/step - loss: 0.1067 - accuracy: 0.9583 - val_loss: 1.5656 - val_accuracy: 0.8000\n",
    "Epoch 413/500\n",
    "3/3 [==============================] - 2s 477ms/step - loss: 0.0836 - accuracy: 0.9762 - val_loss: 1.5603 - val_accuracy: 0.8000\n",
    "Epoch 414/500\n",
    "3/3 [==============================] - 2s 469ms/step - loss: 0.0777 - accuracy: 0.9702 - val_loss: 1.6945 - val_accuracy: 0.7778\n",
    "Epoch 415/500\n",
    "3/3 [==============================] - 2s 468ms/step - loss: 0.0772 - accuracy: 0.9643 - val_loss: 1.9777 - val_accuracy: 0.7556\n",
    "Epoch 416/500\n",
    "3/3 [==============================] - 2s 606ms/step - loss: 0.0780 - accuracy: 0.9762 - val_loss: 2.1065 - val_accuracy: 0.7333\n",
    "Epoch 417/500\n",
    "3/3 [==============================] - 2s 618ms/step - loss: 0.0678 - accuracy: 0.9821 - val_loss: 2.1191 - val_accuracy: 0.7111\n",
    "Epoch 418/500\n",
    "3/3 [==============================] - 2s 474ms/step - loss: 0.1119 - accuracy: 0.9464 - val_loss: 2.1006 - val_accuracy: 0.7556\n",
    "Epoch 419/500\n",
    "3/3 [==============================] - 2s 450ms/step - loss: 0.0801 - accuracy: 0.9643 - val_loss: 1.9354 - val_accuracy: 0.7778\n",
    "Epoch 420/500\n",
    "3/3 [==============================] - 2s 614ms/step - loss: 0.0545 - accuracy: 0.9821 - val_loss: 1.7930 - val_accuracy: 0.8000\n",
    "Epoch 421/500\n",
    "3/3 [==============================] - 2s 480ms/step - loss: 0.0361 - accuracy: 0.9881 - val_loss: 1.6672 - val_accuracy: 0.7778\n",
    "Epoch 422/500\n",
    "3/3 [==============================] - 2s 476ms/step - loss: 0.0594 - accuracy: 0.9583 - val_loss: 1.5352 - val_accuracy: 0.8000\n",
    "Epoch 423/500\n",
    "3/3 [==============================] - 2s 600ms/step - loss: 0.0284 - accuracy: 0.9881 - val_loss: 1.4411 - val_accuracy: 0.8222\n",
    "Epoch 424/500\n",
    "3/3 [==============================] - 2s 453ms/step - loss: 0.0535 - accuracy: 0.9881 - val_loss: 1.3827 - val_accuracy: 0.8444\n",
    "Epoch 425/500\n",
    "3/3 [==============================] - 2s 492ms/step - loss: 0.1130 - accuracy: 0.9524 - val_loss: 1.2937 - val_accuracy: 0.8444\n",
    "Epoch 426/500\n",
    "3/3 [==============================] - 2s 449ms/step - loss: 0.1020 - accuracy: 0.9702 - val_loss: 1.1519 - val_accuracy: 0.8444\n",
    "Epoch 427/500\n",
    "3/3 [==============================] - 2s 611ms/step - loss: 0.0393 - accuracy: 0.9940 - val_loss: 1.1439 - val_accuracy: 0.8222\n",
    "Epoch 428/500\n",
    "3/3 [==============================] - 2s 472ms/step - loss: 0.0693 - accuracy: 0.9702 - val_loss: 1.2249 - val_accuracy: 0.8222\n",
    "Epoch 429/500\n",
    "3/3 [==============================] - 2s 454ms/step - loss: 0.0732 - accuracy: 0.9702 - val_loss: 1.2592 - val_accuracy: 0.8444\n",
    "Epoch 430/500\n",
    "3/3 [==============================] - 2s 472ms/step - loss: 0.0560 - accuracy: 0.9821 - val_loss: 1.3164 - val_accuracy: 0.8667\n",
    "Epoch 431/500\n",
    "3/3 [==============================] - 2s 453ms/step - loss: 0.0317 - accuracy: 0.9940 - val_loss: 1.3884 - val_accuracy: 0.8667\n",
    "Epoch 432/500\n",
    "3/3 [==============================] - 2s 475ms/step - loss: 0.0684 - accuracy: 0.9702 - val_loss: 1.4105 - val_accuracy: 0.8667\n",
    "Epoch 433/500\n",
    "\n",
    "3/3 [==============================] - 2s 592ms/step - loss: 0.0903 - accuracy: 0.9762 - val_loss: 1.3685 - val_accuracy: 0.8444\n",
    "Epoch 434/500\n",
    "3/3 [==============================] - 2s 464ms/step - loss: 0.0947 - accuracy: 0.9821 - val_loss: 1.3189 - val_accuracy: 0.8444\n",
    "Epoch 435/500\n",
    "3/3 [==============================] - 2s 447ms/step - loss: 0.0717 - accuracy: 0.9702 - val_loss: 1.3140 - val_accuracy: 0.8444\n",
    "Epoch 436/500\n",
    "3/3 [==============================] - 2s 600ms/step - loss: 0.0600 - accuracy: 0.9821 - val_loss: 1.3491 - val_accuracy: 0.8444\n",
    "Epoch 437/500\n",
    "3/3 [==============================] - 2s 454ms/step - loss: 0.0796 - accuracy: 0.9643 - val_loss: 1.3331 - val_accuracy: 0.8222\n",
    "Epoch 438/500\n",
    "3/3 [==============================] - 2s 471ms/step - loss: 0.0609 - accuracy: 0.9762 - val_loss: 1.2571 - val_accuracy: 0.8444\n",
    "Epoch 439/500\n",
    "3/3 [==============================] - 2s 450ms/step - loss: 0.1019 - accuracy: 0.9643 - val_loss: 1.1812 - val_accuracy: 0.8667\n",
    "Epoch 440/500\n",
    "3/3 [==============================] - 2s 607ms/step - loss: 0.0293 - accuracy: 0.9881 - val_loss: 1.2069 - val_accuracy: 0.8667\n",
    "Epoch 441/500\n",
    "3/3 [==============================] - 2s 449ms/step - loss: 0.0850 - accuracy: 0.9702 - val_loss: 1.2562 - val_accuracy: 0.8667\n",
    "Epoch 442/500\n",
    "3/3 [==============================] - 2s 470ms/step - loss: 0.0310 - accuracy: 0.9881 - val_loss: 1.2757 - val_accuracy: 0.8667\n",
    "Epoch 443/500\n",
    "3/3 [==============================] - 2s 457ms/step - loss: 0.0370 - accuracy: 0.9821 - val_loss: 1.2953 - val_accuracy: 0.8667\n",
    "Epoch 444/500\n",
    "3/3 [==============================] - 2s 605ms/step - loss: 0.1266 - accuracy: 0.9583 - val_loss: 1.2716 - val_accuracy: 0.8444\n",
    "Epoch 445/500\n",
    "3/3 [==============================] - 2s 611ms/step - loss: 0.0415 - accuracy: 0.9881 - val_loss: 1.2085 - val_accuracy: 0.8667\n",
    "Epoch 446/500\n",
    "3/3 [==============================] - 2s 610ms/step - loss: 0.0599 - accuracy: 0.9643 - val_loss: 1.1571 - val_accuracy: 0.9111\n",
    "Epoch 447/500\n",
    "3/3 [==============================] - 2s 603ms/step - loss: 0.0419 - accuracy: 0.9821 - val_loss: 1.1223 - val_accuracy: 0.9111\n",
    "Epoch 448/500\n",
    "3/3 [==============================] - 2s 456ms/step - loss: 0.0903 - accuracy: 0.9583 - val_loss: 1.0503 - val_accuracy: 0.8889\n",
    "Epoch 449/500\n",
    "3/3 [==============================] - 2s 608ms/step - loss: 0.0456 - accuracy: 0.9881 - val_loss: 1.0374 - val_accuracy: 0.8889\n",
    "Epoch 450/500\n",
    "3/3 [==============================] - 2s 449ms/step - loss: 0.1364 - accuracy: 0.9702 - val_loss: 1.0574 - val_accuracy: 0.8889\n",
    "Epoch 451/500\n",
    "3/3 [==============================] - 2s 607ms/step - loss: 0.1043 - accuracy: 0.9762 - val_loss: 1.1142 - val_accuracy: 0.8889\n",
    "Epoch 452/500\n",
    "3/3 [==============================] - 2s 454ms/step - loss: 0.0492 - accuracy: 0.9881 - val_loss: 1.1619 - val_accuracy: 0.8889\n",
    "Epoch 453/500\n",
    "3/3 [==============================] - 2s 476ms/step - loss: 0.0536 - accuracy: 0.9881 - val_loss: 1.2254 - val_accuracy: 0.8889\n",
    "Epoch 454/500\n",
    "3/3 [==============================] - 2s 449ms/step - loss: 0.0701 - accuracy: 0.9881 - val_loss: 1.3053 - val_accuracy: 0.8667\n",
    "Epoch 455/500\n",
    "3/3 [==============================] - 2s 447ms/step - loss: 0.0923 - accuracy: 0.9643 - val_loss: 1.3051 - val_accuracy: 0.8444\n",
    "Epoch 456/500\n",
    "3/3 [==============================] - 2s 603ms/step - loss: 0.1053 - accuracy: 0.9643 - val_loss: 1.3277 - val_accuracy: 0.8444\n",
    "Epoch 457/500\n",
    "3/3 [==============================] - 2s 479ms/step - loss: 0.0448 - accuracy: 0.9762 - val_loss: 1.3303 - val_accuracy: 0.8889\n",
    "Epoch 458/500\n",
    "3/3 [==============================] - 2s 611ms/step - loss: 0.0322 - accuracy: 0.9821 - val_loss: 1.3451 - val_accuracy: 0.8667\n",
    "Epoch 459/500\n",
    "3/3 [==============================] - 2s 468ms/step - loss: 0.1168 - accuracy: 0.9524 - val_loss: 1.3109 - val_accuracy: 0.8667\n",
    "Epoch 460/500\n",
    "3/3 [==============================] - 2s 463ms/step - loss: 0.0367 - accuracy: 0.9881 - val_loss: 1.2464 - val_accuracy: 0.8667\n",
    "Epoch 461/500\n",
    "3/3 [==============================] - 2s 448ms/step - loss: 0.1102 - accuracy: 0.9643 - val_loss: 1.1628 - val_accuracy: 0.8667\n",
    "Epoch 462/500\n",
    "3/3 [==============================] - 2s 474ms/step - loss: 0.0434 - accuracy: 0.9881 - val_loss: 1.0462 - val_accuracy: 0.8667\n",
    "Epoch 463/500\n",
    "3/3 [==============================] - 2s 478ms/step - loss: 0.0942 - accuracy: 0.9762 - val_loss: 0.8891 - val_accuracy: 0.8667\n",
    "Epoch 464/500\n",
    "3/3 [==============================] - 2s 451ms/step - loss: 0.0505 - accuracy: 0.9821 - val_loss: 0.7751 - val_accuracy: 0.9111\n",
    "Epoch 465/500\n",
    "3/3 [==============================] - 2s 450ms/step - loss: 0.0921 - accuracy: 0.9762 - val_loss: 0.7646 - val_accuracy: 0.9333\n",
    "Epoch 466/500\n",
    "3/3 [==============================] - 2s 475ms/step - loss: 0.1150 - accuracy: 0.9762 - val_loss: 0.8233 - val_accuracy: 0.9111\n",
    "Epoch 467/500\n",
    "3/3 [==============================] - 2s 450ms/step - loss: 0.0615 - accuracy: 0.9702 - val_loss: 0.8654 - val_accuracy: 0.8889\n",
    "Epoch 468/500\n",
    "3/3 [==============================] - 2s 605ms/step - loss: 0.0581 - accuracy: 0.9821 - val_loss: 0.9084 - val_accuracy: 0.8889\n",
    "Epoch 469/500\n",
    "3/3 [==============================] - 2s 446ms/step - loss: 0.1851 - accuracy: 0.9583 - val_loss: 0.9609 - val_accuracy: 0.8667\n",
    "Epoch 470/500\n",
    "3/3 [==============================] - 2s 471ms/step - loss: 0.0653 - accuracy: 0.9762 - val_loss: 1.0594 - val_accuracy: 0.8667\n",
    "Epoch 471/500\n",
    "3/3 [==============================] - 2s 610ms/step - loss: 0.0218 - accuracy: 0.9940 - val_loss: 1.1327 - val_accuracy: 0.8667\n",
    "Epoch 472/500\n",
    "3/3 [==============================] - 2s 468ms/step - loss: 0.0976 - accuracy: 0.9583 - val_loss: 1.1873 - val_accuracy: 0.8667\n",
    "Epoch 473/500\n",
    "3/3 [==============================] - 2s 454ms/step - loss: 0.0985 - accuracy: 0.9762 - val_loss: 1.2537 - val_accuracy: 0.8667\n",
    "Epoch 474/500\n",
    "3/3 [==============================] - 2s 466ms/step - loss: 0.0592 - accuracy: 0.9762 - val_loss: 1.2963 - val_accuracy: 0.8667\n",
    "Epoch 475/500\n",
    "3/3 [==============================] - 2s 606ms/step - loss: 0.0242 - accuracy: 0.9940 - val_loss: 1.2869 - val_accuracy: 0.8667\n",
    "Epoch 476/500\n",
    "3/3 [==============================] - 2s 608ms/step - loss: 0.0629 - accuracy: 0.9762 - val_loss: 1.2552 - val_accuracy: 0.8889\n",
    "Epoch 477/500\n",
    "3/3 [==============================] - 2s 446ms/step - loss: 0.0362 - accuracy: 0.9881 - val_loss: 1.2196 - val_accuracy: 0.8889\n",
    "Epoch 478/500\n",
    "3/3 [==============================] - 2s 451ms/step - loss: 0.1161 - accuracy: 0.9643 - val_loss: 1.1777 - val_accuracy: 0.8889\n",
    "Epoch 479/500\n",
    "3/3 [==============================] - 2s 452ms/step - loss: 0.0270 - accuracy: 0.9940 - val_loss: 1.1529 - val_accuracy: 0.8889\n",
    "Epoch 480/500\n",
    "3/3 [==============================] - 2s 460ms/step - loss: 0.0636 - accuracy: 0.9821 - val_loss: 1.1503 - val_accuracy: 0.8889\n",
    "Epoch 481/500\n",
    "3/3 [==============================] - 2s 603ms/step - loss: 0.0477 - accuracy: 0.9821 - val_loss: 1.1613 - val_accuracy: 0.8889\n",
    "Epoch 482/500\n",
    "3/3 [==============================] - 2s 602ms/step - loss: 0.0513 - accuracy: 0.9881 - val_loss: 1.1639 - val_accuracy: 0.8889\n",
    "Epoch 483/500\n",
    "3/3 [==============================] - 2s 464ms/step - loss: 0.0842 - accuracy: 0.9762 - val_loss: 1.1598 - val_accuracy: 0.8889\n",
    "Epoch 484/500\n",
    "3/3 [==============================] - 2s 613ms/step - loss: 0.0784 - accuracy: 0.9524 - val_loss: 1.1462 - val_accuracy: 0.8889\n",
    "Epoch 485/500\n",
    "3/3 [==============================] - 2s 609ms/step - loss: 0.0744 - accuracy: 0.9643 - val_loss: 1.1358 - val_accuracy: 0.8889\n",
    "Epoch 486/500\n",
    "3/3 [==============================] - 2s 473ms/step - loss: 0.0423 - accuracy: 0.9821 - val_loss: 1.1558 - val_accuracy: 0.8889\n",
    "Epoch 487/500\n",
    "3/3 [==============================] - 2s 466ms/step - loss: 0.0202 - accuracy: 0.9940 - val_loss: 1.1859 - val_accuracy: 0.8667\n",
    "Epoch 488/500\n",
    "3/3 [==============================] - 2s 595ms/step - loss: 0.0517 - accuracy: 0.9821 - val_loss: 1.2096 - val_accuracy: 0.8667\n",
    "Epoch 489/500\n",
    "3/3 [==============================] - 2s 451ms/step - loss: 0.0468 - accuracy: 0.9881 - val_loss: 1.2216 - val_accuracy: 0.8889\n",
    "Epoch 490/500\n",
    "\n",
    "3/3 [==============================] - 2s 594ms/step - loss: 0.0540 - accuracy: 0.9762 - val_loss: 1.1952 - val_accuracy: 0.8889\n",
    "Epoch 491/500\n",
    "3/3 [==============================] - 2s 612ms/step - loss: 0.0525 - accuracy: 0.9881 - val_loss: 1.1758 - val_accuracy: 0.8889\n",
    "Epoch 492/500\n",
    "3/3 [==============================] - 2s 466ms/step - loss: 0.0595 - accuracy: 0.9702 - val_loss: 1.1480 - val_accuracy: 0.8889\n",
    "Epoch 493/500\n",
    "3/3 [==============================] - 2s 458ms/step - loss: 0.0822 - accuracy: 0.9702 - val_loss: 1.1023 - val_accuracy: 0.8889\n",
    "Epoch 494/500\n",
    "3/3 [==============================] - 2s 474ms/step - loss: 0.1237 - accuracy: 0.9464 - val_loss: 1.1134 - val_accuracy: 0.8889\n",
    "Epoch 495/500\n",
    "3/3 [==============================] - 2s 612ms/step - loss: 0.0402 - accuracy: 0.9881 - val_loss: 1.1393 - val_accuracy: 0.9111\n",
    "Epoch 496/500\n",
    "3/3 [==============================] - 2s 477ms/step - loss: 0.0164 - accuracy: 0.9940 - val_loss: 1.1544 - val_accuracy: 0.9111\n",
    "Epoch 497/500\n",
    "3/3 [==============================] - 2s 617ms/step - loss: 0.0443 - accuracy: 0.9821 - val_loss: 1.1417 - val_accuracy: 0.9111\n",
    "Epoch 498/500\n",
    "3/3 [==============================] - 2s 476ms/step - loss: 0.0502 - accuracy: 0.9881 - val_loss: 1.1041 - val_accuracy: 0.9111\n",
    "Epoch 499/500\n",
    "3/3 [==============================] - 2s 460ms/step - loss: 0.0630 - accuracy: 0.9821 - val_loss: 1.0835 - val_accuracy: 0.9111\n",
    "Epoch 500/500\n",
    "3/3 [==============================] - 2s 603ms/step - loss: 0.0204 - accuracy: 0.9940 - val_loss: 1.0726 - val_accuracy: 0.9111\n",
    "{'loss': [2.8224217891693115, 2.307492256164551, 1.994598388671875, 1.8551826477050781, 1.8314214944839478, 2.0271480083465576, 1.6069685220718384, 1.8269587755203247, 1.539365291595459, 1.5557992458343506, 1.409258484840393, 1.338297724723816, 1.234679102897644, 1.20840585231781, 1.1639065742492676, 1.343855381011963, 1.130843162536621, 1.088213562965393, 0.8120269775390625, 0.9573601484298706, 0.8671271800994873, 0.9490194916725159, 0.798831582069397, 0.8559786677360535, 0.7380879521369934, 0.7872075438499451, 0.6459723711013794, 0.7751896977424622, 0.7371075749397278, 0.6784279346466064, 0.6030135154724121, 0.6757642030715942, 0.5510818958282471, 0.651553213596344, 0.5127008557319641, 0.5398967862129211, 0.5874668955802917, 0.46521830558776855, 0.39679858088493347, 0.516497015953064, 0.39172208309173584, 0.338593065738678, 0.3551972806453705, 0.4558383822441101, 0.5933687686920166, 0.37788721919059753, 0.34864839911460876, 0.4599301815032959, 0.4007563889026642, 0.39260202646255493, 0.2824111580848694, 0.3739345669746399, 0.3197607398033142, 0.25949782133102417, 0.3937382400035858, 0.2809348404407501, 0.5069593191146851, 0.48495787382125854, 0.36590662598609924, 0.4087330996990204, 0.29836705327033997, 0.3539017140865326, 0.25533586740493774, 0.33993542194366455, 0.3560571074485779, 0.339379221200943, 0.3281932473182678, 0.37223315238952637, 0.2878851294517517, 0.3615613579750061, 0.369499534368515, 0.29120609164237976, 0.29351717233657837, 0.33716264367103577, 0.3578410744667053, 0.260154664516449, 0.25839492678642273, 0.2115575671195984, 0.28780806064605713, 0.3300240635871887, 0.24455468356609344, 0.2550758719444275, 0.21945849061012268, 0.2274947464466095, 0.1838548630475998, 0.2503534257411957, 0.27114227414131165, 0.188394233584404, 0.19129717350006104, 0.20720073580741882, 0.2674778997898102, 0.31509289145469666, 0.2363845258951187, 0.23374271392822266, 0.2864980697631836, 0.32071080803871155, 0.19905953109264374, 0.17592327296733856, 0.14928333461284637, 0.20060399174690247, 0.1437661200761795, 0.1973515748977661, 0.19222013652324677, 0.1876702606678009, 0.30959492921829224, 0.22306308150291443, 0.2780722975730896, 0.14166772365570068, 0.24871154129505157, 0.269429475069046, 0.18496064841747284, 0.2690466344356537, 0.13800743222236633, 0.1422424614429474, 0.20399191975593567, 0.1540071666240692, 0.216140478849411, 0.16966775059700012, 0.17321547865867615, 0.1643412709236145, 0.15451401472091675, 0.17720331251621246, 0.18317772448062897, 0.23695264756679535, 0.18292362987995148, 0.11310259252786636, 0.13002866506576538, 0.18578580021858215, 0.16985416412353516, 0.19216541945934296, 0.09833260625600815, 0.17483003437519073, 0.19597165286540985, 0.15634094178676605, 0.22535750269889832, 0.17375396192073822, 0.14085468649864197, 0.14180879294872284, 0.12704333662986755, 0.16394174098968506, 0.2031429409980774, 0.15416137874126434, 0.14615799486637115, 0.17951703071594238, 0.21234039962291718, 0.1904236376285553, 0.18486911058425903, 0.1527751237154007, 0.20382998883724213, 0.17855612933635712, 0.20225706696510315, 0.11893397569656372, 0.16308733820915222, 0.2218848466873169, 0.12312446534633636, 0.15527647733688354, 0.09073331207036972, 0.12395654618740082, 0.14817698299884796, 0.1783285290002823, 0.12493714690208435, 0.11677178740501404, 0.10217130929231644, 0.09281755983829498, 0.15938593447208405, 0.09261645376682281, 0.13660350441932678, 0.16562484204769135, 0.17298978567123413, 0.16407664120197296, 0.11505001783370972, 0.14793230593204498, 0.062178630381822586, 0.19415469467639923, 0.2143091857433319, 0.13172969222068787, 0.07488714903593063, 0.12040863931179047, 0.08308526128530502, 0.1524660438299179, 0.13776786625385284, 0.10149163007736206, 0.11168185621500015, 0.1788744032382965, 0.07307473570108414, 0.1602007895708084, 0.1260756105184555, 0.10749638825654984, 0.16486728191375732, 0.09555304795503616, 0.1926749348640442, 0.10240525007247925, 0.12376286089420319, 0.09845870733261108, 0.20235751569271088, 0.09756587445735931, 0.16097989678382874, 0.12000658363103867, 0.08330173790454865, 0.1564360409975052, 0.2536056935787201, 0.1534622311592102, 0.14010114967823029, 0.09414049237966537, 0.18094150722026825, 0.14951284229755402, 0.10091827064752579, 0.17994225025177002, 0.19439515471458435, 0.10957089811563492, 0.17405255138874054, 0.16462089121341705, 0.0926625058054924, 0.15540388226509094, 0.07829469442367554, 0.09119518101215363, 0.10509807616472244, 0.10533858835697174, 0.03315813094377518, 0.14761945605278015, 0.10472781211137772, 0.17131192982196808, 0.1444299817085266, 0.11009422689676285, 0.05831124633550644, 0.1157703623175621, 0.11591304838657379, 0.12528793513774872, 0.18633927404880524, 0.12935560941696167, 0.10684877634048462, 0.08322399109601974, 0.07120376080274582, 0.10559218376874924, 0.1128934919834137, 0.1420895755290985, 0.13493289053440094, 0.09708965569734573, 0.09633632004261017, 0.15293574333190918, 0.0715714544057846, 0.14870887994766235, 0.15533190965652466, 0.12509320676326752, 0.1922074556350708, 0.13421165943145752, 0.08262503892183304, 0.0727519616484642, 0.0719902291893959, 0.05341276526451111, 0.06730524450540543, 0.17837856709957123, 0.2625708281993866, 0.11378294229507446, 0.10733732581138611, 0.17753997445106506, 0.16631433367729187, 0.09201028198003769, 0.12030994147062302, 0.0741690993309021, 0.09613452851772308, 0.15001656115055084, 0.07846461236476898, 0.09966366738080978, 0.2258208990097046, 0.1122635006904602, 0.11959970742464066, 0.08408494293689728, 0.09461768716573715, 0.09483546018600464, 0.10352186858654022, 0.06326095759868622, 0.1502048522233963, 0.0703878253698349, 0.09581815451383591, 0.16834756731987, 0.08002930879592896, 0.09251087158918381, 0.11759074777364731, 0.06891705095767975, 0.07891450822353363, 0.0628010630607605, 0.09911145269870758, 0.05181766673922539, 0.08299386501312256, 0.09819740056991577, 0.14953601360321045, 0.11727360635995865, 0.05537617951631546, 0.10955636948347092, 0.07913141697645187, 0.10424189269542694, 0.09687326848506927, 0.07430999726057053, 0.08599274605512619, 0.07031026482582092, 0.05841686576604843, 0.2321072518825531, 0.15868283808231354, 0.14459607005119324, 0.11572744697332382, 0.030205365270376205, 0.08863581717014313, 0.12749579548835754, 0.16623355448246002, 0.03514781594276428, 0.1006508320569992, 0.07075796276330948, 0.049844175577163696, 0.09993630647659302, 0.11136694997549057, 0.09780556708574295, 0.10739176720380783, 0.1664840430021286, 0.11027862131595612, 0.07882898300886154, 0.07683448493480682, 0.0552983283996582, 0.0668041855096817, 0.10167470574378967, 0.1315435916185379, 0.1143864318728447, 0.07308913767337799, 0.08727207779884338, 0.08333650976419449, 0.10899515450000763, 0.04548678919672966, 0.056088946759700775, 0.07496507465839386, 0.08605604618787766, 0.025923410430550575, 0.15872065722942352, 0.11514659225940704, 0.0743180364370346, 0.07593890279531479, 0.05417661368846893, 0.04485912621021271, 0.05683862417936325, 0.09847096353769302, 0.13952188193798065, 0.06548687070608139, 0.10706530511379242, 0.03591208904981613, 0.09560736268758774, 0.06422265619039536, 0.041368402540683746, 0.11318756639957428, 0.045167673379182816, 0.09281139075756073, 0.07193703204393387, 0.07392141968011856, 0.07754917442798615, 0.07186774164438248, 0.07198206335306168, 0.05284671485424042, 0.07321227341890335, 0.051749978214502335, 0.07945087552070618, 0.125327929854393, 0.07854713499546051, 0.07137070596218109, 0.10268958657979965, 0.09125036001205444, 0.06791439652442932, 0.05662412941455841, 0.06313806027173996, 0.03384179621934891, 0.08823336660861969, 0.15769705176353455, 0.0875343605875969, 0.04060029238462448, 0.08019647747278214, 0.12424568831920624, 0.11002770811319351, 0.13605137169361115, 0.05669182911515236, 0.08075080811977386, 0.12946870923042297, 0.07415460050106049, 0.06607138365507126, 0.05749139189720154, 0.11361972242593765, 0.14154547452926636, 0.17372038960456848, 0.12463053315877914, 0.11092901229858398, 0.1584329605102539, 0.091721311211586, 0.0734875276684761, 0.09679070115089417, 0.045969098806381226, 0.08680596947669983, 0.06359099596738815, 0.025604089722037315, 0.10519381612539291, 0.055564265698194504, 0.08566044270992279, 0.09857995063066483, 0.06899437308311462, 0.034004777669906616, 0.0637800544500351, 0.13599209487438202, 0.054910141974687576, 0.05649874731898308, 0.15527449548244476, 0.1110977903008461, 0.16852326691150665, 0.09762527793645859, 0.1700916290283203, 0.09462311118841171, 0.1356775015592575, 0.10670071840286255, 0.083563894033432, 0.07774008065462112, 0.07715076208114624, 0.07804445177316666, 0.06776537001132965, 0.11185002326965332, 0.08008062094449997, 0.05449835956096649, 0.0361257866024971, 0.059446629136800766, 0.028381625190377235, 0.053466796875, 0.1130339726805687, 0.10200177878141403, 0.03928723186254501, 0.06926580518484116, 0.07322932779788971, 0.0559970997273922, 0.03174571692943573, 0.0684010237455368, 0.09030309319496155, 0.09469141066074371, 0.07171043008565903, 0.0600229948759079, 0.07955216616392136, 0.06091585010290146, 0.1018814891576767, 0.02929069660604, 0.08500386774539948, 0.03095346689224243, 0.03699827939271927, 0.12662586569786072, 0.041548825800418854, 0.059857506304979324, 0.041862424463033676, 0.09031499177217484, 0.04564560204744339, 0.13644832372665405, 0.10429058969020844, 0.04922924563288689, 0.05360935255885124, 0.07007168978452682, 0.09229651838541031, 0.10533139854669571, 0.044791072607040405, 0.03224358707666397, 0.11676438897848129, 0.03671470284461975, 0.11024972051382065, 0.043401896953582764, 0.09418369829654694, 0.05053850635886192, 0.09206666797399521, 0.1150449812412262, 0.061521269381046295, 0.05814468115568161, 0.18513643741607666, 0.06529881060123444, 0.021806472912430763, 0.09762122482061386, 0.0985167920589447, 0.059163205325603485, 0.02420944534242153, 0.06293102353811264, 0.0361831896007061, 0.11607794463634491, 0.026970013976097107, 0.06363938748836517, 0.047653310000896454, 0.05132370442152023, 0.08419083058834076, 0.07843827456235886, 0.07444396615028381, 0.042310070246458054, 0.020230527967214584, 0.051724422723054886, 0.0468270368874073, 0.054002683609724045, 0.05247224122285843, 0.059479519724845886, 0.08217521756887436, 0.12369146943092346, 0.04019686207175255, 0.016434570774435997, 0.044283557683229446, 0.05015663802623749, 0.06297947466373444, 0.020406344905495644], 'accuracy': [0.1726190447807312, 0.3392857015132904, 0.2857142984867096, 0.3273809552192688, 0.3928571343421936, 0.3035714328289032, 0.4107142984867096, 0.3928571343421936, 0.4345238208770752, 0.4226190447807312, 0.488095223903656, 0.5178571343421936, 0.5357142686843872, 0.5773809552192688, 0.5654761791229248, 0.523809552192688, 0.5773809552192688, 0.636904776096344, 0.7083333134651184, 0.6547619104385376, 0.6726190447807312, 0.636904776096344, 0.6726190447807312, 0.7023809552192688, 0.6964285969734192, 0.7142857313156128, 0.7857142686843872, 0.7023809552192688, 0.7142857313156128, 0.7023809552192688, 0.8035714030265808, 0.7142857313156128, 0.8035714030265808, 0.75, 0.8273809552192688, 0.7916666865348816, 0.7916666865348816, 0.8273809552192688, 0.8571428656578064, 0.8392857313156128, 0.8392857313156128, 0.8690476417541504, 0.886904776096344, 0.851190447807312, 0.7916666865348816, 0.851190447807312, 0.875, 0.8154761791229248, 0.851190447807312, 0.8571428656578064, 0.8928571343421936, 0.8571428656578064, 0.875, 0.9047619104385376, 0.875, 0.8809523582458496, 0.8214285969734192, 0.8333333134651184, 0.8571428656578064, 0.875, 0.8928571343421936, 0.8452380895614624, 0.9107142686843872, 0.8690476417541504, 0.875, 0.898809552192688, 0.863095223903656, 0.863095223903656, 0.886904776096344, 0.8333333134651184, 0.851190447807312, 0.9107142686843872, 0.9107142686843872, 0.9047619104385376, 0.8809523582458496, 0.9226190447807312, 0.9047619104385376, 0.9226190447807312, 0.886904776096344, 0.8690476417541504, 0.9226190447807312, 0.898809552192688, 0.9107142686843872, 0.9047619104385376, 0.9166666865348816, 0.9047619104385376, 0.8928571343421936, 0.9404761791229248, 0.9226190447807312, 0.9226190447807312, 0.898809552192688, 0.875, 0.9404761791229248, 0.9166666865348816, 0.9107142686843872, 0.9107142686843872, 0.9166666865348816, 0.9345238208770752, 0.9345238208770752, 0.9166666865348816, 0.9464285969734192, 0.9166666865348816, 0.9226190447807312, 0.9345238208770752, 0.8928571343421936, 0.9107142686843872, 0.898809552192688, 0.9464285969734192, 0.9166666865348816, 0.9107142686843872, 0.9285714030265808, 0.9047619104385376, 0.9642857313156128, 0.9464285969734192, 0.9047619104385376, 0.9583333134651184, 0.9166666865348816, 0.9583333134651184, 0.9226190447807312, 0.9404761791229248, 0.9226190447807312, 0.9404761791229248, 0.9166666865348816, 0.9345238208770752, 0.9464285969734192, 0.9583333134651184, 0.9523809552192688, 0.9404761791229248, 0.9285714030265808, 0.9464285969734192, 0.9702380895614624, 0.9523809552192688, 0.9404761791229248, 0.9404761791229248, 0.9285714030265808, 0.9523809552192688, 0.9404761791229248, 0.9464285969734192, 0.9404761791229248, 0.9345238208770752, 0.9285714030265808, 0.9345238208770752, 0.9345238208770752, 0.9523809552192688, 0.9166666865348816, 0.9464285969734192, 0.9285714030265808, 0.9523809552192688, 0.9285714030265808, 0.9464285969734192, 0.9226190447807312, 0.976190447807312, 0.9404761791229248, 0.9285714030265808, 0.9464285969734192, 0.9583333134651184, 0.9583333134651184, 0.9404761791229248, 0.9464285969734192, 0.9404761791229248, 0.9464285969734192, 0.9583333134651184, 0.9523809552192688, 0.9642857313156128, 0.9404761791229248, 0.9702380895614624, 0.9523809552192688, 0.9523809552192688, 0.9702380895614624, 0.9404761791229248, 0.9642857313156128, 0.9404761791229248, 0.9940476417541504, 0.9285714030265808, 0.9404761791229248, 0.9285714030265808, 0.976190447807312, 0.9583333134651184, 0.9642857313156128, 0.9642857313156128, 0.9345238208770752, 0.9702380895614624, 0.9583333134651184, 0.9345238208770752, 0.9821428656578064, 0.9345238208770752, 0.9642857313156128, 0.9642857313156128, 0.9404761791229248, 0.9642857313156128, 0.9226190447807312, 0.9523809552192688, 0.9583333134651184, 0.9642857313156128, 0.9285714030265808, 0.9702380895614624, 0.9464285969734192, 0.9583333134651184, 0.9642857313156128, 0.9583333134651184, 0.9166666865348816, 0.9464285969734192, 0.976190447807312, 0.9642857313156128, 0.9285714030265808, 0.9404761791229248, 0.9523809552192688, 0.9226190447807312, 0.9166666865348816, 0.9702380895614624, 0.9464285969734192, 0.9345238208770752, 0.9583333134651184, 0.9464285969734192, 0.9702380895614624, 0.9642857313156128, 0.9464285969734192, 0.9583333134651184, 0.9940476417541504, 0.9464285969734192, 0.9702380895614624, 0.9345238208770752, 0.9583333134651184, 0.9642857313156128, 0.976190447807312, 0.9523809552192688, 0.9583333134651184, 0.9583333134651184, 0.9523809552192688, 0.9523809552192688, 0.9642857313156128, 0.9821428656578064, 0.9642857313156128, 0.9523809552192688, 0.9583333134651184, 0.9523809552192688, 0.9523809552192688, 0.9642857313156128, 0.976190447807312, 0.9464285969734192, 0.9821428656578064, 0.9523809552192688, 0.9464285969734192, 0.9583333134651184, 0.9345238208770752, 0.9583333134651184, 0.9642857313156128, 0.9642857313156128, 0.9821428656578064, 0.976190447807312, 0.988095223903656, 0.9404761791229248, 0.9107142686843872, 0.9642857313156128, 0.9583333134651184, 0.9226190447807312, 0.9404761791229248, 0.9642857313156128, 0.9404761791229248, 0.9702380895614624, 0.9642857313156128, 0.9523809552192688, 0.9702380895614624, 0.9702380895614624, 0.9404761791229248, 0.9523809552192688, 0.9583333134651184, 0.9642857313156128, 0.976190447807312, 0.9702380895614624, 0.9523809552192688, 0.9821428656578064, 0.9404761791229248, 0.9702380895614624, 0.9404761791229248, 0.9523809552192688, 0.9702380895614624, 0.9523809552192688, 0.9523809552192688, 0.988095223903656, 0.9642857313156128, 0.9821428656578064, 0.976190447807312, 0.9940476417541504, 0.9702380895614624, 0.9583333134651184, 0.9464285969734192, 0.976190447807312, 0.9821428656578064, 0.9523809552192688, 0.9642857313156128, 0.9702380895614624, 0.9642857313156128, 0.976190447807312, 0.9642857313156128, 0.9702380895614624, 0.9821428656578064, 0.9166666865348816, 0.9583333134651184, 0.9523809552192688, 0.9642857313156128, 0.9940476417541504, 0.9642857313156128, 0.9702380895614624, 0.9285714030265808, 0.988095223903656, 0.9583333134651184, 0.9583333134651184, 0.9821428656578064, 0.9642857313156128, 0.9702380895614624, 0.9523809552192688, 0.9583333134651184, 0.9464285969734192, 0.9583333134651184, 0.9821428656578064, 0.9642857313156128, 0.976190447807312, 0.9821428656578064, 0.9821428656578064, 0.9464285969734192, 0.9583333134651184, 0.9702380895614624, 0.9702380895614624, 0.9702380895614624, 0.9523809552192688, 0.976190447807312, 0.9821428656578064, 0.9702380895614624, 0.9702380895614624, 1.0, 0.9464285969734192, 0.9464285969734192, 0.9702380895614624, 0.9702380895614624, 0.988095223903656, 0.9821428656578064, 0.976190447807312, 0.976190447807312, 0.9404761791229248, 0.976190447807312, 0.9821428656578064, 0.9821428656578064, 0.9642857313156128, 0.9642857313156128, 0.988095223903656, 0.9642857313156128, 0.9821428656578064, 0.9642857313156128, 0.988095223903656, 0.9821428656578064, 0.9821428656578064, 0.9702380895614624, 0.9702380895614624, 0.9940476417541504, 0.9821428656578064, 0.988095223903656, 0.9642857313156128, 0.9642857313156128, 0.9583333134651184, 0.9642857313156128, 0.9583333134651184, 0.9642857313156128, 0.9702380895614624, 0.9821428656578064, 0.9821428656578064, 0.9821428656578064, 0.9821428656578064, 0.9345238208770752, 0.9642857313156128, 0.988095223903656, 0.976190447807312, 0.9464285969734192, 0.976190447807312, 0.9523809552192688, 0.9821428656578064, 0.9702380895614624, 0.9464285969734192, 0.9821428656578064, 0.9642857313156128, 0.976190447807312, 0.9583333134651184, 0.9523809552192688, 0.9285714030265808, 0.9523809552192688, 0.9642857313156128, 0.9345238208770752, 0.9702380895614624, 0.9702380895614624, 0.9523809552192688, 0.9821428656578064, 0.9702380895614624, 0.9583333134651184, 0.9940476417541504, 0.9702380895614624, 0.976190447807312, 0.9702380895614624, 0.9523809552192688, 0.976190447807312, 0.988095223903656, 0.9642857313156128, 0.9583333134651184, 0.976190447807312, 0.976190447807312, 0.9404761791229248, 0.9523809552192688, 0.9523809552192688, 0.9642857313156128, 0.9523809552192688, 0.9583333134651184, 0.9523809552192688, 0.9583333134651184, 0.976190447807312, 0.9702380895614624, 0.9642857313156128, 0.976190447807312, 0.9821428656578064, 0.9464285969734192, 0.9642857313156128, 0.9821428656578064, 0.988095223903656, 0.9583333134651184, 0.988095223903656, 0.988095223903656, 0.9523809552192688, 0.9702380895614624, 0.9940476417541504, 0.9702380895614624, 0.9702380895614624, 0.9821428656578064, 0.9940476417541504, 0.9702380895614624, 0.976190447807312, 0.9821428656578064, 0.9702380895614624, 0.9821428656578064, 0.9642857313156128, 0.976190447807312, 0.9642857313156128, 0.988095223903656, 0.9702380895614624, 0.988095223903656, 0.9821428656578064, 0.9583333134651184, 0.988095223903656, 0.9642857313156128, 0.9821428656578064, 0.9583333134651184, 0.988095223903656, 0.9702380895614624, 0.976190447807312, 0.988095223903656, 0.988095223903656, 0.988095223903656, 0.9642857313156128, 0.9642857313156128, 0.976190447807312, 0.9821428656578064, 0.9523809552192688, 0.988095223903656, 0.9642857313156128, 0.988095223903656, 0.976190447807312, 0.9821428656578064, 0.976190447807312, 0.976190447807312, 0.9702380895614624, 0.9821428656578064, 0.9583333134651184, 0.976190447807312, 0.9940476417541504, 0.9583333134651184, 0.976190447807312, 0.976190447807312, 0.9940476417541504, 0.976190447807312, 0.988095223903656, 0.9642857313156128, 0.9940476417541504, 0.9821428656578064, 0.9821428656578064, 0.988095223903656, 0.976190447807312, 0.9523809552192688, 0.9642857313156128, 0.9821428656578064, 0.9940476417541504, 0.9821428656578064, 0.988095223903656, 0.976190447807312, 0.988095223903656, 0.9702380895614624, 0.9702380895614624, 0.9464285969734192, 0.988095223903656, 0.9940476417541504, 0.9821428656578064, 0.988095223903656, 0.9821428656578064, 0.9940476417541504], 'val_loss': [4.761107921600342, 5.716984748840332, 7.926214694976807, 10.111655235290527, 12.125667572021484, 12.271166801452637, 12.110692024230957, 12.183996200561523, 11.966476440429688, 11.570469856262207, 11.049687385559082, 11.115815162658691, 11.496758460998535, 11.827466011047363, 11.898171424865723, 11.872821807861328, 11.78728199005127, 10.869684219360352, 10.062997817993164, 9.487110137939453, 9.554932594299316, 11.474834442138672, 12.374039649963379, 12.55247688293457, 11.365551948547363, 9.856300354003906, 8.98522663116455, 8.279279708862305, 7.591799736022949, 7.379309177398682, 7.199765205383301, 6.631732940673828, 5.968405723571777, 5.474574565887451, 5.274090766906738, 4.91510009765625, 4.661971092224121, 4.598205089569092, 4.480343818664551, 4.522656440734863, 4.634719371795654, 4.544109344482422, 4.534922122955322, 4.285414218902588, 3.7940425872802734, 3.4934234619140625, 3.343228578567505, 3.4152791500091553, 3.6979258060455322, 3.9371705055236816, 3.9974074363708496, 4.195215225219727, 4.140135288238525, 4.085948944091797, 3.6432650089263916, 2.888256549835205, 2.2376818656921387, 1.9369925260543823, 1.8029320240020752, 1.760904312133789, 1.7220097780227661, 1.813461422920227, 1.9209191799163818, 1.976680040359497, 2.11909818649292, 2.209634304046631, 2.3346259593963623, 6.987068176269531, 12.147565841674805, 8.894291877746582, 5.424215793609619, 3.412714719772339, 2.6249451637268066, 2.0144617557525635, 1.46725594997406, 1.2091784477233887, 1.0819660425186157, 0.9912264347076416, 0.9616617560386658, 1.0293821096420288, 3.406682014465332, 3.635148525238037, 3.0907552242279053, 2.738438844680786, 2.4600467681884766, 2.1308748722076416, 1.8072855472564697, 1.6461378335952759, 1.4651618003845215, 1.2967779636383057, 1.0588735342025757, 0.8410387635231018, 0.7694234848022461, 0.7665587663650513, 0.9214909076690674, 1.03565514087677, 0.9717164635658264, 0.8708226680755615, 0.807704508304596, 0.7566825151443481, 0.7148683071136475, 0.7377995252609253, 0.8151150345802307, 0.8900367021560669, 1.0465638637542725, 1.2527223825454712, 1.3261301517486572, 1.2211710214614868, 0.9811808466911316, 0.8248740434646606, 0.7457797527313232, 0.7164226174354553, 0.7531378865242004, 0.8378333449363708, 0.9434073567390442, 1.0399774312973022, 0.972613513469696, 0.9254617691040039, 0.9194939732551575, 0.893642008304596, 0.919171929359436, 0.9799838662147522, 0.9786590337753296, 4.409329414367676, 8.150156021118164, 2.7160754203796387, 5.229169845581055, 6.436294078826904, 2.5184414386749268, 2.2690186500549316, 2.1304521560668945, 2.2447075843811035, 2.3274874687194824, 2.4279072284698486, 2.2866392135620117, 1.9334419965744019, 1.5240204334259033, 1.3268874883651733, 1.1958235502243042, 1.0285998582839966, 0.8766289353370667, 0.8059114217758179, 0.8436004519462585, 0.9148256778717041, 0.9989027976989746, 1.0179083347320557, 1.008378267288208, 1.0308457612991333, 0.963679850101471, 0.8904098272323608, 0.8902294635772705, 0.9542466998100281, 0.9493787288665771, 0.7944151759147644, 0.6977987289428711, 0.6997479796409607, 0.7194944620132446, 0.7678890824317932, 0.8262254595756531, 0.863200306892395, 0.7888800501823425, 0.7127224206924438, 0.6249749064445496, 0.544212281703949, 0.5494248270988464, 0.5923129320144653, 0.6605303287506104, 0.6794772744178772, 0.6689481139183044, 0.6572151780128479, 0.6089580059051514, 0.6277825832366943, 0.6714686155319214, 0.7348595261573792, 0.8008192181587219, 0.8601977825164795, 0.9424934983253479, 1.0015383958816528, 1.0492610931396484, 1.1439005136489868, 1.1746407747268677, 1.0579895973205566, 1.0157660245895386, 0.990966260433197, 1.0697568655014038, 1.0958003997802734, 1.0379645824432373, 1.0210285186767578, 1.0310256481170654, 1.1170710325241089, 1.101582646369934, 1.0268782377243042, 1.0022786855697632, 1.0175634622573853, 1.0568512678146362, 1.1332714557647705, 1.2606651782989502, 1.3338871002197266, 1.3078533411026, 1.1763664484024048, 1.2043633460998535, 1.4289296865463257, 1.6150873899459839, 1.7913357019424438, 1.9536687135696411, 2.0848751068115234, 2.142789602279663, 2.0124709606170654, 2.0948565006256104, 2.325021982192993, 2.3271541595458984, 2.060575485229492, 1.9458991289138794, 1.6946755647659302, 1.59987473487854, 1.5954408645629883, 1.6068817377090454, 1.7667791843414307, 1.978654146194458, 2.023369789123535, 1.9408769607543945, 1.6919218301773071, 1.494688630104065, 1.4076309204101562, 1.3873146772384644, 1.2671163082122803, 1.1661971807479858, 1.1886903047561646, 1.1673641204833984, 1.1667919158935547, 1.1492984294891357, 1.1236376762390137, 1.1083378791809082, 1.0377938747406006, 0.9854242205619812, 0.95195072889328, 0.7995302081108093, 0.6785786151885986, 0.6084803938865662, 0.5779880285263062, 0.5663532018661499, 0.5818465948104858, 0.7002716064453125, 0.8951821327209473, 1.0642526149749756, 1.2037055492401123, 1.2266656160354614, 21.199554443359375, 7.787640571594238, 4.1620659828186035, 3.324389696121216, 2.9167745113372803, 2.474203586578369, 2.0533337593078613, 1.9169316291809082, 1.643696904182434, 1.3619030714035034, 1.1319612264633179, 1.0638880729675293, 1.094112753868103, 1.1520962715148926, 1.2152202129364014, 1.2790265083312988, 1.3219770193099976, 1.409566879272461, 1.4178135395050049, 1.3474472761154175, 1.2825201749801636, 1.29958975315094, 1.260447382926941, 1.245270848274231, 1.2000033855438232, 1.1526211500167847, 1.1580733060836792, 1.2632728815078735, 1.2575587034225464, 1.215065836906433, 1.2054030895233154, 1.1484450101852417, 1.1311885118484497, 1.1701006889343262, 1.1940771341323853, 1.0799031257629395, 0.9943057298660278, 0.9276975989341736, 0.8515248894691467, 0.8961430191993713, 0.9856849908828735, 1.0516386032104492, 1.071190595626831, 1.065324068069458, 1.0502984523773193, 1.0011125802993774, 0.9133965969085693, 0.8931356072425842, 0.8970546126365662, 0.8743988275527954, 0.7977387309074402, 0.734006941318512, 0.7401500940322876, 0.720051646232605, 0.6820043325424194, 0.6592574119567871, 0.6706207394599915, 0.7695515155792236, 0.8582797646522522, 0.9132132530212402, 0.9174262285232544, 0.8908319473266602, 0.8932625651359558, 0.9061110615730286, 0.9345803260803223, 0.8679454326629639, 0.7587918639183044, 0.6677159667015076, 0.6532837152481079, 0.613395094871521, 0.5829243063926697, 0.6337964534759521, 0.7269904017448425, 0.8584062457084656, 0.9841039776802063, 1.082879662513733, 1.1724425554275513, 1.340463399887085, 1.4247918128967285, 1.490293025970459, 1.462647795677185, 1.2935487031936646, 1.2166504859924316, 1.1881288290023804, 1.2604527473449707, 1.3410382270812988, 1.3497071266174316, 1.3431321382522583, 1.3148189783096313, 1.2914882898330688, 1.2482070922851562, 1.2253104448318481, 1.1706873178482056, 1.1234052181243896, 1.0855432748794556, 1.1052734851837158, 1.1222412586212158, 1.1178860664367676, 1.1002618074417114, 1.1381744146347046, 1.217652678489685, 1.244125485420227, 1.258140206336975, 1.2607535123825073, 1.2690786123275757, 1.2947168350219727, 1.2966398000717163, 1.289893627166748, 1.2772886753082275, 1.2559950351715088, 1.2251276969909668, 1.2135162353515625, 1.2166080474853516, 1.2088643312454224, 1.1677615642547607, 1.0731346607208252, 0.99653559923172, 1.0296823978424072, 1.0554624795913696, 1.0724546909332275, 1.1703181266784668, 1.3097832202911377, 1.5215446949005127, 1.6763334274291992, 1.6584664583206177, 1.6129002571105957, 1.5785293579101562, 1.5545449256896973, 1.4843064546585083, 1.3738712072372437, 1.2661277055740356, 1.1912866830825806, 1.1702466011047363, 1.184305191040039, 1.1561840772628784, 1.171318531036377, 1.3309839963912964, 1.4564181566238403, 1.3836722373962402, 1.0794174671173096, 0.9645946025848389, 0.8987753391265869, 0.8432127237319946, 0.8295395970344543, 0.8337527513504028, 0.8728302121162415, 0.9075205326080322, 0.9092769026756287, 0.8991921544075012, 0.905415415763855, 0.948214054107666, 1.0057849884033203, 1.0565375089645386, 1.0982576608657837, 1.090137004852295, 1.0701022148132324, 1.056814432144165, 1.0277734994888306, 1.0611207485198975, 1.1721447706222534, 1.1936676502227783, 1.2946652173995972, 1.4024995565414429, 1.517559289932251, 1.565585970878601, 1.5602902173995972, 1.6944793462753296, 1.9777209758758545, 2.106480121612549, 2.119086980819702, 2.1006484031677246, 1.9353580474853516, 1.793031930923462, 1.6671916246414185, 1.535239815711975, 1.4411424398422241, 1.3827005624771118, 1.2936698198318481, 1.151929259300232, 1.1438857316970825, 1.2249252796173096, 1.2592262029647827, 1.3164011240005493, 1.3884005546569824, 1.410519003868103, 1.3684637546539307, 1.3189036846160889, 1.3139866590499878, 1.3490829467773438, 1.333138346672058, 1.2571266889572144, 1.1811944246292114, 1.2069385051727295, 1.2561942338943481, 1.2756657600402832, 1.2953190803527832, 1.2715590000152588, 1.208493947982788, 1.157052993774414, 1.1222641468048096, 1.0502808094024658, 1.0373659133911133, 1.0574209690093994, 1.114188313484192, 1.1618671417236328, 1.2254059314727783, 1.3052574396133423, 1.3051023483276367, 1.3277281522750854, 1.3302853107452393, 1.345069169998169, 1.3109177350997925, 1.2463544607162476, 1.162773847579956, 1.0462197065353394, 0.8890531659126282, 0.7750985026359558, 0.7645847797393799, 0.8233193159103394, 0.8653666377067566, 0.9083502888679504, 0.9608766436576843, 1.0593715906143188, 1.1327153444290161, 1.187296748161316, 1.2536927461624146, 1.2962896823883057, 1.28688383102417, 1.2551530599594116, 1.2196348905563354, 1.1776726245880127, 1.1529345512390137, 1.1503421068191528, 1.1613484621047974, 1.1639245748519897, 1.1598089933395386, 1.1461771726608276, 1.1358203887939453, 1.1557577848434448, 1.1859164237976074, 1.2096115350723267, 1.2216442823410034, 1.1951572895050049, 1.1758041381835938, 1.1480222940444946, 1.1022909879684448, 1.1133835315704346, 1.1393475532531738, 1.1543771028518677, 1.141670823097229, 1.1041207313537598, 1.083509087562561, 1.0726014375686646], 'val_accuracy': [0.24444444477558136, 0.20000000298023224, 0.17777778208255768, 0.17777778208255768, 0.15555556118488312, 0.2222222238779068, 0.15555556118488312, 0.13333334028720856, 0.13333334028720856, 0.13333334028720856, 0.15555556118488312, 0.17777778208255768, 0.15555556118488312, 0.13333334028720856, 0.13333334028720856, 0.13333334028720856, 0.13333334028720856, 0.13333334028720856, 0.15555556118488312, 0.17777778208255768, 0.17777778208255768, 0.15555556118488312, 0.13333334028720856, 0.13333334028720856, 0.15555556118488312, 0.15555556118488312, 0.2222222238779068, 0.2666666805744171, 0.2222222238779068, 0.2666666805744171, 0.2666666805744171, 0.2666666805744171, 0.2666666805744171, 0.2666666805744171, 0.2888889014720917, 0.2888889014720917, 0.2666666805744171, 0.2666666805744171, 0.24444444477558136, 0.2222222238779068, 0.2666666805744171, 0.2888889014720917, 0.2888889014720917, 0.2888889014720917, 0.2888889014720917, 0.31111112236976624, 0.31111112236976624, 0.35555556416511536, 0.2888889014720917, 0.2888889014720917, 0.31111112236976624, 0.2888889014720917, 0.31111112236976624, 0.31111112236976624, 0.3777777850627899, 0.42222222685813904, 0.5555555820465088, 0.5333333611488342, 0.5555555820465088, 0.5111111402511597, 0.4888888895511627, 0.46666666865348816, 0.4888888895511627, 0.46666666865348816, 0.46666666865348816, 0.5111111402511597, 0.4444444477558136, 0.15555556118488312, 0.13333334028720856, 0.17777778208255768, 0.2666666805744171, 0.35555556416511536, 0.4444444477558136, 0.46666666865348816, 0.6222222447395325, 0.6666666865348816, 0.7111111283302307, 0.7333333492279053, 0.7333333492279053, 0.7111111283302307, 0.5333333611488342, 0.5333333611488342, 0.6000000238418579, 0.6222222447395325, 0.6000000238418579, 0.644444465637207, 0.6222222447395325, 0.5777778029441833, 0.6000000238418579, 0.6666666865348816, 0.7111111283302307, 0.7333333492279053, 0.7777777910232544, 0.7777777910232544, 0.7555555701255798, 0.7333333492279053, 0.7777777910232544, 0.8222222328186035, 0.8222222328186035, 0.8222222328186035, 0.8444444537162781, 0.8444444537162781, 0.8222222328186035, 0.7777777910232544, 0.7555555701255798, 0.7777777910232544, 0.7777777910232544, 0.800000011920929, 0.8222222328186035, 0.8888888955116272, 0.8888888955116272, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8444444537162781, 0.800000011920929, 0.8444444537162781, 0.8222222328186035, 0.8222222328186035, 0.8222222328186035, 0.8222222328186035, 0.8222222328186035, 0.800000011920929, 0.46666666865348816, 0.42222222685813904, 0.5555555820465088, 0.5111111402511597, 0.3777777850627899, 0.644444465637207, 0.6222222447395325, 0.644444465637207, 0.6666666865348816, 0.6666666865348816, 0.644444465637207, 0.644444465637207, 0.7333333492279053, 0.7555555701255798, 0.7777777910232544, 0.7777777910232544, 0.8222222328186035, 0.8666666746139526, 0.8666666746139526, 0.8444444537162781, 0.8666666746139526, 0.8444444537162781, 0.8444444537162781, 0.8222222328186035, 0.7777777910232544, 0.8222222328186035, 0.8222222328186035, 0.8444444537162781, 0.8222222328186035, 0.8444444537162781, 0.8888888955116272, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8888888955116272, 0.8888888955116272, 0.9111111164093018, 0.9111111164093018, 0.8888888955116272, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8888888955116272, 0.8666666746139526, 0.8444444537162781, 0.8444444537162781, 0.8222222328186035, 0.8222222328186035, 0.7777777910232544, 0.7777777910232544, 0.8222222328186035, 0.8444444537162781, 0.8666666746139526, 0.8444444537162781, 0.8444444537162781, 0.8222222328186035, 0.8666666746139526, 0.8666666746139526, 0.8444444537162781, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8444444537162781, 0.8444444537162781, 0.8444444537162781, 0.7777777910232544, 0.7555555701255798, 0.7555555701255798, 0.800000011920929, 0.8222222328186035, 0.800000011920929, 0.7777777910232544, 0.7333333492279053, 0.6888889074325562, 0.6666666865348816, 0.6666666865348816, 0.6666666865348816, 0.6888889074325562, 0.6666666865348816, 0.6888889074325562, 0.6888889074325562, 0.7333333492279053, 0.8222222328186035, 0.8222222328186035, 0.800000011920929, 0.800000011920929, 0.7777777910232544, 0.7555555701255798, 0.7555555701255798, 0.7555555701255798, 0.8222222328186035, 0.8222222328186035, 0.8222222328186035, 0.8222222328186035, 0.8444444537162781, 0.8666666746139526, 0.8666666746139526, 0.8888888955116272, 0.8888888955116272, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8444444537162781, 0.8222222328186035, 0.8444444537162781, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8444444537162781, 0.8666666746139526, 0.8444444537162781, 0.8444444537162781, 0.24444444477558136, 0.46666666865348816, 0.5333333611488342, 0.5777778029441833, 0.5777778029441833, 0.644444465637207, 0.7111111283302307, 0.7555555701255798, 0.7777777910232544, 0.800000011920929, 0.8222222328186035, 0.8444444537162781, 0.8444444537162781, 0.8444444537162781, 0.8444444537162781, 0.8444444537162781, 0.8222222328186035, 0.800000011920929, 0.800000011920929, 0.8222222328186035, 0.8444444537162781, 0.8666666746139526, 0.8444444537162781, 0.8444444537162781, 0.8222222328186035, 0.8222222328186035, 0.8222222328186035, 0.8222222328186035, 0.7777777910232544, 0.7777777910232544, 0.7777777910232544, 0.7777777910232544, 0.800000011920929, 0.7777777910232544, 0.7777777910232544, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8444444537162781, 0.8444444537162781, 0.8444444537162781, 0.8666666746139526, 0.8666666746139526, 0.8444444537162781, 0.8666666746139526, 0.8888888955116272, 0.8888888955116272, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.9111111164093018, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8444444537162781, 0.8222222328186035, 0.8222222328186035, 0.8444444537162781, 0.8444444537162781, 0.8444444537162781, 0.8444444537162781, 0.8666666746139526, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8666666746139526, 0.8444444537162781, 0.8444444537162781, 0.8444444537162781, 0.8444444537162781, 0.8222222328186035, 0.8222222328186035, 0.8222222328186035, 0.8444444537162781, 0.8444444537162781, 0.8666666746139526, 0.8444444537162781, 0.8444444537162781, 0.8444444537162781, 0.8666666746139526, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.9111111164093018, 0.9111111164093018, 0.9111111164093018, 0.9111111164093018, 0.9111111164093018, 0.9111111164093018, 0.9111111164093018, 0.9111111164093018, 0.9111111164093018, 0.9111111164093018, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.9111111164093018, 0.9111111164093018, 0.9111111164093018, 0.8666666746139526, 0.8666666746139526, 0.8888888955116272, 0.8666666746139526, 0.8888888955116272, 0.8666666746139526, 0.8444444537162781, 0.8666666746139526, 0.8666666746139526, 0.8888888955116272, 0.8888888955116272, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8222222328186035, 0.8222222328186035, 0.8222222328186035, 0.8222222328186035, 0.8222222328186035, 0.8444444537162781, 0.8444444537162781, 0.8444444537162781, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8444444537162781, 0.8444444537162781, 0.800000011920929, 0.800000011920929, 0.800000011920929, 0.8222222328186035, 0.8666666746139526, 0.8888888955116272, 0.8888888955116272, 0.9111111164093018, 0.9111111164093018, 0.9111111164093018, 0.9111111164093018, 0.9111111164093018, 0.9111111164093018, 0.9111111164093018, 0.9111111164093018, 0.9111111164093018, 0.9111111164093018, 0.9111111164093018, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.800000011920929, 0.8222222328186035, 0.800000011920929, 0.800000011920929, 0.800000011920929, 0.7777777910232544, 0.7555555701255798, 0.7333333492279053, 0.7111111283302307, 0.7555555701255798, 0.7777777910232544, 0.800000011920929, 0.7777777910232544, 0.800000011920929, 0.8222222328186035, 0.8444444537162781, 0.8444444537162781, 0.8444444537162781, 0.8222222328186035, 0.8222222328186035, 0.8444444537162781, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8444444537162781, 0.8444444537162781, 0.8444444537162781, 0.8444444537162781, 0.8222222328186035, 0.8444444537162781, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8444444537162781, 0.8666666746139526, 0.9111111164093018, 0.9111111164093018, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8666666746139526, 0.8444444537162781, 0.8444444537162781, 0.8888888955116272, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.9111111164093018, 0.9333333373069763, 0.9111111164093018, 0.8888888955116272, 0.8888888955116272, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8666666746139526, 0.8666666746139526, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.9111111164093018, 0.9111111164093018, 0.9111111164093018, 0.9111111164093018, 0.9111111164093018, 0.9111111164093018]}\n",
    "\n",
    "#exp3\n",
    "\n",
    "import tensorflow\n",
    "\n",
    "from keras.layers import Dense,Flatten\n",
    "\n",
    "from keras.applications.vgg16 import VGG16\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "from keras import Sequential\n",
    "\n",
    "from keras.layers import Dense,Conv2D,MaxPooling2D,Flatten,BatchNormalization,Dropout\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam,SGD,RMSprop\n",
    "\n",
    "from tensorflow.keras.layers import Dense,Input,Dropout,GlobalAveragePooling2D,Flatten,Conv2D,BatchNormalization,Activation,MaxPooling2D\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Flatten, BatchNormalization, Dropout\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from tensorflow.keras.applications.vgg19 import VGG19\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Importing Deep Learning Libraries\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "from keras import regularizers\n",
    "\n",
    "from keras.preprocessing import image\n",
    "\n",
    "​\n",
    "\n",
    "#HISTOGRAM CODE\n",
    "\n",
    "#histogram code\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "\n",
    "​\n",
    "\n",
    "emotions = [\"happy\", \"sadness\", \"anger\", \"disgust\", \"neutral\", \"fear\", \"surprise\"]\n",
    "\n",
    "​\n",
    "\n",
    "folder_path = \"Jaffetrainvalidation/train\"\n",
    "\n",
    "# Counting the number of images per emotion\n",
    "\n",
    "counts = [len(os.listdir(os.path.join(folder_path, emotion))) for emotion in emotions]\n",
    "\n",
    "​\n",
    "\n",
    "# Plotting the bar chart\n",
    "\n",
    "colors = ['red', 'yellow', 'black', 'blue', 'orange', 'green', 'pink']\n",
    "\n",
    "plt.bar(emotions, height=counts, color=colors)\n",
    "\n",
    "plt.ylabel('Number')\n",
    "\n",
    "plt.xlabel('Emotions')\n",
    "\n",
    "​\n",
    "\n",
    "​\n",
    "\n",
    "#plt.savefig('hostgoarm.png')\n",
    "\n",
    "#plt.show()\n",
    "\n",
    "​\n",
    "\n",
    "​\n",
    "\n",
    "# Data generators\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "​\n",
    "\n",
    "# Data augmentation for training set\n",
    "\n",
    "datagen_train = ImageDataGenerator(\n",
    "\n",
    "    rescale=1./255,\n",
    "\n",
    "    rotation_range=15,\n",
    "\n",
    "    width_shift_range=0.1,\n",
    "\n",
    "    height_shift_range=0.1,\n",
    "\n",
    "    shear_range=0.2,\n",
    "\n",
    "    zoom_range=0.2,\n",
    "\n",
    "    horizontal_flip=True,\n",
    "\n",
    "    fill_mode='nearest'\n",
    "\n",
    ")\n",
    "\n",
    "​\n",
    "\n",
    "# Normalization for validation set (no augmentation)\n",
    "\n",
    "datagen_val = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "​\n",
    "\n",
    "​\n",
    "\n",
    "​\n",
    "\n",
    "train_ds = datagen_train.flow_from_directory(\"Jaffetrainvalidation/train\",\n",
    "\n",
    "                                             target_size=(256, 256),\n",
    "\n",
    "                                             color_mode=\"rgb\",\n",
    "\n",
    "                                             batch_size=batch_size,\n",
    "\n",
    "                                             class_mode='categorical',\n",
    "\n",
    "                                             shuffle=True)\n",
    "\n",
    "​\n",
    "\n",
    "test_ds = datagen_val.flow_from_directory(\"Jaffetrainvalidation/validation\",\n",
    "\n",
    "                                         target_size=(256, 256),\n",
    "\n",
    "                                         color_mode=\"rgb\",\n",
    "\n",
    "                                         batch_size=batch_size,\n",
    "\n",
    "                                         class_mode='categorical',\n",
    "\n",
    "                                         shuffle=False)\n",
    "\n",
    "print('Train and Validation sets have been created.')\n",
    "\n",
    "​\n",
    "\n",
    "​\n",
    "\n",
    "#model vgg19\n",
    "\n",
    "​\n",
    "\n",
    "from keras.applications.vgg19 import VGG19\n",
    "\n",
    "conv_base = VGG19(weights='imagenet', include_top=False, input_shape=(256, 256, 3))\n",
    "\n",
    "conv_base.summary()\n",
    "\n",
    "​\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(conv_base)\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(256,activation = 'relu'))\n",
    "\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dropout(0.30))\n",
    "\n",
    "# Second fully connected layer  \n",
    "\n",
    "model.add(Dense(512,activation = 'relu'))\n",
    "\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dropout(0.30))\n",
    "\n",
    "​\n",
    "\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "​\n",
    "\n",
    "# model.summary()\n",
    "\n",
    "# Visualize the model.\n",
    "\n",
    "#plot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True)\n",
    "\n",
    "​\n",
    "\n",
    "conv_base.trainable = False\n",
    "\n",
    "​\n",
    "\n",
    "model.compile(optimizer = Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "​\n",
    "\n",
    "print('CNN model has been created you can proceed to train you data with this model.')\n",
    "\n",
    "​\n",
    "\n",
    "​\n",
    "\n",
    "​\n",
    "\n",
    "​\n",
    "\n",
    "# Training the model\n",
    "\n",
    "epochs = 500\n",
    "\n",
    "​\n",
    "\n",
    "history = model.fit(x=train_ds,\n",
    "\n",
    "                    epochs=epochs,\n",
    "\n",
    "                    validation_data=test_ds)\n",
    "\n",
    "​\n",
    "\n",
    "# Print training history\n",
    "\n",
    "print(history.history)\n",
    "\n",
    "​\n",
    "\n",
    "# Plotting training history\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "​\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "\n",
    "plt.title('Loss')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "​\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "\n",
    "plt.title('Accuracy')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "Found 168 images belonging to 7 classes.\n",
    "Found 45 images belonging to 7 classes.\n",
    "Train and Validation sets have been created.\n",
    "Model: \"vgg19\"\n",
    "_________________________________________________________________\n",
    " Layer (type)                Output Shape              Param #   \n",
    "=================================================================\n",
    " input_4 (InputLayer)        [(None, 256, 256, 3)]     0         \n",
    "                                                                 \n",
    " block1_conv1 (Conv2D)       (None, 256, 256, 64)      1792      \n",
    "                                                                 \n",
    " block1_conv2 (Conv2D)       (None, 256, 256, 64)      36928     \n",
    "                                                                 \n",
    " block1_pool (MaxPooling2D)  (None, 128, 128, 64)      0         \n",
    "                                                                 \n",
    " block2_conv1 (Conv2D)       (None, 128, 128, 128)     73856     \n",
    "                                                                 \n",
    " block2_conv2 (Conv2D)       (None, 128, 128, 128)     147584    \n",
    "                                                                 \n",
    " block2_pool (MaxPooling2D)  (None, 64, 64, 128)       0         \n",
    "                                                                 \n",
    " block3_conv1 (Conv2D)       (None, 64, 64, 256)       295168    \n",
    "                                                                 \n",
    " block3_conv2 (Conv2D)       (None, 64, 64, 256)       590080    \n",
    "                                                                 \n",
    " block3_conv3 (Conv2D)       (None, 64, 64, 256)       590080    \n",
    "                                                                 \n",
    " block3_conv4 (Conv2D)       (None, 64, 64, 256)       590080    \n",
    "                                                                 \n",
    " block3_pool (MaxPooling2D)  (None, 32, 32, 256)       0         \n",
    "                                                                 \n",
    " block4_conv1 (Conv2D)       (None, 32, 32, 512)       1180160   \n",
    "                                                                 \n",
    " block4_conv2 (Conv2D)       (None, 32, 32, 512)       2359808   \n",
    "                                                                 \n",
    " block4_conv3 (Conv2D)       (None, 32, 32, 512)       2359808   \n",
    "                                                                 \n",
    " block4_conv4 (Conv2D)       (None, 32, 32, 512)       2359808   \n",
    "                                                                 \n",
    " block4_pool (MaxPooling2D)  (None, 16, 16, 512)       0         \n",
    "                                                                 \n",
    " block5_conv1 (Conv2D)       (None, 16, 16, 512)       2359808   \n",
    "                                                                 \n",
    " block5_conv2 (Conv2D)       (None, 16, 16, 512)       2359808   \n",
    "                                                                 \n",
    " block5_conv3 (Conv2D)       (None, 16, 16, 512)       2359808   \n",
    "                                                                 \n",
    " block5_conv4 (Conv2D)       (None, 16, 16, 512)       2359808   \n",
    "                                                                 \n",
    " block5_pool (MaxPooling2D)  (None, 8, 8, 512)         0         \n",
    "                                                                 \n",
    "=================================================================\n",
    "Total params: 20024384 (76.39 MB)\n",
    "Trainable params: 20024384 (76.39 MB)\n",
    "Non-trainable params: 0 (0.00 Byte)\n",
    "_________________________________________________________________\n",
    "CNN model has been created you can proceed to train you data with this model.\n",
    "Epoch 1/500\n",
    "3/3 [==============================] - 3s 576ms/step - loss: 3.1598 - accuracy: 0.1429 - val_loss: 3.4972 - val_accuracy: 0.2000\n",
    "Epoch 2/500\n",
    "3/3 [==============================] - 2s 447ms/step - loss: 2.4358 - accuracy: 0.2619 - val_loss: 4.7518 - val_accuracy: 0.1333\n",
    "Epoch 3/500\n",
    "3/3 [==============================] - 2s 613ms/step - loss: 1.9867 - accuracy: 0.3512 - val_loss: 6.6971 - val_accuracy: 0.1333\n",
    "Epoch 4/500\n",
    "3/3 [==============================] - 2s 474ms/step - loss: 2.0522 - accuracy: 0.3095 - val_loss: 7.9068 - val_accuracy: 0.1333\n",
    "Epoch 5/500\n",
    "3/3 [==============================] - 2s 473ms/step - loss: 2.0507 - accuracy: 0.3690 - val_loss: 8.5534 - val_accuracy: 0.1333\n",
    "Epoch 6/500\n",
    "3/3 [==============================] - 2s 602ms/step - loss: 1.5487 - accuracy: 0.4286 - val_loss: 8.1810 - val_accuracy: 0.1333\n",
    "Epoch 7/500\n",
    "3/3 [==============================] - 2s 601ms/step - loss: 1.7621 - accuracy: 0.4048 - val_loss: 7.9538 - val_accuracy: 0.1333\n",
    "Epoch 8/500\n",
    "3/3 [==============================] - 2s 608ms/step - loss: 1.3705 - accuracy: 0.4643 - val_loss: 8.1091 - val_accuracy: 0.1333\n",
    "Epoch 9/500\n",
    "3/3 [==============================] - 2s 618ms/step - loss: 1.7724 - accuracy: 0.3750 - val_loss: 7.9859 - val_accuracy: 0.1333\n",
    "Epoch 10/500\n",
    "3/3 [==============================] - 2s 451ms/step - loss: 1.4400 - accuracy: 0.4643 - val_loss: 7.8434 - val_accuracy: 0.1333\n",
    "Epoch 11/500\n",
    "3/3 [==============================] - 2s 454ms/step - loss: 1.3837 - accuracy: 0.4464 - val_loss: 7.4510 - val_accuracy: 0.1333\n",
    "Epoch 12/500\n",
    "3/3 [==============================] - 2s 470ms/step - loss: 1.2711 - accuracy: 0.4881 - val_loss: 7.0009 - val_accuracy: 0.1333\n",
    "Epoch 13/500\n",
    "3/3 [==============================] - 2s 446ms/step - loss: 1.4235 - accuracy: 0.4881 - val_loss: 6.6313 - val_accuracy: 0.1333\n",
    "Epoch 14/500\n",
    "3/3 [==============================] - 2s 471ms/step - loss: 1.2500 - accuracy: 0.5417 - val_loss: 6.2158 - val_accuracy: 0.1333\n",
    "Epoch 15/500\n",
    "3/3 [==============================] - 2s 611ms/step - loss: 1.2451 - accuracy: 0.5298 - val_loss: 6.2431 - val_accuracy: 0.1333\n",
    "Epoch 16/500\n",
    "3/3 [==============================] - 2s 451ms/step - loss: 1.1545 - accuracy: 0.6071 - val_loss: 6.6600 - val_accuracy: 0.1333\n",
    "Epoch 17/500\n",
    "3/3 [==============================] - 2s 473ms/step - loss: 1.0437 - accuracy: 0.5833 - val_loss: 6.8654 - val_accuracy: 0.1333\n",
    "Epoch 18/500\n",
    "3/3 [==============================] - 2s 449ms/step - loss: 1.1108 - accuracy: 0.5952 - val_loss: 6.8680 - val_accuracy: 0.1333\n",
    "Epoch 19/500\n",
    "3/3 [==============================] - 2s 600ms/step - loss: 0.9771 - accuracy: 0.6667 - val_loss: 6.5336 - val_accuracy: 0.1333\n",
    "Epoch 20/500\n",
    "3/3 [==============================] - 2s 472ms/step - loss: 0.9718 - accuracy: 0.6488 - val_loss: 6.0839 - val_accuracy: 0.1333\n",
    "Epoch 21/500\n",
    "3/3 [==============================] - 2s 453ms/step - loss: 0.9760 - accuracy: 0.6369 - val_loss: 5.6344 - val_accuracy: 0.1333\n",
    "Epoch 22/500\n",
    "3/3 [==============================] - 2s 594ms/step - loss: 0.9271 - accuracy: 0.6429 - val_loss: 5.0961 - val_accuracy: 0.1778\n",
    "Epoch 23/500\n",
    "3/3 [==============================] - 2s 454ms/step - loss: 0.8374 - accuracy: 0.6964 - val_loss: 4.6044 - val_accuracy: 0.2000\n",
    "Epoch 24/500\n",
    "3/3 [==============================] - 2s 474ms/step - loss: 0.8008 - accuracy: 0.6845 - val_loss: 4.0270 - val_accuracy: 0.2000\n",
    "Epoch 25/500\n",
    "3/3 [==============================] - 2s 448ms/step - loss: 0.8122 - accuracy: 0.7202 - val_loss: 4.0109 - val_accuracy: 0.2000\n",
    "Epoch 26/500\n",
    "3/3 [==============================] - 2s 449ms/step - loss: 0.7367 - accuracy: 0.7619 - val_loss: 3.8289 - val_accuracy: 0.2000\n",
    "Epoch 27/500\n",
    "3/3 [==============================] - 2s 453ms/step - loss: 0.7986 - accuracy: 0.6964 - val_loss: 3.5833 - val_accuracy: 0.2000\n",
    "Epoch 28/500\n",
    "3/3 [==============================] - 2s 450ms/step - loss: 0.7223 - accuracy: 0.7321 - val_loss: 3.7430 - val_accuracy: 0.2000\n",
    "Epoch 29/500\n",
    "3/3 [==============================] - 2s 588ms/step - loss: 0.6804 - accuracy: 0.7321 - val_loss: 3.9204 - val_accuracy: 0.2000\n",
    "Epoch 30/500\n",
    "3/3 [==============================] - 2s 593ms/step - loss: 0.7178 - accuracy: 0.7083 - val_loss: 4.4543 - val_accuracy: 0.1778\n",
    "Epoch 31/500\n",
    "3/3 [==============================] - 2s 453ms/step - loss: 0.6619 - accuracy: 0.7619 - val_loss: 4.8095 - val_accuracy: 0.2000\n",
    "Epoch 32/500\n",
    "3/3 [==============================] - 2s 476ms/step - loss: 0.6173 - accuracy: 0.7679 - val_loss: 4.8255 - val_accuracy: 0.1778\n",
    "Epoch 33/500\n",
    "3/3 [==============================] - 2s 474ms/step - loss: 0.5106 - accuracy: 0.8095 - val_loss: 4.8144 - val_accuracy: 0.1778\n",
    "\n",
    "Epoch 34/500\n",
    "3/3 [==============================] - 2s 449ms/step - loss: 0.6015 - accuracy: 0.7619 - val_loss: 4.5199 - val_accuracy: 0.2000\n",
    "Epoch 35/500\n",
    "3/3 [==============================] - 2s 449ms/step - loss: 0.5114 - accuracy: 0.8095 - val_loss: 4.0941 - val_accuracy: 0.2222\n",
    "Epoch 36/500\n",
    "3/3 [==============================] - 2s 595ms/step - loss: 0.6078 - accuracy: 0.7619 - val_loss: 3.9722 - val_accuracy: 0.2444\n",
    "Epoch 37/500\n",
    "3/3 [==============================] - 2s 448ms/step - loss: 0.5512 - accuracy: 0.7976 - val_loss: 3.8250 - val_accuracy: 0.2444\n",
    "Epoch 38/500\n",
    "3/3 [==============================] - 2s 473ms/step - loss: 0.5101 - accuracy: 0.7857 - val_loss: 3.8152 - val_accuracy: 0.2444\n",
    "Epoch 39/500\n",
    "3/3 [==============================] - 2s 480ms/step - loss: 0.4564 - accuracy: 0.8571 - val_loss: 3.8143 - val_accuracy: 0.2667\n",
    "Epoch 40/500\n",
    "3/3 [==============================] - 2s 596ms/step - loss: 0.5609 - accuracy: 0.8274 - val_loss: 3.4832 - val_accuracy: 0.2667\n",
    "Epoch 41/500\n",
    "3/3 [==============================] - 2s 446ms/step - loss: 0.4009 - accuracy: 0.8452 - val_loss: 3.2251 - val_accuracy: 0.2889\n",
    "Epoch 42/500\n",
    "3/3 [==============================] - 2s 593ms/step - loss: 0.5334 - accuracy: 0.7798 - val_loss: 3.1296 - val_accuracy: 0.3111\n",
    "Epoch 43/500\n",
    "3/3 [==============================] - 2s 448ms/step - loss: 0.4488 - accuracy: 0.8393 - val_loss: 3.0698 - val_accuracy: 0.3111\n",
    "Epoch 44/500\n",
    "3/3 [==============================] - 2s 448ms/step - loss: 0.4446 - accuracy: 0.8393 - val_loss: 3.1331 - val_accuracy: 0.3333\n",
    "Epoch 45/500\n",
    "3/3 [==============================] - 2s 469ms/step - loss: 0.4328 - accuracy: 0.8571 - val_loss: 3.1552 - val_accuracy: 0.3333\n",
    "Epoch 46/500\n",
    "3/3 [==============================] - 2s 594ms/step - loss: 0.4531 - accuracy: 0.7976 - val_loss: 3.3593 - val_accuracy: 0.2889\n",
    "Epoch 47/500\n",
    "3/3 [==============================] - 2s 467ms/step - loss: 0.3614 - accuracy: 0.8512 - val_loss: 3.6245 - val_accuracy: 0.2889\n",
    "Epoch 48/500\n",
    "3/3 [==============================] - 2s 590ms/step - loss: 0.3358 - accuracy: 0.8988 - val_loss: 3.1806 - val_accuracy: 0.3111\n",
    "Epoch 49/500\n",
    "3/3 [==============================] - 2s 463ms/step - loss: 0.3791 - accuracy: 0.8512 - val_loss: 2.6613 - val_accuracy: 0.3556\n",
    "Epoch 50/500\n",
    "3/3 [==============================] - 2s 444ms/step - loss: 0.3535 - accuracy: 0.8452 - val_loss: 2.2639 - val_accuracy: 0.4222\n",
    "Epoch 51/500\n",
    "3/3 [==============================] - 2s 454ms/step - loss: 0.4505 - accuracy: 0.8274 - val_loss: 1.9437 - val_accuracy: 0.4667\n",
    "Epoch 52/500\n",
    "3/3 [==============================] - 2s 449ms/step - loss: 0.4328 - accuracy: 0.8452 - val_loss: 1.5984 - val_accuracy: 0.5556\n",
    "Epoch 53/500\n",
    "3/3 [==============================] - 2s 593ms/step - loss: 0.4179 - accuracy: 0.8810 - val_loss: 1.6801 - val_accuracy: 0.5556\n",
    "Epoch 54/500\n",
    "3/3 [==============================] - 2s 592ms/step - loss: 0.4075 - accuracy: 0.8631 - val_loss: 1.9436 - val_accuracy: 0.4222\n",
    "Epoch 55/500\n",
    "3/3 [==============================] - 2s 448ms/step - loss: 0.3771 - accuracy: 0.8690 - val_loss: 1.9707 - val_accuracy: 0.4444\n",
    "Epoch 56/500\n",
    "3/3 [==============================] - 2s 468ms/step - loss: 0.3397 - accuracy: 0.8690 - val_loss: 2.0094 - val_accuracy: 0.4444\n",
    "Epoch 57/500\n",
    "3/3 [==============================] - 2s 589ms/step - loss: 0.3318 - accuracy: 0.8810 - val_loss: 2.0351 - val_accuracy: 0.4444\n",
    "Epoch 58/500\n",
    "3/3 [==============================] - 2s 450ms/step - loss: 0.3536 - accuracy: 0.8810 - val_loss: 1.9892 - val_accuracy: 0.4667\n",
    "Epoch 59/500\n",
    "3/3 [==============================] - 2s 452ms/step - loss: 0.2804 - accuracy: 0.8750 - val_loss: 1.9570 - val_accuracy: 0.4889\n",
    "Epoch 60/500\n",
    "3/3 [==============================] - 2s 475ms/step - loss: 0.3311 - accuracy: 0.8869 - val_loss: 1.6252 - val_accuracy: 0.5111\n",
    "Epoch 61/500\n",
    "3/3 [==============================] - 2s 598ms/step - loss: 0.2660 - accuracy: 0.9167 - val_loss: 1.3834 - val_accuracy: 0.5556\n",
    "Epoch 62/500\n",
    "3/3 [==============================] - 2s 587ms/step - loss: 0.3706 - accuracy: 0.8631 - val_loss: 1.1255 - val_accuracy: 0.6000\n",
    "Epoch 63/500\n",
    "3/3 [==============================] - 2s 472ms/step - loss: 0.3100 - accuracy: 0.8571 - val_loss: 0.9384 - val_accuracy: 0.6889\n",
    "Epoch 64/500\n",
    "3/3 [==============================] - 2s 473ms/step - loss: 0.2146 - accuracy: 0.9167 - val_loss: 0.7757 - val_accuracy: 0.7111\n",
    "Epoch 65/500\n",
    "3/3 [==============================] - 2s 596ms/step - loss: 0.2541 - accuracy: 0.8929 - val_loss: 0.7205 - val_accuracy: 0.7778\n",
    "Epoch 66/500\n",
    "3/3 [==============================] - 2s 475ms/step - loss: 0.4260 - accuracy: 0.8571 - val_loss: 0.7689 - val_accuracy: 0.7333\n",
    "Epoch 67/500\n",
    "3/3 [==============================] - 2s 476ms/step - loss: 0.3281 - accuracy: 0.8988 - val_loss: 0.8877 - val_accuracy: 0.7333\n",
    "Epoch 68/500\n",
    "3/3 [==============================] - 2s 476ms/step - loss: 0.2802 - accuracy: 0.8988 - val_loss: 0.8807 - val_accuracy: 0.7111\n",
    "Epoch 69/500\n",
    "3/3 [==============================] - 2s 595ms/step - loss: 0.2758 - accuracy: 0.8810 - val_loss: 0.9771 - val_accuracy: 0.7333\n",
    "Epoch 70/500\n",
    "3/3 [==============================] - 2s 589ms/step - loss: 0.2851 - accuracy: 0.8988 - val_loss: 1.0397 - val_accuracy: 0.7333\n",
    "Epoch 71/500\n",
    "3/3 [==============================] - 2s 470ms/step - loss: 0.3438 - accuracy: 0.8690 - val_loss: 1.0283 - val_accuracy: 0.7556\n",
    "Epoch 72/500\n",
    "3/3 [==============================] - 2s 592ms/step - loss: 0.2495 - accuracy: 0.9107 - val_loss: 1.0731 - val_accuracy: 0.7333\n",
    "Epoch 73/500\n",
    "3/3 [==============================] - 2s 451ms/step - loss: 0.2842 - accuracy: 0.9048 - val_loss: 1.0637 - val_accuracy: 0.7333\n",
    "Epoch 74/500\n",
    "3/3 [==============================] - 2s 592ms/step - loss: 0.3551 - accuracy: 0.8690 - val_loss: 1.0698 - val_accuracy: 0.6889\n",
    "Epoch 75/500\n",
    "3/3 [==============================] - 2s 603ms/step - loss: 0.2141 - accuracy: 0.9167 - val_loss: 1.0765 - val_accuracy: 0.7111\n",
    "Epoch 76/500\n",
    "3/3 [==============================] - 2s 471ms/step - loss: 0.3282 - accuracy: 0.8929 - val_loss: 1.0084 - val_accuracy: 0.7333\n",
    "Epoch 77/500\n",
    "3/3 [==============================] - 2s 464ms/step - loss: 0.2102 - accuracy: 0.9226 - val_loss: 0.9577 - val_accuracy: 0.7778\n",
    "Epoch 78/500\n",
    "3/3 [==============================] - 2s 470ms/step - loss: 0.1897 - accuracy: 0.9345 - val_loss: 0.9522 - val_accuracy: 0.7778\n",
    "Epoch 79/500\n",
    "3/3 [==============================] - 2s 453ms/step - loss: 0.2185 - accuracy: 0.9286 - val_loss: 1.0140 - val_accuracy: 0.8000\n",
    "Epoch 80/500\n",
    "3/3 [==============================] - 2s 453ms/step - loss: 0.2923 - accuracy: 0.8988 - val_loss: 1.0559 - val_accuracy: 0.7556\n",
    "Epoch 81/500\n",
    "3/3 [==============================] - 2s 469ms/step - loss: 0.4367 - accuracy: 0.8869 - val_loss: 1.0316 - val_accuracy: 0.7111\n",
    "Epoch 82/500\n",
    "3/3 [==============================] - 2s 473ms/step - loss: 0.1733 - accuracy: 0.9345 - val_loss: 0.9781 - val_accuracy: 0.7333\n",
    "Epoch 83/500\n",
    "3/3 [==============================] - 2s 451ms/step - loss: 0.3289 - accuracy: 0.8929 - val_loss: 0.9262 - val_accuracy: 0.7778\n",
    "Epoch 84/500\n",
    "3/3 [==============================] - 2s 453ms/step - loss: 0.2534 - accuracy: 0.8988 - val_loss: 0.8627 - val_accuracy: 0.8222\n",
    "Epoch 85/500\n",
    "3/3 [==============================] - 2s 461ms/step - loss: 0.2004 - accuracy: 0.9464 - val_loss: 0.8328 - val_accuracy: 0.8444\n",
    "Epoch 86/500\n",
    "3/3 [==============================] - 2s 467ms/step - loss: 0.2038 - accuracy: 0.9405 - val_loss: 0.8214 - val_accuracy: 0.8222\n",
    "Epoch 87/500\n",
    "3/3 [==============================] - 2s 596ms/step - loss: 0.3009 - accuracy: 0.8869 - val_loss: 0.8005 - val_accuracy: 0.8222\n",
    "Epoch 88/500\n",
    "3/3 [==============================] - 2s 474ms/step - loss: 0.3401 - accuracy: 0.8810 - val_loss: 0.8158 - val_accuracy: 0.7778\n",
    "Epoch 89/500\n",
    "3/3 [==============================] - 2s 476ms/step - loss: 0.2004 - accuracy: 0.9286 - val_loss: 0.8697 - val_accuracy: 0.7778\n",
    "Epoch 90/500\n",
    "3/3 [==============================] - 2s 476ms/step - loss: 0.2559 - accuracy: 0.9167 - val_loss: 0.9602 - val_accuracy: 0.7556\n",
    "Epoch 91/500\n",
    "3/3 [==============================] - 2s 463ms/step - loss: 0.2120 - accuracy: 0.9405 - val_loss: 1.0819 - val_accuracy: 0.7333\n",
    "\n",
    "Epoch 92/500\n",
    "3/3 [==============================] - 2s 589ms/step - loss: 0.3490 - accuracy: 0.8869 - val_loss: 1.2215 - val_accuracy: 0.7111\n",
    "Epoch 93/500\n",
    "3/3 [==============================] - 2s 463ms/step - loss: 0.1703 - accuracy: 0.9345 - val_loss: 1.2794 - val_accuracy: 0.7111\n",
    "Epoch 94/500\n",
    "3/3 [==============================] - 2s 452ms/step - loss: 0.2277 - accuracy: 0.9167 - val_loss: 1.2466 - val_accuracy: 0.7111\n",
    "Epoch 95/500\n",
    "3/3 [==============================] - 2s 598ms/step - loss: 0.3225 - accuracy: 0.8810 - val_loss: 1.2225 - val_accuracy: 0.7333\n",
    "Epoch 96/500\n",
    "3/3 [==============================] - 2s 469ms/step - loss: 0.3111 - accuracy: 0.9048 - val_loss: 1.3834 - val_accuracy: 0.6889\n",
    "Epoch 97/500\n",
    "3/3 [==============================] - 2s 588ms/step - loss: 0.2189 - accuracy: 0.9286 - val_loss: 1.4762 - val_accuracy: 0.7111\n",
    "Epoch 98/500\n",
    "3/3 [==============================] - 2s 597ms/step - loss: 0.1420 - accuracy: 0.9524 - val_loss: 1.4381 - val_accuracy: 0.7333\n",
    "Epoch 99/500\n",
    "3/3 [==============================] - 2s 481ms/step - loss: 0.2279 - accuracy: 0.9345 - val_loss: 1.1877 - val_accuracy: 0.7556\n",
    "Epoch 100/500\n",
    "3/3 [==============================] - 2s 467ms/step - loss: 0.2986 - accuracy: 0.9107 - val_loss: 0.9736 - val_accuracy: 0.7778\n",
    "Epoch 101/500\n",
    "3/3 [==============================] - 2s 477ms/step - loss: 0.2051 - accuracy: 0.9345 - val_loss: 0.8966 - val_accuracy: 0.8222\n",
    "Epoch 102/500\n",
    "3/3 [==============================] - 2s 596ms/step - loss: 0.1921 - accuracy: 0.9167 - val_loss: 0.8997 - val_accuracy: 0.8000\n",
    "Epoch 103/500\n",
    "3/3 [==============================] - 2s 455ms/step - loss: 0.2884 - accuracy: 0.9167 - val_loss: 1.0023 - val_accuracy: 0.8444\n",
    "Epoch 104/500\n",
    "3/3 [==============================] - 2s 467ms/step - loss: 0.1495 - accuracy: 0.9583 - val_loss: 1.1087 - val_accuracy: 0.8444\n",
    "Epoch 105/500\n",
    "3/3 [==============================] - 2s 587ms/step - loss: 0.3173 - accuracy: 0.8690 - val_loss: 1.2174 - val_accuracy: 0.8222\n",
    "Epoch 106/500\n",
    "3/3 [==============================] - 2s 473ms/step - loss: 0.2100 - accuracy: 0.9286 - val_loss: 1.2983 - val_accuracy: 0.8000\n",
    "Epoch 107/500\n",
    "3/3 [==============================] - 2s 468ms/step - loss: 0.1826 - accuracy: 0.9345 - val_loss: 1.2395 - val_accuracy: 0.8000\n",
    "Epoch 108/500\n",
    "3/3 [==============================] - 2s 593ms/step - loss: 0.2689 - accuracy: 0.8869 - val_loss: 1.2410 - val_accuracy: 0.8222\n",
    "Epoch 109/500\n",
    "3/3 [==============================] - 2s 470ms/step - loss: 0.2147 - accuracy: 0.9345 - val_loss: 1.2419 - val_accuracy: 0.8000\n",
    "Epoch 110/500\n",
    "3/3 [==============================] - 2s 445ms/step - loss: 0.1792 - accuracy: 0.9405 - val_loss: 1.1744 - val_accuracy: 0.7778\n",
    "Epoch 111/500\n",
    "3/3 [==============================] - 2s 597ms/step - loss: 0.2259 - accuracy: 0.9345 - val_loss: 1.1339 - val_accuracy: 0.8000\n",
    "Epoch 112/500\n",
    "3/3 [==============================] - 2s 598ms/step - loss: 0.2667 - accuracy: 0.9048 - val_loss: 1.1282 - val_accuracy: 0.8000\n",
    "Epoch 113/500\n",
    "3/3 [==============================] - 2s 467ms/step - loss: 0.3665 - accuracy: 0.8631 - val_loss: 1.1140 - val_accuracy: 0.8000\n",
    "Epoch 114/500\n",
    "3/3 [==============================] - 2s 477ms/step - loss: 0.2181 - accuracy: 0.9167 - val_loss: 1.0024 - val_accuracy: 0.8444\n",
    "Epoch 115/500\n",
    "3/3 [==============================] - 2s 452ms/step - loss: 0.1814 - accuracy: 0.9167 - val_loss: 1.0059 - val_accuracy: 0.8000\n",
    "Epoch 116/500\n",
    "3/3 [==============================] - 2s 451ms/step - loss: 0.1899 - accuracy: 0.9583 - val_loss: 1.0704 - val_accuracy: 0.8000\n",
    "Epoch 117/500\n",
    "3/3 [==============================] - 2s 595ms/step - loss: 0.2553 - accuracy: 0.8869 - val_loss: 1.0523 - val_accuracy: 0.7778\n",
    "Epoch 118/500\n",
    "3/3 [==============================] - 2s 593ms/step - loss: 0.2582 - accuracy: 0.9286 - val_loss: 0.9136 - val_accuracy: 0.8000\n",
    "Epoch 119/500\n",
    "3/3 [==============================] - 2s 595ms/step - loss: 0.1344 - accuracy: 0.9524 - val_loss: 0.8316 - val_accuracy: 0.8444\n",
    "Epoch 120/500\n",
    "3/3 [==============================] - 2s 469ms/step - loss: 0.1927 - accuracy: 0.9405 - val_loss: 0.8341 - val_accuracy: 0.8444\n",
    "Epoch 121/500\n",
    "3/3 [==============================] - 2s 456ms/step - loss: 0.2124 - accuracy: 0.9167 - val_loss: 0.9013 - val_accuracy: 0.8444\n",
    "Epoch 122/500\n",
    "3/3 [==============================] - 2s 594ms/step - loss: 0.2507 - accuracy: 0.9048 - val_loss: 0.9947 - val_accuracy: 0.8444\n",
    "Epoch 123/500\n",
    "3/3 [==============================] - 2s 447ms/step - loss: 0.3075 - accuracy: 0.9107 - val_loss: 1.0897 - val_accuracy: 0.8000\n",
    "Epoch 124/500\n",
    "3/3 [==============================] - 2s 448ms/step - loss: 0.2853 - accuracy: 0.9107 - val_loss: 1.1383 - val_accuracy: 0.8444\n",
    "Epoch 125/500\n",
    "3/3 [==============================] - 2s 448ms/step - loss: 0.1620 - accuracy: 0.9345 - val_loss: 1.1227 - val_accuracy: 0.8000\n",
    "Epoch 126/500\n",
    "3/3 [==============================] - 2s 587ms/step - loss: 0.2622 - accuracy: 0.8869 - val_loss: 1.1897 - val_accuracy: 0.8000\n",
    "Epoch 127/500\n",
    "3/3 [==============================] - 2s 458ms/step - loss: 0.2888 - accuracy: 0.8750 - val_loss: 1.2134 - val_accuracy: 0.8222\n",
    "Epoch 128/500\n",
    "3/3 [==============================] - 2s 585ms/step - loss: 0.2030 - accuracy: 0.9345 - val_loss: 1.1451 - val_accuracy: 0.8222\n",
    "Epoch 129/500\n",
    "3/3 [==============================] - 2s 450ms/step - loss: 0.1856 - accuracy: 0.9345 - val_loss: 1.1560 - val_accuracy: 0.8000\n",
    "Epoch 130/500\n",
    "3/3 [==============================] - 2s 449ms/step - loss: 0.2302 - accuracy: 0.8929 - val_loss: 1.1071 - val_accuracy: 0.8444\n",
    "Epoch 131/500\n",
    "3/3 [==============================] - 2s 593ms/step - loss: 0.1244 - accuracy: 0.9583 - val_loss: 1.0641 - val_accuracy: 0.8222\n",
    "Epoch 132/500\n",
    "3/3 [==============================] - 2s 468ms/step - loss: 0.0960 - accuracy: 0.9643 - val_loss: 1.0345 - val_accuracy: 0.8222\n",
    "Epoch 133/500\n",
    "3/3 [==============================] - 2s 452ms/step - loss: 0.1519 - accuracy: 0.9405 - val_loss: 0.9956 - val_accuracy: 0.8222\n",
    "Epoch 134/500\n",
    "3/3 [==============================] - 2s 471ms/step - loss: 0.2106 - accuracy: 0.9286 - val_loss: 0.9595 - val_accuracy: 0.8222\n",
    "Epoch 135/500\n",
    "3/3 [==============================] - 2s 595ms/step - loss: 0.1970 - accuracy: 0.9167 - val_loss: 0.9676 - val_accuracy: 0.8222\n",
    "Epoch 136/500\n",
    "3/3 [==============================] - 2s 450ms/step - loss: 0.1233 - accuracy: 0.9405 - val_loss: 0.9555 - val_accuracy: 0.8444\n",
    "Epoch 137/500\n",
    "3/3 [==============================] - 2s 472ms/step - loss: 0.2243 - accuracy: 0.9167 - val_loss: 0.8987 - val_accuracy: 0.8444\n",
    "Epoch 138/500\n",
    "3/3 [==============================] - 2s 467ms/step - loss: 0.1420 - accuracy: 0.9643 - val_loss: 0.8622 - val_accuracy: 0.8444\n",
    "Epoch 139/500\n",
    "3/3 [==============================] - 2s 461ms/step - loss: 0.2335 - accuracy: 0.9226 - val_loss: 0.8442 - val_accuracy: 0.8444\n",
    "Epoch 140/500\n",
    "3/3 [==============================] - 2s 470ms/step - loss: 0.1363 - accuracy: 0.9524 - val_loss: 0.8450 - val_accuracy: 0.8667\n",
    "Epoch 141/500\n",
    "3/3 [==============================] - 2s 596ms/step - loss: 0.1700 - accuracy: 0.9226 - val_loss: 0.8766 - val_accuracy: 0.8667\n",
    "Epoch 142/500\n",
    "3/3 [==============================] - 2s 453ms/step - loss: 0.1538 - accuracy: 0.9583 - val_loss: 0.8776 - val_accuracy: 0.8667\n",
    "Epoch 143/500\n",
    "3/3 [==============================] - 2s 602ms/step - loss: 0.1014 - accuracy: 0.9643 - val_loss: 0.9149 - val_accuracy: 0.8222\n",
    "Epoch 144/500\n",
    "3/3 [==============================] - 2s 471ms/step - loss: 0.1580 - accuracy: 0.9524 - val_loss: 1.0050 - val_accuracy: 0.8222\n",
    "Epoch 145/500\n",
    "3/3 [==============================] - 2s 452ms/step - loss: 0.2035 - accuracy: 0.9464 - val_loss: 1.1476 - val_accuracy: 0.8000\n",
    "Epoch 146/500\n",
    "3/3 [==============================] - 2s 469ms/step - loss: 0.1323 - accuracy: 0.9405 - val_loss: 1.3951 - val_accuracy: 0.7778\n",
    "Epoch 147/500\n",
    "3/3 [==============================] - 2s 445ms/step - loss: 0.1680 - accuracy: 0.9524 - val_loss: 1.5618 - val_accuracy: 0.7556\n",
    "Epoch 148/500\n",
    "3/3 [==============================] - 2s 476ms/step - loss: 0.1749 - accuracy: 0.9345 - val_loss: 1.4915 - val_accuracy: 0.7556\n",
    "Epoch 149/500\n",
    "\n",
    "3/3 [==============================] - 2s 591ms/step - loss: 0.1699 - accuracy: 0.9286 - val_loss: 1.3854 - val_accuracy: 0.7333\n",
    "Epoch 150/500\n",
    "3/3 [==============================] - 2s 592ms/step - loss: 0.2296 - accuracy: 0.9286 - val_loss: 1.1773 - val_accuracy: 0.8444\n",
    "Epoch 151/500\n",
    "3/3 [==============================] - 2s 472ms/step - loss: 0.2107 - accuracy: 0.9464 - val_loss: 1.0666 - val_accuracy: 0.8444\n",
    "Epoch 152/500\n",
    "3/3 [==============================] - 2s 598ms/step - loss: 0.1785 - accuracy: 0.9524 - val_loss: 1.0289 - val_accuracy: 0.8444\n",
    "Epoch 153/500\n",
    "3/3 [==============================] - 2s 594ms/step - loss: 0.1751 - accuracy: 0.9286 - val_loss: 1.0045 - val_accuracy: 0.8444\n",
    "Epoch 154/500\n",
    "3/3 [==============================] - 2s 468ms/step - loss: 0.2527 - accuracy: 0.9167 - val_loss: 0.9417 - val_accuracy: 0.8222\n",
    "Epoch 155/500\n",
    "3/3 [==============================] - 2s 449ms/step - loss: 0.2527 - accuracy: 0.8988 - val_loss: 0.8906 - val_accuracy: 0.8222\n",
    "Epoch 156/500\n",
    "3/3 [==============================] - 2s 454ms/step - loss: 0.1803 - accuracy: 0.9405 - val_loss: 0.8959 - val_accuracy: 0.8000\n",
    "Epoch 157/500\n",
    "3/3 [==============================] - 2s 472ms/step - loss: 0.1981 - accuracy: 0.9286 - val_loss: 0.7638 - val_accuracy: 0.8444\n",
    "Epoch 158/500\n",
    "3/3 [==============================] - 2s 467ms/step - loss: 0.1343 - accuracy: 0.9464 - val_loss: 0.6975 - val_accuracy: 0.8444\n",
    "Epoch 159/500\n",
    "3/3 [==============================] - 2s 597ms/step - loss: 0.1427 - accuracy: 0.9405 - val_loss: 0.6794 - val_accuracy: 0.8444\n",
    "Epoch 160/500\n",
    "3/3 [==============================] - 2s 453ms/step - loss: 0.2143 - accuracy: 0.9286 - val_loss: 0.7285 - val_accuracy: 0.8000\n",
    "Epoch 161/500\n",
    "3/3 [==============================] - 2s 591ms/step - loss: 0.2216 - accuracy: 0.9167 - val_loss: 0.7935 - val_accuracy: 0.8000\n",
    "Epoch 162/500\n",
    "3/3 [==============================] - 2s 605ms/step - loss: 0.0873 - accuracy: 0.9762 - val_loss: 0.8521 - val_accuracy: 0.8000\n",
    "Epoch 163/500\n",
    "3/3 [==============================] - 2s 448ms/step - loss: 0.1662 - accuracy: 0.9524 - val_loss: 0.9057 - val_accuracy: 0.8000\n",
    "Epoch 164/500\n",
    "3/3 [==============================] - 2s 596ms/step - loss: 0.2336 - accuracy: 0.9226 - val_loss: 0.8620 - val_accuracy: 0.8222\n",
    "Epoch 165/500\n",
    "3/3 [==============================] - 2s 450ms/step - loss: 0.2479 - accuracy: 0.9107 - val_loss: 0.8415 - val_accuracy: 0.8000\n",
    "Epoch 166/500\n",
    "3/3 [==============================] - 2s 465ms/step - loss: 0.1807 - accuracy: 0.9524 - val_loss: 0.8480 - val_accuracy: 0.7778\n",
    "Epoch 167/500\n",
    "3/3 [==============================] - 2s 471ms/step - loss: 0.1862 - accuracy: 0.9345 - val_loss: 0.8792 - val_accuracy: 0.8000\n",
    "Epoch 168/500\n",
    "3/3 [==============================] - 2s 452ms/step - loss: 0.1427 - accuracy: 0.9643 - val_loss: 0.8752 - val_accuracy: 0.7778\n",
    "Epoch 169/500\n",
    "3/3 [==============================] - 2s 449ms/step - loss: 0.2141 - accuracy: 0.9226 - val_loss: 0.8439 - val_accuracy: 0.7778\n",
    "Epoch 170/500\n",
    "3/3 [==============================] - 2s 454ms/step - loss: 0.0857 - accuracy: 0.9702 - val_loss: 0.8011 - val_accuracy: 0.8444\n",
    "Epoch 171/500\n",
    "3/3 [==============================] - 2s 606ms/step - loss: 0.2104 - accuracy: 0.9107 - val_loss: 0.7207 - val_accuracy: 0.9111\n",
    "Epoch 172/500\n",
    "3/3 [==============================] - 2s 452ms/step - loss: 0.1378 - accuracy: 0.9405 - val_loss: 0.6912 - val_accuracy: 0.9111\n",
    "Epoch 173/500\n",
    "3/3 [==============================] - 2s 595ms/step - loss: 0.1743 - accuracy: 0.9405 - val_loss: 0.6964 - val_accuracy: 0.8889\n",
    "Epoch 174/500\n",
    "3/3 [==============================] - 2s 591ms/step - loss: 0.1484 - accuracy: 0.9464 - val_loss: 0.7081 - val_accuracy: 0.8889\n",
    "Epoch 175/500\n",
    "3/3 [==============================] - 2s 469ms/step - loss: 0.0860 - accuracy: 0.9643 - val_loss: 0.6791 - val_accuracy: 0.9111\n",
    "Epoch 176/500\n",
    "3/3 [==============================] - 2s 467ms/step - loss: 0.1059 - accuracy: 0.9464 - val_loss: 0.6123 - val_accuracy: 0.9111\n",
    "Epoch 177/500\n",
    "3/3 [==============================] - 2s 597ms/step - loss: 0.1389 - accuracy: 0.9464 - val_loss: 0.5384 - val_accuracy: 0.9333\n",
    "Epoch 178/500\n",
    "3/3 [==============================] - 2s 589ms/step - loss: 0.1127 - accuracy: 0.9464 - val_loss: 0.5087 - val_accuracy: 0.9333\n",
    "Epoch 179/500\n",
    "3/3 [==============================] - 2s 594ms/step - loss: 0.1110 - accuracy: 0.9643 - val_loss: 0.5426 - val_accuracy: 0.8889\n",
    "Epoch 180/500\n",
    "3/3 [==============================] - 2s 588ms/step - loss: 0.1301 - accuracy: 0.9464 - val_loss: 0.6088 - val_accuracy: 0.8667\n",
    "Epoch 181/500\n",
    "3/3 [==============================] - 2s 446ms/step - loss: 0.1156 - accuracy: 0.9702 - val_loss: 0.6739 - val_accuracy: 0.8667\n",
    "Epoch 182/500\n",
    "3/3 [==============================] - 2s 451ms/step - loss: 0.1522 - accuracy: 0.9405 - val_loss: 0.7416 - val_accuracy: 0.8444\n",
    "Epoch 183/500\n",
    "3/3 [==============================] - 2s 593ms/step - loss: 0.1807 - accuracy: 0.9286 - val_loss: 0.8660 - val_accuracy: 0.8444\n",
    "Epoch 184/500\n",
    "3/3 [==============================] - 2s 598ms/step - loss: 0.1114 - accuracy: 0.9702 - val_loss: 0.9720 - val_accuracy: 0.8444\n",
    "Epoch 185/500\n",
    "3/3 [==============================] - 2s 451ms/step - loss: 0.1171 - accuracy: 0.9583 - val_loss: 1.0371 - val_accuracy: 0.8222\n",
    "Epoch 186/500\n",
    "3/3 [==============================] - 2s 473ms/step - loss: 0.1185 - accuracy: 0.9643 - val_loss: 1.0068 - val_accuracy: 0.8222\n",
    "Epoch 187/500\n",
    "3/3 [==============================] - 2s 588ms/step - loss: 0.1284 - accuracy: 0.9405 - val_loss: 0.9316 - val_accuracy: 0.8444\n",
    "Epoch 188/500\n",
    "3/3 [==============================] - 2s 590ms/step - loss: 0.0974 - accuracy: 0.9643 - val_loss: 0.8677 - val_accuracy: 0.8444\n",
    "Epoch 189/500\n",
    "3/3 [==============================] - 2s 465ms/step - loss: 0.1926 - accuracy: 0.9286 - val_loss: 0.9453 - val_accuracy: 0.8444\n",
    "Epoch 190/500\n",
    "3/3 [==============================] - 2s 448ms/step - loss: 0.0795 - accuracy: 0.9524 - val_loss: 1.0756 - val_accuracy: 0.8444\n",
    "Epoch 191/500\n",
    "3/3 [==============================] - 2s 466ms/step - loss: 0.0822 - accuracy: 0.9762 - val_loss: 1.2427 - val_accuracy: 0.8000\n",
    "Epoch 192/500\n",
    "3/3 [==============================] - 2s 448ms/step - loss: 0.1339 - accuracy: 0.9345 - val_loss: 1.3071 - val_accuracy: 0.8000\n",
    "Epoch 193/500\n",
    "3/3 [==============================] - 2s 591ms/step - loss: 0.1239 - accuracy: 0.9405 - val_loss: 1.3214 - val_accuracy: 0.8222\n",
    "Epoch 194/500\n",
    "3/3 [==============================] - 2s 452ms/step - loss: 0.1399 - accuracy: 0.9464 - val_loss: 1.3549 - val_accuracy: 0.7778\n",
    "Epoch 195/500\n",
    "3/3 [==============================] - 2s 447ms/step - loss: 0.0971 - accuracy: 0.9702 - val_loss: 1.3938 - val_accuracy: 0.7778\n",
    "Epoch 196/500\n",
    "3/3 [==============================] - 2s 452ms/step - loss: 0.1165 - accuracy: 0.9643 - val_loss: 1.3608 - val_accuracy: 0.8222\n",
    "Epoch 197/500\n",
    "3/3 [==============================] - 2s 466ms/step - loss: 0.0992 - accuracy: 0.9702 - val_loss: 1.2155 - val_accuracy: 0.8444\n",
    "Epoch 198/500\n",
    "3/3 [==============================] - 2s 598ms/step - loss: 0.1901 - accuracy: 0.9345 - val_loss: 1.0741 - val_accuracy: 0.8667\n",
    "Epoch 199/500\n",
    "3/3 [==============================] - 2s 603ms/step - loss: 0.1311 - accuracy: 0.9524 - val_loss: 0.9401 - val_accuracy: 0.8889\n",
    "Epoch 200/500\n",
    "3/3 [==============================] - 2s 597ms/step - loss: 0.1434 - accuracy: 0.9405 - val_loss: 0.8346 - val_accuracy: 0.9111\n",
    "Epoch 201/500\n",
    "3/3 [==============================] - 2s 595ms/step - loss: 0.1241 - accuracy: 0.9583 - val_loss: 0.8117 - val_accuracy: 0.9111\n",
    "Epoch 202/500\n",
    "3/3 [==============================] - 2s 444ms/step - loss: 0.0791 - accuracy: 0.9643 - val_loss: 0.8482 - val_accuracy: 0.8889\n",
    "Epoch 203/500\n",
    "3/3 [==============================] - 2s 470ms/step - loss: 0.1701 - accuracy: 0.9524 - val_loss: 0.8713 - val_accuracy: 0.8667\n",
    "Epoch 204/500\n",
    "3/3 [==============================] - 2s 592ms/step - loss: 0.0896 - accuracy: 0.9643 - val_loss: 0.8578 - val_accuracy: 0.8667\n",
    "Epoch 205/500\n",
    "3/3 [==============================] - 2s 477ms/step - loss: 0.1240 - accuracy: 0.9643 - val_loss: 0.8098 - val_accuracy: 0.8889\n",
    "Epoch 206/500\n",
    "\n",
    "3/3 [==============================] - 2s 454ms/step - loss: 0.0755 - accuracy: 0.9821 - val_loss: 0.7360 - val_accuracy: 0.8889\n",
    "Epoch 207/500\n",
    "3/3 [==============================] - 2s 445ms/step - loss: 0.1335 - accuracy: 0.9583 - val_loss: 0.6549 - val_accuracy: 0.8889\n",
    "Epoch 208/500\n",
    "3/3 [==============================] - 2s 456ms/step - loss: 0.0746 - accuracy: 0.9821 - val_loss: 0.5638 - val_accuracy: 0.8889\n",
    "Epoch 209/500\n",
    "3/3 [==============================] - 2s 469ms/step - loss: 0.1158 - accuracy: 0.9583 - val_loss: 0.4740 - val_accuracy: 0.9111\n",
    "Epoch 210/500\n",
    "3/3 [==============================] - 2s 448ms/step - loss: 0.1395 - accuracy: 0.9464 - val_loss: 0.4492 - val_accuracy: 0.9111\n",
    "Epoch 211/500\n",
    "3/3 [==============================] - 2s 596ms/step - loss: 0.1559 - accuracy: 0.9583 - val_loss: 0.4659 - val_accuracy: 0.8889\n",
    "Epoch 212/500\n",
    "3/3 [==============================] - 2s 450ms/step - loss: 0.0564 - accuracy: 0.9762 - val_loss: 0.5322 - val_accuracy: 0.9111\n",
    "Epoch 213/500\n",
    "3/3 [==============================] - 2s 448ms/step - loss: 0.0905 - accuracy: 0.9643 - val_loss: 0.5886 - val_accuracy: 0.9111\n",
    "Epoch 214/500\n",
    "3/3 [==============================] - 2s 452ms/step - loss: 0.1195 - accuracy: 0.9464 - val_loss: 0.6487 - val_accuracy: 0.9111\n",
    "Epoch 215/500\n",
    "3/3 [==============================] - 2s 598ms/step - loss: 0.1246 - accuracy: 0.9524 - val_loss: 0.7394 - val_accuracy: 0.9111\n",
    "Epoch 216/500\n",
    "3/3 [==============================] - 2s 598ms/step - loss: 0.1025 - accuracy: 0.9762 - val_loss: 0.8235 - val_accuracy: 0.8889\n",
    "Epoch 217/500\n",
    "3/3 [==============================] - 2s 467ms/step - loss: 0.0942 - accuracy: 0.9583 - val_loss: 0.9458 - val_accuracy: 0.8889\n",
    "Epoch 218/500\n",
    "3/3 [==============================] - 2s 589ms/step - loss: 0.0718 - accuracy: 0.9881 - val_loss: 1.0806 - val_accuracy: 0.8667\n",
    "Epoch 219/500\n",
    "3/3 [==============================] - 2s 478ms/step - loss: 0.1242 - accuracy: 0.9583 - val_loss: 1.1876 - val_accuracy: 0.8667\n",
    "Epoch 220/500\n",
    "3/3 [==============================] - 2s 594ms/step - loss: 0.0686 - accuracy: 0.9702 - val_loss: 1.2906 - val_accuracy: 0.8444\n",
    "Epoch 221/500\n",
    "3/3 [==============================] - 2s 590ms/step - loss: 0.1345 - accuracy: 0.9643 - val_loss: 1.2617 - val_accuracy: 0.8667\n",
    "Epoch 222/500\n",
    "3/3 [==============================] - 2s 466ms/step - loss: 0.1875 - accuracy: 0.9464 - val_loss: 1.1540 - val_accuracy: 0.8889\n",
    "Epoch 223/500\n",
    "3/3 [==============================] - 2s 598ms/step - loss: 0.1194 - accuracy: 0.9583 - val_loss: 1.1087 - val_accuracy: 0.8889\n",
    "Epoch 224/500\n",
    "3/3 [==============================] - 2s 449ms/step - loss: 0.1175 - accuracy: 0.9702 - val_loss: 1.0287 - val_accuracy: 0.8889\n",
    "Epoch 225/500\n",
    "3/3 [==============================] - 2s 446ms/step - loss: 0.0902 - accuracy: 0.9643 - val_loss: 0.9493 - val_accuracy: 0.8889\n",
    "Epoch 226/500\n",
    "3/3 [==============================] - 2s 584ms/step - loss: 0.1088 - accuracy: 0.9643 - val_loss: 0.9038 - val_accuracy: 0.8667\n",
    "Epoch 227/500\n",
    "3/3 [==============================] - 2s 589ms/step - loss: 0.0817 - accuracy: 0.9702 - val_loss: 0.8510 - val_accuracy: 0.8667\n",
    "Epoch 228/500\n",
    "3/3 [==============================] - 2s 449ms/step - loss: 0.0998 - accuracy: 0.9643 - val_loss: 0.7668 - val_accuracy: 0.8444\n",
    "Epoch 229/500\n",
    "3/3 [==============================] - 2s 593ms/step - loss: 0.0842 - accuracy: 0.9702 - val_loss: 0.7573 - val_accuracy: 0.8667\n",
    "Epoch 230/500\n",
    "3/3 [==============================] - 2s 471ms/step - loss: 0.0532 - accuracy: 0.9881 - val_loss: 0.7892 - val_accuracy: 0.8889\n",
    "Epoch 231/500\n",
    "3/3 [==============================] - 2s 473ms/step - loss: 0.1270 - accuracy: 0.9405 - val_loss: 0.8981 - val_accuracy: 0.8444\n",
    "Epoch 232/500\n",
    "3/3 [==============================] - 2s 585ms/step - loss: 0.0387 - accuracy: 0.9940 - val_loss: 1.0912 - val_accuracy: 0.8444\n",
    "Epoch 233/500\n",
    "3/3 [==============================] - 2s 591ms/step - loss: 0.0939 - accuracy: 0.9583 - val_loss: 1.2691 - val_accuracy: 0.8444\n",
    "Epoch 234/500\n",
    "3/3 [==============================] - 2s 601ms/step - loss: 0.1433 - accuracy: 0.9643 - val_loss: 1.3770 - val_accuracy: 0.8444\n",
    "Epoch 235/500\n",
    "3/3 [==============================] - 2s 446ms/step - loss: 0.1043 - accuracy: 0.9583 - val_loss: 1.4078 - val_accuracy: 0.8444\n",
    "Epoch 236/500\n",
    "3/3 [==============================] - 2s 467ms/step - loss: 0.0678 - accuracy: 0.9762 - val_loss: 1.3730 - val_accuracy: 0.8444\n",
    "Epoch 237/500\n",
    "3/3 [==============================] - 2s 451ms/step - loss: 0.1197 - accuracy: 0.9583 - val_loss: 1.2035 - val_accuracy: 0.8222\n",
    "Epoch 238/500\n",
    "3/3 [==============================] - 2s 468ms/step - loss: 0.0616 - accuracy: 0.9821 - val_loss: 1.0672 - val_accuracy: 0.8222\n",
    "Epoch 239/500\n",
    "3/3 [==============================] - 2s 463ms/step - loss: 0.1079 - accuracy: 0.9464 - val_loss: 0.9353 - val_accuracy: 0.8444\n",
    "Epoch 240/500\n",
    "3/3 [==============================] - 2s 469ms/step - loss: 0.1328 - accuracy: 0.9583 - val_loss: 0.8682 - val_accuracy: 0.8667\n",
    "Epoch 241/500\n",
    "3/3 [==============================] - 2s 450ms/step - loss: 0.1608 - accuracy: 0.9464 - val_loss: 0.9096 - val_accuracy: 0.8889\n",
    "Epoch 242/500\n",
    "3/3 [==============================] - 2s 446ms/step - loss: 0.1307 - accuracy: 0.9524 - val_loss: 1.0117 - val_accuracy: 0.8889\n",
    "Epoch 243/500\n",
    "3/3 [==============================] - 2s 457ms/step - loss: 0.1246 - accuracy: 0.9643 - val_loss: 1.1184 - val_accuracy: 0.8444\n",
    "Epoch 244/500\n",
    "3/3 [==============================] - 2s 590ms/step - loss: 0.2309 - accuracy: 0.9286 - val_loss: 1.1454 - val_accuracy: 0.8444\n",
    "Epoch 245/500\n",
    "3/3 [==============================] - 2s 462ms/step - loss: 0.1546 - accuracy: 0.9702 - val_loss: 1.0692 - val_accuracy: 0.8667\n",
    "Epoch 246/500\n",
    "3/3 [==============================] - 2s 450ms/step - loss: 0.0821 - accuracy: 0.9702 - val_loss: 1.0079 - val_accuracy: 0.8889\n",
    "Epoch 247/500\n",
    "3/3 [==============================] - 2s 600ms/step - loss: 0.0931 - accuracy: 0.9583 - val_loss: 0.9660 - val_accuracy: 0.8889\n",
    "Epoch 248/500\n",
    "3/3 [==============================] - 2s 467ms/step - loss: 0.0861 - accuracy: 0.9702 - val_loss: 0.9671 - val_accuracy: 0.8889\n",
    "Epoch 249/500\n",
    "3/3 [==============================] - 2s 467ms/step - loss: 0.0669 - accuracy: 0.9762 - val_loss: 1.0095 - val_accuracy: 0.8889\n",
    "Epoch 250/500\n",
    "3/3 [==============================] - 2s 453ms/step - loss: 0.1755 - accuracy: 0.9464 - val_loss: 1.1080 - val_accuracy: 0.8444\n",
    "Epoch 251/500\n",
    "3/3 [==============================] - 2s 445ms/step - loss: 0.1272 - accuracy: 0.9583 - val_loss: 1.1684 - val_accuracy: 0.8222\n",
    "Epoch 252/500\n",
    "3/3 [==============================] - 2s 592ms/step - loss: 0.0778 - accuracy: 0.9762 - val_loss: 1.2101 - val_accuracy: 0.7778\n",
    "Epoch 253/500\n",
    "3/3 [==============================] - 2s 468ms/step - loss: 0.1035 - accuracy: 0.9583 - val_loss: 1.2660 - val_accuracy: 0.7556\n",
    "Epoch 254/500\n",
    "3/3 [==============================] - 2s 598ms/step - loss: 0.0689 - accuracy: 0.9702 - val_loss: 1.3432 - val_accuracy: 0.7778\n",
    "Epoch 255/500\n",
    "3/3 [==============================] - 2s 597ms/step - loss: 0.0787 - accuracy: 0.9762 - val_loss: 1.3886 - val_accuracy: 0.8000\n",
    "Epoch 256/500\n",
    "3/3 [==============================] - 2s 452ms/step - loss: 0.1402 - accuracy: 0.9524 - val_loss: 1.4062 - val_accuracy: 0.8000\n",
    "Epoch 257/500\n",
    "3/3 [==============================] - 2s 474ms/step - loss: 0.0871 - accuracy: 0.9762 - val_loss: 1.3011 - val_accuracy: 0.8222\n",
    "Epoch 258/500\n",
    "3/3 [==============================] - 2s 450ms/step - loss: 0.1630 - accuracy: 0.9345 - val_loss: 1.1176 - val_accuracy: 0.8444\n",
    "Epoch 259/500\n",
    "3/3 [==============================] - 2s 595ms/step - loss: 0.0686 - accuracy: 0.9702 - val_loss: 0.9440 - val_accuracy: 0.8222\n",
    "Epoch 260/500\n",
    "3/3 [==============================] - 2s 595ms/step - loss: 0.0867 - accuracy: 0.9702 - val_loss: 0.8639 - val_accuracy: 0.8222\n",
    "Epoch 261/500\n",
    "3/3 [==============================] - 2s 598ms/step - loss: 0.1020 - accuracy: 0.9524 - val_loss: 0.8761 - val_accuracy: 0.8222\n",
    "Epoch 262/500\n",
    "3/3 [==============================] - 2s 597ms/step - loss: 0.1242 - accuracy: 0.9583 - val_loss: 0.9002 - val_accuracy: 0.8444\n",
    "Epoch 263/500\n",
    "\n",
    "3/3 [==============================] - 2s 594ms/step - loss: 0.0965 - accuracy: 0.9702 - val_loss: 0.9134 - val_accuracy: 0.8222\n",
    "Epoch 264/500\n",
    "3/3 [==============================] - 2s 476ms/step - loss: 0.0604 - accuracy: 0.9881 - val_loss: 0.9468 - val_accuracy: 0.8222\n",
    "Epoch 265/500\n",
    "3/3 [==============================] - 2s 458ms/step - loss: 0.0972 - accuracy: 0.9464 - val_loss: 0.9561 - val_accuracy: 0.8222\n",
    "Epoch 266/500\n",
    "3/3 [==============================] - 2s 459ms/step - loss: 0.1060 - accuracy: 0.9583 - val_loss: 1.0389 - val_accuracy: 0.8222\n",
    "Epoch 267/500\n",
    "3/3 [==============================] - 2s 595ms/step - loss: 0.0951 - accuracy: 0.9583 - val_loss: 1.0250 - val_accuracy: 0.8222\n",
    "Epoch 268/500\n",
    "3/3 [==============================] - 2s 593ms/step - loss: 0.1226 - accuracy: 0.9464 - val_loss: 1.0619 - val_accuracy: 0.8222\n",
    "Epoch 269/500\n",
    "3/3 [==============================] - 2s 591ms/step - loss: 0.0647 - accuracy: 0.9821 - val_loss: 18.1161 - val_accuracy: 0.3111\n",
    "Epoch 270/500\n",
    "3/3 [==============================] - 2s 474ms/step - loss: 0.1556 - accuracy: 0.9286 - val_loss: 4.9612 - val_accuracy: 0.4667\n",
    "Epoch 271/500\n",
    "3/3 [==============================] - 2s 586ms/step - loss: 0.0555 - accuracy: 0.9821 - val_loss: 3.2251 - val_accuracy: 0.6000\n",
    "Epoch 272/500\n",
    "3/3 [==============================] - 2s 590ms/step - loss: 0.0797 - accuracy: 0.9702 - val_loss: 2.4833 - val_accuracy: 0.7111\n",
    "Epoch 273/500\n",
    "3/3 [==============================] - 2s 595ms/step - loss: 0.2021 - accuracy: 0.9226 - val_loss: 1.9315 - val_accuracy: 0.6889\n",
    "Epoch 274/500\n",
    "3/3 [==============================] - 2s 454ms/step - loss: 0.0758 - accuracy: 0.9821 - val_loss: 1.6965 - val_accuracy: 0.7111\n",
    "Epoch 275/500\n",
    "3/3 [==============================] - 2s 595ms/step - loss: 0.1310 - accuracy: 0.9524 - val_loss: 1.4391 - val_accuracy: 0.7556\n",
    "Epoch 276/500\n",
    "3/3 [==============================] - 2s 598ms/step - loss: 0.1309 - accuracy: 0.9524 - val_loss: 1.3559 - val_accuracy: 0.7778\n",
    "Epoch 277/500\n",
    "3/3 [==============================] - 2s 452ms/step - loss: 0.1486 - accuracy: 0.9524 - val_loss: 1.3775 - val_accuracy: 0.8000\n",
    "Epoch 278/500\n",
    "3/3 [==============================] - 2s 468ms/step - loss: 0.0789 - accuracy: 0.9702 - val_loss: 1.3435 - val_accuracy: 0.8222\n",
    "Epoch 279/500\n",
    "3/3 [==============================] - 2s 448ms/step - loss: 0.1106 - accuracy: 0.9524 - val_loss: 1.1967 - val_accuracy: 0.8000\n",
    "Epoch 280/500\n",
    "3/3 [==============================] - 2s 584ms/step - loss: 0.1004 - accuracy: 0.9643 - val_loss: 1.1139 - val_accuracy: 0.8000\n",
    "Epoch 281/500\n",
    "3/3 [==============================] - 2s 468ms/step - loss: 0.1282 - accuracy: 0.9583 - val_loss: 1.0304 - val_accuracy: 0.8444\n",
    "Epoch 282/500\n",
    "3/3 [==============================] - 2s 467ms/step - loss: 0.0504 - accuracy: 0.9881 - val_loss: 0.9917 - val_accuracy: 0.8444\n",
    "Epoch 283/500\n",
    "3/3 [==============================] - 2s 460ms/step - loss: 0.1005 - accuracy: 0.9702 - val_loss: 0.9215 - val_accuracy: 0.8444\n",
    "Epoch 284/500\n",
    "3/3 [==============================] - 2s 461ms/step - loss: 0.1088 - accuracy: 0.9524 - val_loss: 0.9116 - val_accuracy: 0.8444\n",
    "Epoch 285/500\n",
    "3/3 [==============================] - 2s 590ms/step - loss: 0.1210 - accuracy: 0.9702 - val_loss: 0.9710 - val_accuracy: 0.8222\n",
    "Epoch 286/500\n",
    "3/3 [==============================] - 2s 589ms/step - loss: 0.0982 - accuracy: 0.9643 - val_loss: 1.0690 - val_accuracy: 0.8222\n",
    "Epoch 287/500\n",
    "3/3 [==============================] - 2s 595ms/step - loss: 0.1205 - accuracy: 0.9643 - val_loss: 1.0951 - val_accuracy: 0.8000\n",
    "Epoch 288/500\n",
    "3/3 [==============================] - 2s 591ms/step - loss: 0.1052 - accuracy: 0.9583 - val_loss: 1.0999 - val_accuracy: 0.8222\n",
    "Epoch 289/500\n",
    "3/3 [==============================] - 2s 470ms/step - loss: 0.0974 - accuracy: 0.9762 - val_loss: 1.0571 - val_accuracy: 0.8444\n",
    "Epoch 290/500\n",
    "3/3 [==============================] - 2s 477ms/step - loss: 0.1437 - accuracy: 0.9583 - val_loss: 1.0267 - val_accuracy: 0.8444\n",
    "Epoch 291/500\n",
    "3/3 [==============================] - 2s 449ms/step - loss: 0.0413 - accuracy: 0.9821 - val_loss: 1.0156 - val_accuracy: 0.8444\n",
    "Epoch 292/500\n",
    "3/3 [==============================] - 2s 453ms/step - loss: 0.1409 - accuracy: 0.9583 - val_loss: 0.9928 - val_accuracy: 0.8667\n",
    "Epoch 293/500\n",
    "3/3 [==============================] - 2s 468ms/step - loss: 0.0528 - accuracy: 0.9702 - val_loss: 0.9965 - val_accuracy: 0.8667\n",
    "Epoch 294/500\n",
    "3/3 [==============================] - 2s 451ms/step - loss: 0.0926 - accuracy: 0.9643 - val_loss: 1.0319 - val_accuracy: 0.8667\n",
    "Epoch 295/500\n",
    "3/3 [==============================] - 2s 595ms/step - loss: 0.1383 - accuracy: 0.9345 - val_loss: 1.0291 - val_accuracy: 0.8667\n",
    "Epoch 296/500\n",
    "3/3 [==============================] - 2s 444ms/step - loss: 0.1196 - accuracy: 0.9464 - val_loss: 1.0379 - val_accuracy: 0.8667\n",
    "Epoch 297/500\n",
    "3/3 [==============================] - 2s 455ms/step - loss: 0.0612 - accuracy: 0.9762 - val_loss: 1.0893 - val_accuracy: 0.8667\n",
    "Epoch 298/500\n",
    "3/3 [==============================] - 2s 593ms/step - loss: 0.1674 - accuracy: 0.9286 - val_loss: 1.1815 - val_accuracy: 0.8444\n",
    "Epoch 299/500\n",
    "3/3 [==============================] - 2s 454ms/step - loss: 0.0882 - accuracy: 0.9702 - val_loss: 1.2787 - val_accuracy: 0.8667\n",
    "Epoch 300/500\n",
    "3/3 [==============================] - 2s 461ms/step - loss: 0.1494 - accuracy: 0.9524 - val_loss: 1.3283 - val_accuracy: 0.8667\n",
    "Epoch 301/500\n",
    "3/3 [==============================] - 2s 475ms/step - loss: 0.1227 - accuracy: 0.9524 - val_loss: 1.2669 - val_accuracy: 0.8667\n",
    "Epoch 302/500\n",
    "3/3 [==============================] - 2s 454ms/step - loss: 0.0736 - accuracy: 0.9762 - val_loss: 1.2200 - val_accuracy: 0.8667\n",
    "Epoch 303/500\n",
    "3/3 [==============================] - 2s 455ms/step - loss: 0.0947 - accuracy: 0.9643 - val_loss: 1.2073 - val_accuracy: 0.8667\n",
    "Epoch 304/500\n",
    "3/3 [==============================] - 2s 460ms/step - loss: 0.1313 - accuracy: 0.9345 - val_loss: 1.1849 - val_accuracy: 0.8667\n",
    "Epoch 305/500\n",
    "3/3 [==============================] - 2s 453ms/step - loss: 0.1064 - accuracy: 0.9702 - val_loss: 1.1658 - val_accuracy: 0.8667\n",
    "Epoch 306/500\n",
    "3/3 [==============================] - 2s 450ms/step - loss: 0.1098 - accuracy: 0.9643 - val_loss: 1.1285 - val_accuracy: 0.8667\n",
    "Epoch 307/500\n",
    "3/3 [==============================] - 2s 450ms/step - loss: 0.1021 - accuracy: 0.9643 - val_loss: 1.0688 - val_accuracy: 0.8667\n",
    "Epoch 308/500\n",
    "3/3 [==============================] - 2s 596ms/step - loss: 0.1014 - accuracy: 0.9643 - val_loss: 1.0085 - val_accuracy: 0.8889\n",
    "Epoch 309/500\n",
    "3/3 [==============================] - 2s 598ms/step - loss: 0.1420 - accuracy: 0.9464 - val_loss: 1.0660 - val_accuracy: 0.8889\n",
    "Epoch 310/500\n",
    "3/3 [==============================] - 2s 470ms/step - loss: 0.1307 - accuracy: 0.9643 - val_loss: 1.1665 - val_accuracy: 0.8889\n",
    "Epoch 311/500\n",
    "3/3 [==============================] - 2s 447ms/step - loss: 0.0989 - accuracy: 0.9583 - val_loss: 1.2596 - val_accuracy: 0.8444\n",
    "Epoch 312/500\n",
    "3/3 [==============================] - 2s 590ms/step - loss: 0.0625 - accuracy: 0.9762 - val_loss: 1.3318 - val_accuracy: 0.8222\n",
    "Epoch 313/500\n",
    "3/3 [==============================] - 2s 589ms/step - loss: 0.0653 - accuracy: 0.9762 - val_loss: 1.4449 - val_accuracy: 0.8000\n",
    "Epoch 314/500\n",
    "3/3 [==============================] - 2s 472ms/step - loss: 0.0657 - accuracy: 0.9702 - val_loss: 1.4787 - val_accuracy: 0.8000\n",
    "Epoch 315/500\n",
    "3/3 [==============================] - 2s 469ms/step - loss: 0.0761 - accuracy: 0.9881 - val_loss: 1.4334 - val_accuracy: 0.8000\n",
    "Epoch 316/500\n",
    "3/3 [==============================] - 2s 606ms/step - loss: 0.0797 - accuracy: 0.9702 - val_loss: 1.2974 - val_accuracy: 0.8000\n",
    "Epoch 317/500\n",
    "3/3 [==============================] - 2s 597ms/step - loss: 0.1103 - accuracy: 0.9583 - val_loss: 1.0995 - val_accuracy: 0.8444\n",
    "Epoch 318/500\n",
    "3/3 [==============================] - 2s 449ms/step - loss: 0.1311 - accuracy: 0.9524 - val_loss: 1.0371 - val_accuracy: 0.8889\n",
    "Epoch 319/500\n",
    "3/3 [==============================] - 2s 467ms/step - loss: 0.1201 - accuracy: 0.9821 - val_loss: 1.0653 - val_accuracy: 0.9111\n",
    "Epoch 320/500\n",
    "\n",
    "3/3 [==============================] - 2s 453ms/step - loss: 0.0561 - accuracy: 0.9821 - val_loss: 1.0863 - val_accuracy: 0.8889\n",
    "Epoch 321/500\n",
    "3/3 [==============================] - 2s 446ms/step - loss: 0.1281 - accuracy: 0.9643 - val_loss: 1.0963 - val_accuracy: 0.8889\n",
    "Epoch 322/500\n",
    "3/3 [==============================] - 2s 584ms/step - loss: 0.0955 - accuracy: 0.9524 - val_loss: 1.1858 - val_accuracy: 0.8667\n",
    "Epoch 323/500\n",
    "3/3 [==============================] - 2s 591ms/step - loss: 0.1204 - accuracy: 0.9464 - val_loss: 1.1176 - val_accuracy: 0.8889\n",
    "Epoch 324/500\n",
    "3/3 [==============================] - 2s 474ms/step - loss: 0.1425 - accuracy: 0.9226 - val_loss: 0.9408 - val_accuracy: 0.9111\n",
    "Epoch 325/500\n",
    "3/3 [==============================] - 2s 467ms/step - loss: 0.0751 - accuracy: 0.9762 - val_loss: 0.7687 - val_accuracy: 0.9111\n",
    "Epoch 326/500\n",
    "3/3 [==============================] - 2s 475ms/step - loss: 0.0650 - accuracy: 0.9583 - val_loss: 0.6929 - val_accuracy: 0.9111\n",
    "Epoch 327/500\n",
    "3/3 [==============================] - 2s 469ms/step - loss: 0.0770 - accuracy: 0.9702 - val_loss: 0.6397 - val_accuracy: 0.9111\n",
    "Epoch 328/500\n",
    "3/3 [==============================] - 2s 592ms/step - loss: 0.0713 - accuracy: 0.9643 - val_loss: 0.6400 - val_accuracy: 0.8889\n",
    "Epoch 329/500\n",
    "3/3 [==============================] - 2s 466ms/step - loss: 0.1108 - accuracy: 0.9464 - val_loss: 0.6385 - val_accuracy: 0.8889\n",
    "Epoch 330/500\n",
    "3/3 [==============================] - 2s 476ms/step - loss: 0.0728 - accuracy: 0.9821 - val_loss: 0.6413 - val_accuracy: 0.8889\n",
    "Epoch 331/500\n",
    "3/3 [==============================] - 2s 445ms/step - loss: 0.1283 - accuracy: 0.9643 - val_loss: 0.6505 - val_accuracy: 0.9111\n",
    "Epoch 332/500\n",
    "3/3 [==============================] - 2s 474ms/step - loss: 0.1579 - accuracy: 0.9405 - val_loss: 0.6994 - val_accuracy: 0.9111\n",
    "Epoch 333/500\n",
    "3/3 [==============================] - 2s 471ms/step - loss: 0.1226 - accuracy: 0.9524 - val_loss: 0.7877 - val_accuracy: 0.9111\n",
    "Epoch 334/500\n",
    "3/3 [==============================] - 2s 467ms/step - loss: 0.0842 - accuracy: 0.9643 - val_loss: 0.8527 - val_accuracy: 0.9111\n",
    "Epoch 335/500\n",
    "3/3 [==============================] - 2s 458ms/step - loss: 0.0797 - accuracy: 0.9821 - val_loss: 0.9210 - val_accuracy: 0.9111\n",
    "Epoch 336/500\n",
    "3/3 [==============================] - 2s 481ms/step - loss: 0.0977 - accuracy: 0.9762 - val_loss: 0.9844 - val_accuracy: 0.9111\n",
    "Epoch 337/500\n",
    "3/3 [==============================] - 2s 592ms/step - loss: 0.1123 - accuracy: 0.9524 - val_loss: 1.0482 - val_accuracy: 0.9111\n",
    "Epoch 338/500\n",
    "3/3 [==============================] - 2s 461ms/step - loss: 0.0800 - accuracy: 0.9702 - val_loss: 1.1708 - val_accuracy: 0.9111\n",
    "Epoch 339/500\n",
    "3/3 [==============================] - 2s 596ms/step - loss: 0.1039 - accuracy: 0.9524 - val_loss: 1.2285 - val_accuracy: 0.8667\n",
    "Epoch 340/500\n",
    "3/3 [==============================] - 2s 462ms/step - loss: 0.0722 - accuracy: 0.9583 - val_loss: 1.1994 - val_accuracy: 0.8444\n",
    "Epoch 341/500\n",
    "3/3 [==============================] - 2s 597ms/step - loss: 0.0576 - accuracy: 0.9762 - val_loss: 1.1112 - val_accuracy: 0.9111\n",
    "Epoch 342/500\n",
    "3/3 [==============================] - 2s 474ms/step - loss: 0.1119 - accuracy: 0.9464 - val_loss: 1.0043 - val_accuracy: 0.9111\n",
    "Epoch 343/500\n",
    "3/3 [==============================] - 2s 450ms/step - loss: 0.1136 - accuracy: 0.9583 - val_loss: 0.9866 - val_accuracy: 0.9111\n",
    "Epoch 344/500\n",
    "3/3 [==============================] - 2s 466ms/step - loss: 0.1105 - accuracy: 0.9524 - val_loss: 0.9863 - val_accuracy: 0.9111\n",
    "Epoch 345/500\n",
    "3/3 [==============================] - 2s 473ms/step - loss: 0.0515 - accuracy: 0.9821 - val_loss: 0.9975 - val_accuracy: 0.8889\n",
    "Epoch 346/500\n",
    "3/3 [==============================] - 2s 594ms/step - loss: 0.0575 - accuracy: 0.9821 - val_loss: 1.0395 - val_accuracy: 0.8889\n",
    "Epoch 347/500\n",
    "3/3 [==============================] - 2s 465ms/step - loss: 0.0596 - accuracy: 0.9881 - val_loss: 1.1250 - val_accuracy: 0.8667\n",
    "Epoch 348/500\n",
    "3/3 [==============================] - 2s 474ms/step - loss: 0.1474 - accuracy: 0.9524 - val_loss: 1.1967 - val_accuracy: 0.8667\n",
    "Epoch 349/500\n",
    "3/3 [==============================] - 2s 598ms/step - loss: 0.1004 - accuracy: 0.9643 - val_loss: 1.2030 - val_accuracy: 0.8667\n",
    "Epoch 350/500\n",
    "3/3 [==============================] - 2s 451ms/step - loss: 0.0732 - accuracy: 0.9702 - val_loss: 1.2001 - val_accuracy: 0.8667\n",
    "Epoch 351/500\n",
    "3/3 [==============================] - 2s 450ms/step - loss: 0.1081 - accuracy: 0.9524 - val_loss: 1.1904 - val_accuracy: 0.8667\n",
    "Epoch 352/500\n",
    "3/3 [==============================] - 2s 592ms/step - loss: 0.0518 - accuracy: 0.9881 - val_loss: 1.1926 - val_accuracy: 0.8667\n",
    "Epoch 353/500\n",
    "3/3 [==============================] - 2s 471ms/step - loss: 0.0657 - accuracy: 0.9821 - val_loss: 1.2220 - val_accuracy: 0.8667\n",
    "Epoch 354/500\n",
    "3/3 [==============================] - 2s 462ms/step - loss: 0.0786 - accuracy: 0.9881 - val_loss: 1.2669 - val_accuracy: 0.8889\n",
    "Epoch 355/500\n",
    "3/3 [==============================] - 2s 475ms/step - loss: 0.0837 - accuracy: 0.9762 - val_loss: 1.3043 - val_accuracy: 0.8889\n",
    "Epoch 356/500\n",
    "3/3 [==============================] - 2s 464ms/step - loss: 0.1266 - accuracy: 0.9583 - val_loss: 1.3165 - val_accuracy: 0.8889\n",
    "Epoch 357/500\n",
    "3/3 [==============================] - 2s 468ms/step - loss: 0.1259 - accuracy: 0.9524 - val_loss: 1.3100 - val_accuracy: 0.8444\n",
    "Epoch 358/500\n",
    "3/3 [==============================] - 2s 455ms/step - loss: 0.0716 - accuracy: 0.9821 - val_loss: 1.3862 - val_accuracy: 0.8222\n",
    "Epoch 359/500\n",
    "3/3 [==============================] - 2s 592ms/step - loss: 0.0484 - accuracy: 0.9881 - val_loss: 1.5228 - val_accuracy: 0.7778\n",
    "Epoch 360/500\n",
    "3/3 [==============================] - 2s 597ms/step - loss: 0.1010 - accuracy: 0.9702 - val_loss: 1.4408 - val_accuracy: 0.8000\n",
    "Epoch 361/500\n",
    "3/3 [==============================] - 2s 466ms/step - loss: 0.0720 - accuracy: 0.9702 - val_loss: 1.3823 - val_accuracy: 0.8222\n",
    "Epoch 362/500\n",
    "3/3 [==============================] - 2s 454ms/step - loss: 0.0817 - accuracy: 0.9643 - val_loss: 1.2469 - val_accuracy: 0.8222\n",
    "Epoch 363/500\n",
    "3/3 [==============================] - 2s 453ms/step - loss: 0.0566 - accuracy: 0.9762 - val_loss: 1.0666 - val_accuracy: 0.8667\n",
    "Epoch 364/500\n",
    "3/3 [==============================] - 2s 598ms/step - loss: 0.0695 - accuracy: 0.9702 - val_loss: 0.9494 - val_accuracy: 0.8889\n",
    "Epoch 365/500\n",
    "3/3 [==============================] - 2s 593ms/step - loss: 0.1315 - accuracy: 0.9583 - val_loss: 0.8435 - val_accuracy: 0.8889\n",
    "Epoch 366/500\n",
    "3/3 [==============================] - 2s 588ms/step - loss: 0.1090 - accuracy: 0.9524 - val_loss: 0.7029 - val_accuracy: 0.8889\n",
    "Epoch 367/500\n",
    "3/3 [==============================] - 2s 478ms/step - loss: 0.0458 - accuracy: 0.9821 - val_loss: 0.6240 - val_accuracy: 0.8889\n",
    "Epoch 368/500\n",
    "3/3 [==============================] - 2s 460ms/step - loss: 0.0590 - accuracy: 0.9821 - val_loss: 0.5889 - val_accuracy: 0.8889\n",
    "Epoch 369/500\n",
    "3/3 [==============================] - 2s 468ms/step - loss: 0.0487 - accuracy: 0.9821 - val_loss: 0.6061 - val_accuracy: 0.8889\n",
    "Epoch 370/500\n",
    "3/3 [==============================] - 2s 597ms/step - loss: 0.0832 - accuracy: 0.9702 - val_loss: 0.6579 - val_accuracy: 0.8667\n",
    "Epoch 371/500\n",
    "3/3 [==============================] - 2s 449ms/step - loss: 0.0552 - accuracy: 0.9702 - val_loss: 0.7307 - val_accuracy: 0.8667\n",
    "Epoch 372/500\n",
    "3/3 [==============================] - 2s 596ms/step - loss: 0.1091 - accuracy: 0.9583 - val_loss: 0.8530 - val_accuracy: 0.8667\n",
    "Epoch 373/500\n",
    "3/3 [==============================] - 2s 470ms/step - loss: 0.0666 - accuracy: 0.9643 - val_loss: 0.8893 - val_accuracy: 0.8889\n",
    "Epoch 374/500\n",
    "3/3 [==============================] - 2s 596ms/step - loss: 0.0708 - accuracy: 0.9762 - val_loss: 0.9416 - val_accuracy: 0.8667\n",
    "Epoch 375/500\n",
    "3/3 [==============================] - 2s 601ms/step - loss: 0.0642 - accuracy: 0.9762 - val_loss: 1.0396 - val_accuracy: 0.8444\n",
    "Epoch 376/500\n",
    "3/3 [==============================] - 2s 599ms/step - loss: 0.0452 - accuracy: 0.9762 - val_loss: 1.1295 - val_accuracy: 0.8444\n",
    "Epoch 377/500\n",
    "\n",
    "3/3 [==============================] - 2s 598ms/step - loss: 0.0998 - accuracy: 0.9643 - val_loss: 1.1951 - val_accuracy: 0.8444\n",
    "Epoch 378/500\n",
    "3/3 [==============================] - 2s 594ms/step - loss: 0.0149 - accuracy: 1.0000 - val_loss: 1.2311 - val_accuracy: 0.8444\n",
    "Epoch 379/500\n",
    "3/3 [==============================] - 2s 453ms/step - loss: 0.1186 - accuracy: 0.9643 - val_loss: 1.2130 - val_accuracy: 0.8444\n",
    "Epoch 380/500\n",
    "3/3 [==============================] - 2s 472ms/step - loss: 0.1078 - accuracy: 0.9583 - val_loss: 1.1279 - val_accuracy: 0.8667\n",
    "Epoch 381/500\n",
    "3/3 [==============================] - 2s 445ms/step - loss: 0.0919 - accuracy: 0.9762 - val_loss: 1.1450 - val_accuracy: 0.8667\n",
    "Epoch 382/500\n",
    "3/3 [==============================] - 2s 447ms/step - loss: 0.0995 - accuracy: 0.9583 - val_loss: 1.1313 - val_accuracy: 0.8444\n",
    "Epoch 383/500\n",
    "3/3 [==============================] - 2s 466ms/step - loss: 0.1276 - accuracy: 0.9524 - val_loss: 1.0985 - val_accuracy: 0.8667\n",
    "Epoch 384/500\n",
    "3/3 [==============================] - 2s 592ms/step - loss: 0.1005 - accuracy: 0.9524 - val_loss: 1.0516 - val_accuracy: 0.8667\n",
    "Epoch 385/500\n",
    "3/3 [==============================] - 2s 457ms/step - loss: 0.1240 - accuracy: 0.9583 - val_loss: 1.0106 - val_accuracy: 0.8667\n",
    "Epoch 386/500\n",
    "3/3 [==============================] - 2s 598ms/step - loss: 0.0406 - accuracy: 0.9940 - val_loss: 1.0199 - val_accuracy: 0.8667\n",
    "Epoch 387/500\n",
    "3/3 [==============================] - 2s 470ms/step - loss: 0.0985 - accuracy: 0.9702 - val_loss: 1.0132 - val_accuracy: 0.8667\n",
    "Epoch 388/500\n",
    "3/3 [==============================] - 2s 477ms/step - loss: 0.0931 - accuracy: 0.9643 - val_loss: 1.0143 - val_accuracy: 0.8667\n",
    "Epoch 389/500\n",
    "3/3 [==============================] - 2s 478ms/step - loss: 0.1154 - accuracy: 0.9643 - val_loss: 1.0886 - val_accuracy: 0.8889\n",
    "Epoch 390/500\n",
    "3/3 [==============================] - 2s 455ms/step - loss: 0.0884 - accuracy: 0.9643 - val_loss: 1.2150 - val_accuracy: 0.8444\n",
    "Epoch 391/500\n",
    "3/3 [==============================] - 2s 605ms/step - loss: 0.1182 - accuracy: 0.9583 - val_loss: 1.4039 - val_accuracy: 0.8222\n",
    "Epoch 392/500\n",
    "3/3 [==============================] - 2s 453ms/step - loss: 0.0541 - accuracy: 0.9762 - val_loss: 1.5121 - val_accuracy: 0.8222\n",
    "Epoch 393/500\n",
    "3/3 [==============================] - 2s 448ms/step - loss: 0.0505 - accuracy: 0.9762 - val_loss: 1.5845 - val_accuracy: 0.8000\n",
    "Epoch 394/500\n",
    "3/3 [==============================] - 2s 451ms/step - loss: 0.1137 - accuracy: 0.9583 - val_loss: 1.5676 - val_accuracy: 0.8000\n",
    "Epoch 395/500\n",
    "3/3 [==============================] - 2s 477ms/step - loss: 0.1254 - accuracy: 0.9524 - val_loss: 1.5151 - val_accuracy: 0.7778\n",
    "Epoch 396/500\n",
    "3/3 [==============================] - 2s 455ms/step - loss: 0.0654 - accuracy: 0.9762 - val_loss: 1.5340 - val_accuracy: 0.7556\n",
    "Epoch 397/500\n",
    "3/3 [==============================] - 2s 449ms/step - loss: 0.1295 - accuracy: 0.9524 - val_loss: 1.5082 - val_accuracy: 0.7556\n",
    "Epoch 398/500\n",
    "3/3 [==============================] - 2s 447ms/step - loss: 0.1346 - accuracy: 0.9643 - val_loss: 1.4397 - val_accuracy: 0.7778\n",
    "Epoch 399/500\n",
    "3/3 [==============================] - 2s 601ms/step - loss: 0.0886 - accuracy: 0.9643 - val_loss: 1.4403 - val_accuracy: 0.7778\n",
    "Epoch 400/500\n",
    "3/3 [==============================] - 2s 468ms/step - loss: 0.1456 - accuracy: 0.9405 - val_loss: 1.4118 - val_accuracy: 0.7778\n",
    "Epoch 401/500\n",
    "3/3 [==============================] - 2s 471ms/step - loss: 0.1570 - accuracy: 0.9405 - val_loss: 1.2644 - val_accuracy: 0.8222\n",
    "Epoch 402/500\n",
    "3/3 [==============================] - 2s 473ms/step - loss: 0.1506 - accuracy: 0.9405 - val_loss: 1.0515 - val_accuracy: 0.8222\n",
    "Epoch 403/500\n",
    "3/3 [==============================] - 2s 470ms/step - loss: 0.1194 - accuracy: 0.9583 - val_loss: 0.9491 - val_accuracy: 0.8444\n",
    "Epoch 404/500\n",
    "3/3 [==============================] - 2s 592ms/step - loss: 0.0692 - accuracy: 0.9881 - val_loss: 0.8797 - val_accuracy: 0.8667\n",
    "Epoch 405/500\n",
    "3/3 [==============================] - 2s 594ms/step - loss: 0.1366 - accuracy: 0.9405 - val_loss: 0.9608 - val_accuracy: 0.8889\n",
    "Epoch 406/500\n",
    "3/3 [==============================] - 2s 455ms/step - loss: 0.0912 - accuracy: 0.9583 - val_loss: 1.0438 - val_accuracy: 0.8667\n",
    "Epoch 407/500\n",
    "3/3 [==============================] - 2s 595ms/step - loss: 0.0957 - accuracy: 0.9583 - val_loss: 1.1013 - val_accuracy: 0.8444\n",
    "Epoch 408/500\n",
    "3/3 [==============================] - 2s 449ms/step - loss: 0.1111 - accuracy: 0.9524 - val_loss: 1.0957 - val_accuracy: 0.8444\n",
    "Epoch 409/500\n",
    "3/3 [==============================] - 2s 469ms/step - loss: 0.1281 - accuracy: 0.9583 - val_loss: 1.0971 - val_accuracy: 0.8444\n",
    "Epoch 410/500\n",
    "3/3 [==============================] - 2s 595ms/step - loss: 0.0928 - accuracy: 0.9762 - val_loss: 1.1900 - val_accuracy: 0.8000\n",
    "Epoch 411/500\n",
    "3/3 [==============================] - 2s 446ms/step - loss: 0.1608 - accuracy: 0.9405 - val_loss: 1.2915 - val_accuracy: 0.8222\n",
    "Epoch 412/500\n",
    "3/3 [==============================] - 2s 471ms/step - loss: 0.0564 - accuracy: 0.9821 - val_loss: 1.3403 - val_accuracy: 0.8222\n",
    "Epoch 413/500\n",
    "3/3 [==============================] - 2s 469ms/step - loss: 0.0562 - accuracy: 0.9821 - val_loss: 1.3553 - val_accuracy: 0.8444\n",
    "Epoch 414/500\n",
    "3/3 [==============================] - 2s 447ms/step - loss: 0.0911 - accuracy: 0.9702 - val_loss: 1.2509 - val_accuracy: 0.8444\n",
    "Epoch 415/500\n",
    "3/3 [==============================] - 2s 467ms/step - loss: 0.1010 - accuracy: 0.9821 - val_loss: 1.1097 - val_accuracy: 0.8444\n",
    "Epoch 416/500\n",
    "3/3 [==============================] - 2s 447ms/step - loss: 0.0920 - accuracy: 0.9583 - val_loss: 1.0546 - val_accuracy: 0.8667\n",
    "Epoch 417/500\n",
    "3/3 [==============================] - 2s 469ms/step - loss: 0.0700 - accuracy: 0.9762 - val_loss: 1.0489 - val_accuracy: 0.8667\n",
    "Epoch 418/500\n",
    "3/3 [==============================] - 2s 591ms/step - loss: 0.1380 - accuracy: 0.9405 - val_loss: 1.1060 - val_accuracy: 0.8667\n",
    "Epoch 419/500\n",
    "3/3 [==============================] - 2s 598ms/step - loss: 0.0516 - accuracy: 0.9821 - val_loss: 1.1197 - val_accuracy: 0.8667\n",
    "Epoch 420/500\n",
    "3/3 [==============================] - 2s 598ms/step - loss: 0.1373 - accuracy: 0.9524 - val_loss: 1.1134 - val_accuracy: 0.8667\n",
    "Epoch 421/500\n",
    "3/3 [==============================] - 2s 468ms/step - loss: 0.0614 - accuracy: 0.9762 - val_loss: 13.6690 - val_accuracy: 0.4000\n",
    "Epoch 422/500\n",
    "3/3 [==============================] - 2s 474ms/step - loss: 0.0686 - accuracy: 0.9702 - val_loss: 5.8681 - val_accuracy: 0.5778\n",
    "Epoch 423/500\n",
    "3/3 [==============================] - 2s 599ms/step - loss: 0.0943 - accuracy: 0.9643 - val_loss: 4.8599 - val_accuracy: 0.5778\n",
    "Epoch 424/500\n",
    "3/3 [==============================] - 2s 588ms/step - loss: 0.0466 - accuracy: 0.9762 - val_loss: 3.9209 - val_accuracy: 0.5333\n",
    "Epoch 425/500\n",
    "3/3 [==============================] - 2s 449ms/step - loss: 0.1350 - accuracy: 0.9524 - val_loss: 3.9461 - val_accuracy: 0.5556\n",
    "Epoch 426/500\n",
    "3/3 [==============================] - 2s 450ms/step - loss: 0.0545 - accuracy: 0.9702 - val_loss: 3.9060 - val_accuracy: 0.5556\n",
    "Epoch 427/500\n",
    "3/3 [==============================] - 2s 468ms/step - loss: 0.1220 - accuracy: 0.9762 - val_loss: 3.3075 - val_accuracy: 0.6444\n",
    "Epoch 428/500\n",
    "3/3 [==============================] - 2s 593ms/step - loss: 0.0658 - accuracy: 0.9821 - val_loss: 2.8196 - val_accuracy: 0.7333\n",
    "Epoch 429/500\n",
    "3/3 [==============================] - 2s 464ms/step - loss: 0.1684 - accuracy: 0.9583 - val_loss: 2.5223 - val_accuracy: 0.7333\n",
    "Epoch 430/500\n",
    "3/3 [==============================] - 2s 452ms/step - loss: 0.0583 - accuracy: 0.9702 - val_loss: 2.4046 - val_accuracy: 0.7111\n",
    "Epoch 431/500\n",
    "3/3 [==============================] - 2s 473ms/step - loss: 0.1524 - accuracy: 0.9464 - val_loss: 2.3523 - val_accuracy: 0.7111\n",
    "Epoch 432/500\n",
    "3/3 [==============================] - 2s 453ms/step - loss: 0.1110 - accuracy: 0.9583 - val_loss: 2.0408 - val_accuracy: 0.7778\n",
    "Epoch 433/500\n",
    "3/3 [==============================] - 2s 601ms/step - loss: 0.0604 - accuracy: 0.9881 - val_loss: 1.7517 - val_accuracy: 0.8000\n",
    "Epoch 434/500\n",
    "\n",
    "3/3 [==============================] - 2s 467ms/step - loss: 0.0904 - accuracy: 0.9702 - val_loss: 1.5095 - val_accuracy: 0.8000\n",
    "Epoch 435/500\n",
    "3/3 [==============================] - 2s 471ms/step - loss: 0.1050 - accuracy: 0.9583 - val_loss: 1.4919 - val_accuracy: 0.8000\n",
    "Epoch 436/500\n",
    "3/3 [==============================] - 2s 448ms/step - loss: 0.0568 - accuracy: 0.9762 - val_loss: 1.5300 - val_accuracy: 0.8000\n",
    "Epoch 437/500\n",
    "3/3 [==============================] - 2s 447ms/step - loss: 0.0371 - accuracy: 0.9881 - val_loss: 1.5950 - val_accuracy: 0.8222\n",
    "Epoch 438/500\n",
    "3/3 [==============================] - 2s 447ms/step - loss: 0.0298 - accuracy: 0.9940 - val_loss: 1.7128 - val_accuracy: 0.7778\n",
    "Epoch 439/500\n",
    "3/3 [==============================] - 2s 471ms/step - loss: 0.1104 - accuracy: 0.9524 - val_loss: 1.6871 - val_accuracy: 0.7778\n",
    "Epoch 440/500\n",
    "3/3 [==============================] - 2s 596ms/step - loss: 0.0786 - accuracy: 0.9702 - val_loss: 1.5096 - val_accuracy: 0.7778\n",
    "Epoch 441/500\n",
    "3/3 [==============================] - 2s 472ms/step - loss: 0.0595 - accuracy: 0.9762 - val_loss: 1.3824 - val_accuracy: 0.7778\n",
    "Epoch 442/500\n",
    "3/3 [==============================] - 2s 449ms/step - loss: 0.0949 - accuracy: 0.9583 - val_loss: 1.3933 - val_accuracy: 0.7778\n",
    "Epoch 443/500\n",
    "3/3 [==============================] - 2s 450ms/step - loss: 0.0317 - accuracy: 0.9881 - val_loss: 1.3797 - val_accuracy: 0.7778\n",
    "Epoch 444/500\n",
    "3/3 [==============================] - 2s 470ms/step - loss: 0.0500 - accuracy: 0.9762 - val_loss: 1.4716 - val_accuracy: 0.7556\n",
    "Epoch 445/500\n",
    "3/3 [==============================] - 2s 445ms/step - loss: 0.0686 - accuracy: 0.9821 - val_loss: 1.5786 - val_accuracy: 0.7333\n",
    "Epoch 446/500\n",
    "3/3 [==============================] - 2s 456ms/step - loss: 0.0513 - accuracy: 0.9702 - val_loss: 1.5856 - val_accuracy: 0.7333\n",
    "Epoch 447/500\n",
    "3/3 [==============================] - 2s 587ms/step - loss: 0.0448 - accuracy: 0.9821 - val_loss: 1.5090 - val_accuracy: 0.7333\n",
    "Epoch 448/500\n",
    "3/3 [==============================] - 2s 475ms/step - loss: 0.0848 - accuracy: 0.9583 - val_loss: 1.3194 - val_accuracy: 0.7333\n",
    "Epoch 449/500\n",
    "3/3 [==============================] - 2s 596ms/step - loss: 0.1001 - accuracy: 0.9643 - val_loss: 1.2646 - val_accuracy: 0.7556\n",
    "Epoch 450/500\n",
    "3/3 [==============================] - 2s 466ms/step - loss: 0.0806 - accuracy: 0.9643 - val_loss: 1.2933 - val_accuracy: 0.8000\n",
    "Epoch 451/500\n",
    "3/3 [==============================] - 2s 454ms/step - loss: 0.0804 - accuracy: 0.9702 - val_loss: 1.2811 - val_accuracy: 0.7778\n",
    "Epoch 452/500\n",
    "3/3 [==============================] - 2s 457ms/step - loss: 0.1164 - accuracy: 0.9464 - val_loss: 1.2131 - val_accuracy: 0.8000\n",
    "Epoch 453/500\n",
    "3/3 [==============================] - 2s 471ms/step - loss: 0.0497 - accuracy: 0.9821 - val_loss: 1.2035 - val_accuracy: 0.8222\n",
    "Epoch 454/500\n",
    "3/3 [==============================] - 2s 471ms/step - loss: 0.0905 - accuracy: 0.9583 - val_loss: 1.1745 - val_accuracy: 0.8667\n",
    "Epoch 455/500\n",
    "3/3 [==============================] - 2s 458ms/step - loss: 0.1432 - accuracy: 0.9524 - val_loss: 1.0843 - val_accuracy: 0.8667\n",
    "Epoch 456/500\n",
    "3/3 [==============================] - 2s 472ms/step - loss: 0.0764 - accuracy: 0.9821 - val_loss: 0.9656 - val_accuracy: 0.8667\n",
    "Epoch 457/500\n",
    "3/3 [==============================] - 2s 591ms/step - loss: 0.0563 - accuracy: 0.9821 - val_loss: 0.8357 - val_accuracy: 0.8889\n",
    "Epoch 458/500\n",
    "3/3 [==============================] - 2s 596ms/step - loss: 0.0799 - accuracy: 0.9762 - val_loss: 0.7987 - val_accuracy: 0.8889\n",
    "Epoch 459/500\n",
    "3/3 [==============================] - 2s 449ms/step - loss: 0.0595 - accuracy: 0.9881 - val_loss: 0.7979 - val_accuracy: 0.8889\n",
    "Epoch 460/500\n",
    "3/3 [==============================] - 2s 455ms/step - loss: 0.0919 - accuracy: 0.9524 - val_loss: 0.8317 - val_accuracy: 0.8889\n",
    "Epoch 461/500\n",
    "3/3 [==============================] - 2s 602ms/step - loss: 0.0550 - accuracy: 0.9881 - val_loss: 0.8925 - val_accuracy: 0.8889\n",
    "Epoch 462/500\n",
    "3/3 [==============================] - 2s 601ms/step - loss: 0.0468 - accuracy: 0.9881 - val_loss: 0.9460 - val_accuracy: 0.8889\n",
    "Epoch 463/500\n",
    "3/3 [==============================] - 2s 456ms/step - loss: 0.1399 - accuracy: 0.9345 - val_loss: 0.9962 - val_accuracy: 0.8889\n",
    "Epoch 464/500\n",
    "3/3 [==============================] - 2s 457ms/step - loss: 0.0540 - accuracy: 0.9821 - val_loss: 1.0794 - val_accuracy: 0.8889\n",
    "Epoch 465/500\n",
    "3/3 [==============================] - 2s 470ms/step - loss: 0.0471 - accuracy: 0.9881 - val_loss: 1.1480 - val_accuracy: 0.8667\n",
    "Epoch 466/500\n",
    "3/3 [==============================] - 2s 591ms/step - loss: 0.0726 - accuracy: 0.9702 - val_loss: 1.2248 - val_accuracy: 0.8444\n",
    "Epoch 467/500\n",
    "3/3 [==============================] - 2s 589ms/step - loss: 0.0427 - accuracy: 0.9821 - val_loss: 1.2719 - val_accuracy: 0.8000\n",
    "Epoch 468/500\n",
    "3/3 [==============================] - 2s 448ms/step - loss: 0.0881 - accuracy: 0.9762 - val_loss: 1.3295 - val_accuracy: 0.8000\n",
    "Epoch 469/500\n",
    "3/3 [==============================] - 2s 592ms/step - loss: 0.0929 - accuracy: 0.9702 - val_loss: 1.3455 - val_accuracy: 0.8000\n",
    "Epoch 470/500\n",
    "3/3 [==============================] - 2s 467ms/step - loss: 0.1175 - accuracy: 0.9821 - val_loss: 1.3690 - val_accuracy: 0.8000\n",
    "Epoch 471/500\n",
    "3/3 [==============================] - 2s 468ms/step - loss: 0.0609 - accuracy: 0.9821 - val_loss: 1.3723 - val_accuracy: 0.8000\n",
    "Epoch 472/500\n",
    "3/3 [==============================] - 2s 455ms/step - loss: 0.0956 - accuracy: 0.9821 - val_loss: 1.3222 - val_accuracy: 0.8000\n",
    "Epoch 473/500\n",
    "3/3 [==============================] - 2s 450ms/step - loss: 0.0776 - accuracy: 0.9762 - val_loss: 1.2003 - val_accuracy: 0.8667\n",
    "Epoch 474/500\n",
    "3/3 [==============================] - 2s 442ms/step - loss: 0.0285 - accuracy: 0.9940 - val_loss: 1.1530 - val_accuracy: 0.8667\n",
    "Epoch 475/500\n",
    "3/3 [==============================] - 2s 473ms/step - loss: 0.0961 - accuracy: 0.9762 - val_loss: 1.1448 - val_accuracy: 0.8889\n",
    "Epoch 476/500\n",
    "3/3 [==============================] - 2s 592ms/step - loss: 0.0398 - accuracy: 0.9881 - val_loss: 1.1500 - val_accuracy: 0.8667\n",
    "Epoch 477/500\n",
    "3/3 [==============================] - 2s 467ms/step - loss: 0.0663 - accuracy: 0.9762 - val_loss: 1.1235 - val_accuracy: 0.8667\n",
    "Epoch 478/500\n",
    "3/3 [==============================] - 2s 471ms/step - loss: 0.0245 - accuracy: 0.9940 - val_loss: 1.0839 - val_accuracy: 0.8667\n",
    "Epoch 479/500\n",
    "3/3 [==============================] - 2s 478ms/step - loss: 0.0789 - accuracy: 0.9762 - val_loss: 1.0552 - val_accuracy: 0.8667\n",
    "Epoch 480/500\n",
    "3/3 [==============================] - 2s 448ms/step - loss: 0.0921 - accuracy: 0.9702 - val_loss: 1.0941 - val_accuracy: 0.8667\n",
    "Epoch 481/500\n",
    "3/3 [==============================] - 2s 463ms/step - loss: 0.0618 - accuracy: 0.9762 - val_loss: 1.2575 - val_accuracy: 0.8667\n",
    "Epoch 482/500\n",
    "3/3 [==============================] - 2s 592ms/step - loss: 0.0939 - accuracy: 0.9583 - val_loss: 1.5132 - val_accuracy: 0.8000\n",
    "Epoch 483/500\n",
    "3/3 [==============================] - 2s 453ms/step - loss: 0.0634 - accuracy: 0.9702 - val_loss: 1.7567 - val_accuracy: 0.8000\n",
    "Epoch 484/500\n",
    "3/3 [==============================] - 2s 601ms/step - loss: 0.0476 - accuracy: 0.9881 - val_loss: 1.9212 - val_accuracy: 0.8000\n",
    "Epoch 485/500\n",
    "3/3 [==============================] - 2s 464ms/step - loss: 0.1095 - accuracy: 0.9524 - val_loss: 2.0156 - val_accuracy: 0.8000\n",
    "Epoch 486/500\n",
    "3/3 [==============================] - 2s 473ms/step - loss: 0.0756 - accuracy: 0.9643 - val_loss: 2.0961 - val_accuracy: 0.8000\n",
    "Epoch 487/500\n",
    "3/3 [==============================] - 2s 594ms/step - loss: 0.0761 - accuracy: 0.9702 - val_loss: 2.1395 - val_accuracy: 0.8000\n",
    "Epoch 488/500\n",
    "3/3 [==============================] - 2s 449ms/step - loss: 0.0956 - accuracy: 0.9702 - val_loss: 2.1005 - val_accuracy: 0.8000\n",
    "Epoch 489/500\n",
    "3/3 [==============================] - 2s 449ms/step - loss: 0.0702 - accuracy: 0.9583 - val_loss: 2.0330 - val_accuracy: 0.7778\n",
    "Epoch 490/500\n",
    "3/3 [==============================] - 2s 599ms/step - loss: 0.0803 - accuracy: 0.9702 - val_loss: 1.9847 - val_accuracy: 0.8000\n",
    "Epoch 491/500\n",
    "\n",
    "3/3 [==============================] - 2s 455ms/step - loss: 0.0663 - accuracy: 0.9821 - val_loss: 1.9031 - val_accuracy: 0.8000\n",
    "Epoch 492/500\n",
    "3/3 [==============================] - 2s 591ms/step - loss: 0.0308 - accuracy: 0.9940 - val_loss: 1.7860 - val_accuracy: 0.8222\n",
    "Epoch 493/500\n",
    "3/3 [==============================] - 2s 452ms/step - loss: 0.0324 - accuracy: 0.9821 - val_loss: 1.6475 - val_accuracy: 0.8667\n",
    "Epoch 494/500\n",
    "3/3 [==============================] - 2s 593ms/step - loss: 0.0601 - accuracy: 0.9881 - val_loss: 1.5869 - val_accuracy: 0.8667\n",
    "Epoch 495/500\n",
    "3/3 [==============================] - 2s 468ms/step - loss: 0.0372 - accuracy: 0.9821 - val_loss: 1.5401 - val_accuracy: 0.8667\n",
    "Epoch 496/500\n",
    "3/3 [==============================] - 2s 587ms/step - loss: 0.0298 - accuracy: 0.9881 - val_loss: 1.4452 - val_accuracy: 0.8667\n",
    "Epoch 497/500\n",
    "3/3 [==============================] - 2s 474ms/step - loss: 0.0495 - accuracy: 0.9762 - val_loss: 1.3420 - val_accuracy: 0.8667\n",
    "Epoch 498/500\n",
    "3/3 [==============================] - 2s 451ms/step - loss: 0.0712 - accuracy: 0.9702 - val_loss: 1.2597 - val_accuracy: 0.8444\n",
    "Epoch 499/500\n",
    "3/3 [==============================] - 2s 471ms/step - loss: 0.0496 - accuracy: 0.9702 - val_loss: 1.1915 - val_accuracy: 0.8444\n",
    "Epoch 500/500\n",
    "3/3 [==============================] - 2s 445ms/step - loss: 0.0544 - accuracy: 0.9762 - val_loss: 1.1459 - val_accuracy: 0.8444\n",
    "{'loss': [3.159762144088745, 2.435786247253418, 1.9866918325424194, 2.052199125289917, 2.0506556034088135, 1.5486849546432495, 1.762140154838562, 1.3704887628555298, 1.7724360227584839, 1.439967393875122, 1.383676290512085, 1.2711118459701538, 1.423536777496338, 1.2500168085098267, 1.2450839281082153, 1.1545401811599731, 1.0436686277389526, 1.1108039617538452, 0.9770886301994324, 0.9718390703201294, 0.976006269454956, 0.9270831346511841, 0.8374119997024536, 0.8008355498313904, 0.8121877312660217, 0.7366616129875183, 0.7986176013946533, 0.7223116755485535, 0.6804090142250061, 0.7178475260734558, 0.6619298458099365, 0.6173468232154846, 0.5106102824211121, 0.6014901995658875, 0.5114163756370544, 0.6077810525894165, 0.5511770844459534, 0.5100615620613098, 0.4564008116722107, 0.5608744025230408, 0.4008809030056, 0.5334256291389465, 0.44876334071159363, 0.4445585608482361, 0.43277567625045776, 0.4531014859676361, 0.36136409640312195, 0.3357807695865631, 0.37910810112953186, 0.3534872829914093, 0.4504890441894531, 0.43276605010032654, 0.4179131090641022, 0.4074937701225281, 0.3770923912525177, 0.33972084522247314, 0.3318474590778351, 0.3536345958709717, 0.2803642749786377, 0.33113718032836914, 0.2660260796546936, 0.3706000745296478, 0.3100034296512604, 0.21462848782539368, 0.2541368007659912, 0.4260234832763672, 0.32813161611557007, 0.28020036220550537, 0.2757668197154999, 0.2851088345050812, 0.3437916338443756, 0.24952080845832825, 0.284247487783432, 0.3550802767276764, 0.21411310136318207, 0.3281949460506439, 0.21023304760456085, 0.1897255778312683, 0.21847769618034363, 0.29227888584136963, 0.4367438852787018, 0.17326349020004272, 0.3288622200489044, 0.25337764620780945, 0.20041504502296448, 0.20378784835338593, 0.3008843660354614, 0.34014657139778137, 0.2004447728395462, 0.25592750310897827, 0.21199758350849152, 0.3489530682563782, 0.17032654583454132, 0.2276753932237625, 0.3225401043891907, 0.31114304065704346, 0.2189316749572754, 0.1420268714427948, 0.2279333770275116, 0.2985966205596924, 0.20514406263828278, 0.19212453067302704, 0.2883978486061096, 0.1495417207479477, 0.3172987103462219, 0.20995621383190155, 0.1826062798500061, 0.2689003348350525, 0.2147156000137329, 0.17921386659145355, 0.22593443095684052, 0.2666535973548889, 0.3665063977241516, 0.21811185777187347, 0.18138954043388367, 0.18994037806987762, 0.25533822178840637, 0.25822198390960693, 0.13437573611736298, 0.19266055524349213, 0.21241652965545654, 0.25074249505996704, 0.30746492743492126, 0.28528204560279846, 0.1619567722082138, 0.2621573805809021, 0.2888287901878357, 0.2029600590467453, 0.1855846792459488, 0.23018744587898254, 0.12441331148147583, 0.09603673219680786, 0.15190541744232178, 0.21057483553886414, 0.19701239466667175, 0.12327597290277481, 0.22434274852275848, 0.14199790358543396, 0.23351222276687622, 0.13630416989326477, 0.17000176012516022, 0.15381206572055817, 0.10138887166976929, 0.1580439805984497, 0.20353026688098907, 0.13226385414600372, 0.1680457442998886, 0.17491677403450012, 0.16992045938968658, 0.22959887981414795, 0.21066339313983917, 0.17852628231048584, 0.1751331090927124, 0.2526725232601166, 0.2527132034301758, 0.18031084537506104, 0.19811685383319855, 0.1343051791191101, 0.1427086591720581, 0.21428035199642181, 0.2215842306613922, 0.08727985620498657, 0.1661638468503952, 0.23357732594013214, 0.2479211539030075, 0.1807275116443634, 0.18616615235805511, 0.1426505297422409, 0.21406309306621552, 0.08567303419113159, 0.2103969007730484, 0.13779045641422272, 0.17429864406585693, 0.14841388165950775, 0.08600480109453201, 0.10587503761053085, 0.13891594111919403, 0.11265509575605392, 0.11098984628915787, 0.1300864815711975, 0.115641750395298, 0.15215975046157837, 0.1806756854057312, 0.11139282584190369, 0.11712646484375, 0.11850198358297348, 0.12840066850185394, 0.09741726517677307, 0.1925572007894516, 0.07945603877305984, 0.08219762146472931, 0.1338622123003006, 0.12389594316482544, 0.13988906145095825, 0.09707231819629669, 0.11650130897760391, 0.09919942915439606, 0.19005842506885529, 0.1310538351535797, 0.14340868592262268, 0.12411811202764511, 0.07906664907932281, 0.170060396194458, 0.089582659304142, 0.12401796877384186, 0.07546990364789963, 0.13351497054100037, 0.0745706558227539, 0.11582545191049576, 0.13947813212871552, 0.15589864552021027, 0.056418344378471375, 0.09054132550954819, 0.11954279989004135, 0.12463869154453278, 0.10245568305253983, 0.09418968856334686, 0.07178566604852676, 0.12423662841320038, 0.06859520822763443, 0.1344943344593048, 0.1875109076499939, 0.11942361295223236, 0.11746281385421753, 0.09015610069036484, 0.1087733581662178, 0.08172065019607544, 0.09976078569889069, 0.08415858447551727, 0.053226958960294724, 0.1270126849412918, 0.038746628910303116, 0.09385291486978531, 0.14333459734916687, 0.10430631041526794, 0.06780970096588135, 0.11966913193464279, 0.06160351261496544, 0.10786817222833633, 0.13283796608448029, 0.16080202162265778, 0.130717933177948, 0.12460195273160934, 0.23090305924415588, 0.1545901596546173, 0.08208491653203964, 0.09312492609024048, 0.08606356382369995, 0.06688271462917328, 0.17546431720256805, 0.12717841565608978, 0.07777933776378632, 0.10352227836847305, 0.06893614679574966, 0.07874511182308197, 0.1401638239622116, 0.08709175884723663, 0.16297896206378937, 0.0685882568359375, 0.08672446012496948, 0.10196653753519058, 0.12422563880681992, 0.0965270847082138, 0.06041804328560829, 0.0971917062997818, 0.10595068335533142, 0.0950634554028511, 0.12263321876525879, 0.06471860408782959, 0.1555580347776413, 0.05547114834189415, 0.0796697735786438, 0.2021249532699585, 0.07575012743473053, 0.13100378215312958, 0.1309184432029724, 0.1485622525215149, 0.07891489565372467, 0.11061063408851624, 0.1003619134426117, 0.12821538746356964, 0.05041584372520447, 0.10052167624235153, 0.10880528390407562, 0.12099844217300415, 0.09822234511375427, 0.12053126096725464, 0.10520610213279724, 0.0973609983921051, 0.14372368156909943, 0.04127325862646103, 0.14091679453849792, 0.05275500565767288, 0.09257365763187408, 0.13828608393669128, 0.11961379647254944, 0.06120120733976364, 0.16736681759357452, 0.08822302520275116, 0.1493862420320511, 0.12268590927124023, 0.07362905889749527, 0.09467596560716629, 0.1313207894563675, 0.10639949142932892, 0.10977499932050705, 0.10209205001592636, 0.10144227743148804, 0.1419578194618225, 0.13067972660064697, 0.09888146072626114, 0.06245671957731247, 0.06529106199741364, 0.06567953526973724, 0.07606659084558487, 0.07965832948684692, 0.11033686995506287, 0.13107092678546906, 0.12014647573232651, 0.05610526725649834, 0.12813438475131989, 0.09550625830888748, 0.12044189125299454, 0.14245057106018066, 0.07507835328578949, 0.06498934328556061, 0.07695119827985764, 0.07134503126144409, 0.1107880249619484, 0.0728251114487648, 0.1283162385225296, 0.15793630480766296, 0.12263432145118713, 0.08421814441680908, 0.07965108007192612, 0.09766001999378204, 0.11230061203241348, 0.08000999689102173, 0.10390481352806091, 0.07217319309711456, 0.05756508186459541, 0.11190150678157806, 0.11359918862581253, 0.11052441596984863, 0.05153230205178261, 0.057544175535440445, 0.05956104397773743, 0.1474173367023468, 0.10040601342916489, 0.07321425527334213, 0.10812438279390335, 0.05176813155412674, 0.06571721285581589, 0.07863431423902512, 0.0836540088057518, 0.12661311030387878, 0.1258789300918579, 0.0716453269124031, 0.048409201204776764, 0.10096825659275055, 0.07196559011936188, 0.08174949884414673, 0.05659976974129677, 0.06947282701730728, 0.1315174400806427, 0.10902253538370132, 0.04582490399479866, 0.058972541242837906, 0.04870527237653732, 0.0831611305475235, 0.05521652102470398, 0.10906579345464706, 0.0665777251124382, 0.07082799822092056, 0.06418374180793762, 0.04515910893678665, 0.09982658922672272, 0.014881342649459839, 0.11858218163251877, 0.10775371640920639, 0.09190312772989273, 0.09953979402780533, 0.12756340205669403, 0.10053985565900803, 0.12398897856473923, 0.040573831647634506, 0.09849802404642105, 0.09312470257282257, 0.11541084200143814, 0.08835498243570328, 0.1181526854634285, 0.054143548011779785, 0.0505090057849884, 0.11370024085044861, 0.1254417598247528, 0.0654107853770256, 0.12951840460300446, 0.1346379965543747, 0.08858916908502579, 0.1456315815448761, 0.1570136845111847, 0.15059907734394073, 0.11935549974441528, 0.06920138001441956, 0.1366323083639145, 0.09122644364833832, 0.09572149813175201, 0.11105173081159592, 0.12814396619796753, 0.09278304129838943, 0.16077591478824615, 0.05643037334084511, 0.05621422454714775, 0.09114288538694382, 0.10096322745084763, 0.0919807106256485, 0.06996950507164001, 0.13803298771381378, 0.05158151313662529, 0.13730409741401672, 0.061387162655591965, 0.06855498999357224, 0.09425273537635803, 0.04655253142118454, 0.13501796126365662, 0.05446193367242813, 0.12203001976013184, 0.06577689200639725, 0.16838915646076202, 0.05827023461461067, 0.1523764431476593, 0.11098428070545197, 0.06037349998950958, 0.09043611586093903, 0.10500498861074448, 0.056808069348335266, 0.037125539034605026, 0.029808510094881058, 0.11038485914468765, 0.07856154441833496, 0.059495408087968826, 0.09487151354551315, 0.03170866519212723, 0.049955278635025024, 0.06864577531814575, 0.05133156105875969, 0.04483846202492714, 0.08477415889501572, 0.10006347298622131, 0.08058173954486847, 0.08042776584625244, 0.11640332639217377, 0.04974333941936493, 0.09046530723571777, 0.14319156110286713, 0.07641971856355667, 0.05630964785814285, 0.07994731515645981, 0.059544555842876434, 0.09185105562210083, 0.05503372848033905, 0.04681047424674034, 0.1398923695087433, 0.05399113520979881, 0.04712597653269768, 0.07264252752065659, 0.042661961168050766, 0.08805001527070999, 0.09292404353618622, 0.11748085916042328, 0.06086648628115654, 0.09564722329378128, 0.0775810107588768, 0.028546929359436035, 0.09614542871713638, 0.03984623774886131, 0.066314198076725, 0.024526283144950867, 0.0788864940404892, 0.09209401160478592, 0.061800070106983185, 0.09390290081501007, 0.06336110830307007, 0.047573965042829514, 0.10951165854930878, 0.07557474076747894, 0.07613487541675568, 0.09561572968959808, 0.07019460946321487, 0.08025459945201874, 0.06630724668502808, 0.03083411231637001, 0.032397422939538956, 0.06011601909995079, 0.037163786590099335, 0.029763801023364067, 0.049523863941431046, 0.07122236490249634, 0.04959798976778984, 0.05442694202065468], 'accuracy': [0.1428571492433548, 0.261904776096344, 0.3511904776096344, 0.3095238208770752, 0.369047611951828, 0.4285714328289032, 0.4047619104385376, 0.4642857015132904, 0.375, 0.4642857015132904, 0.4464285671710968, 0.488095223903656, 0.488095223903656, 0.5416666865348816, 0.5297619104385376, 0.6071428656578064, 0.5833333134651184, 0.5952380895614624, 0.6666666865348816, 0.648809552192688, 0.636904776096344, 0.6428571343421936, 0.6964285969734192, 0.6845238208770752, 0.7202380895614624, 0.761904776096344, 0.6964285969734192, 0.7321428656578064, 0.7321428656578064, 0.7083333134651184, 0.761904776096344, 0.7678571343421936, 0.8095238208770752, 0.761904776096344, 0.8095238208770752, 0.761904776096344, 0.7976190447807312, 0.7857142686843872, 0.8571428656578064, 0.8273809552192688, 0.8452380895614624, 0.7797619104385376, 0.8392857313156128, 0.8392857313156128, 0.8571428656578064, 0.7976190447807312, 0.851190447807312, 0.898809552192688, 0.851190447807312, 0.8452380895614624, 0.8273809552192688, 0.8452380895614624, 0.8809523582458496, 0.863095223903656, 0.8690476417541504, 0.8690476417541504, 0.8809523582458496, 0.8809523582458496, 0.875, 0.886904776096344, 0.9166666865348816, 0.863095223903656, 0.8571428656578064, 0.9166666865348816, 0.8928571343421936, 0.8571428656578064, 0.898809552192688, 0.898809552192688, 0.8809523582458496, 0.898809552192688, 0.8690476417541504, 0.9107142686843872, 0.9047619104385376, 0.8690476417541504, 0.9166666865348816, 0.8928571343421936, 0.9226190447807312, 0.9345238208770752, 0.9285714030265808, 0.898809552192688, 0.886904776096344, 0.9345238208770752, 0.8928571343421936, 0.898809552192688, 0.9464285969734192, 0.9404761791229248, 0.886904776096344, 0.8809523582458496, 0.9285714030265808, 0.9166666865348816, 0.9404761791229248, 0.886904776096344, 0.9345238208770752, 0.9166666865348816, 0.8809523582458496, 0.9047619104385376, 0.9285714030265808, 0.9523809552192688, 0.9345238208770752, 0.9107142686843872, 0.9345238208770752, 0.9166666865348816, 0.9166666865348816, 0.9583333134651184, 0.8690476417541504, 0.9285714030265808, 0.9345238208770752, 0.886904776096344, 0.9345238208770752, 0.9404761791229248, 0.9345238208770752, 0.9047619104385376, 0.863095223903656, 0.9166666865348816, 0.9166666865348816, 0.9583333134651184, 0.886904776096344, 0.9285714030265808, 0.9523809552192688, 0.9404761791229248, 0.9166666865348816, 0.9047619104385376, 0.9107142686843872, 0.9107142686843872, 0.9345238208770752, 0.886904776096344, 0.875, 0.9345238208770752, 0.9345238208770752, 0.8928571343421936, 0.9583333134651184, 0.9642857313156128, 0.9404761791229248, 0.9285714030265808, 0.9166666865348816, 0.9404761791229248, 0.9166666865348816, 0.9642857313156128, 0.9226190447807312, 0.9523809552192688, 0.9226190447807312, 0.9583333134651184, 0.9642857313156128, 0.9523809552192688, 0.9464285969734192, 0.9404761791229248, 0.9523809552192688, 0.9345238208770752, 0.9285714030265808, 0.9285714030265808, 0.9464285969734192, 0.9523809552192688, 0.9285714030265808, 0.9166666865348816, 0.898809552192688, 0.9404761791229248, 0.9285714030265808, 0.9464285969734192, 0.9404761791229248, 0.9285714030265808, 0.9166666865348816, 0.976190447807312, 0.9523809552192688, 0.9226190447807312, 0.9107142686843872, 0.9523809552192688, 0.9345238208770752, 0.9642857313156128, 0.9226190447807312, 0.9702380895614624, 0.9107142686843872, 0.9404761791229248, 0.9404761791229248, 0.9464285969734192, 0.9642857313156128, 0.9464285969734192, 0.9464285969734192, 0.9464285969734192, 0.9642857313156128, 0.9464285969734192, 0.9702380895614624, 0.9404761791229248, 0.9285714030265808, 0.9702380895614624, 0.9583333134651184, 0.9642857313156128, 0.9404761791229248, 0.9642857313156128, 0.9285714030265808, 0.9523809552192688, 0.976190447807312, 0.9345238208770752, 0.9404761791229248, 0.9464285969734192, 0.9702380895614624, 0.9642857313156128, 0.9702380895614624, 0.9345238208770752, 0.9523809552192688, 0.9404761791229248, 0.9583333134651184, 0.9642857313156128, 0.9523809552192688, 0.9642857313156128, 0.9642857313156128, 0.9821428656578064, 0.9583333134651184, 0.9821428656578064, 0.9583333134651184, 0.9464285969734192, 0.9583333134651184, 0.976190447807312, 0.9642857313156128, 0.9464285969734192, 0.9523809552192688, 0.976190447807312, 0.9583333134651184, 0.988095223903656, 0.9583333134651184, 0.9702380895614624, 0.9642857313156128, 0.9464285969734192, 0.9583333134651184, 0.9702380895614624, 0.9642857313156128, 0.9642857313156128, 0.9702380895614624, 0.9642857313156128, 0.9702380895614624, 0.988095223903656, 0.9404761791229248, 0.9940476417541504, 0.9583333134651184, 0.9642857313156128, 0.9583333134651184, 0.976190447807312, 0.9583333134651184, 0.9821428656578064, 0.9464285969734192, 0.9583333134651184, 0.9464285969734192, 0.9523809552192688, 0.9642857313156128, 0.9285714030265808, 0.9702380895614624, 0.9702380895614624, 0.9583333134651184, 0.9702380895614624, 0.976190447807312, 0.9464285969734192, 0.9583333134651184, 0.976190447807312, 0.9583333134651184, 0.9702380895614624, 0.976190447807312, 0.9523809552192688, 0.976190447807312, 0.9345238208770752, 0.9702380895614624, 0.9702380895614624, 0.9523809552192688, 0.9583333134651184, 0.9702380895614624, 0.988095223903656, 0.9464285969734192, 0.9583333134651184, 0.9583333134651184, 0.9464285969734192, 0.9821428656578064, 0.9285714030265808, 0.9821428656578064, 0.9702380895614624, 0.9226190447807312, 0.9821428656578064, 0.9523809552192688, 0.9523809552192688, 0.9523809552192688, 0.9702380895614624, 0.9523809552192688, 0.9642857313156128, 0.9583333134651184, 0.988095223903656, 0.9702380895614624, 0.9523809552192688, 0.9702380895614624, 0.9642857313156128, 0.9642857313156128, 0.9583333134651184, 0.976190447807312, 0.9583333134651184, 0.9821428656578064, 0.9583333134651184, 0.9702380895614624, 0.9642857313156128, 0.9345238208770752, 0.9464285969734192, 0.976190447807312, 0.9285714030265808, 0.9702380895614624, 0.9523809552192688, 0.9523809552192688, 0.976190447807312, 0.9642857313156128, 0.9345238208770752, 0.9702380895614624, 0.9642857313156128, 0.9642857313156128, 0.9642857313156128, 0.9464285969734192, 0.9642857313156128, 0.9583333134651184, 0.976190447807312, 0.976190447807312, 0.9702380895614624, 0.988095223903656, 0.9702380895614624, 0.9583333134651184, 0.9523809552192688, 0.9821428656578064, 0.9821428656578064, 0.9642857313156128, 0.9523809552192688, 0.9464285969734192, 0.9226190447807312, 0.976190447807312, 0.9583333134651184, 0.9702380895614624, 0.9642857313156128, 0.9464285969734192, 0.9821428656578064, 0.9642857313156128, 0.9404761791229248, 0.9523809552192688, 0.9642857313156128, 0.9821428656578064, 0.976190447807312, 0.9523809552192688, 0.9702380895614624, 0.9523809552192688, 0.9583333134651184, 0.976190447807312, 0.9464285969734192, 0.9583333134651184, 0.9523809552192688, 0.9821428656578064, 0.9821428656578064, 0.988095223903656, 0.9523809552192688, 0.9642857313156128, 0.9702380895614624, 0.9523809552192688, 0.988095223903656, 0.9821428656578064, 0.988095223903656, 0.976190447807312, 0.9583333134651184, 0.9523809552192688, 0.9821428656578064, 0.988095223903656, 0.9702380895614624, 0.9702380895614624, 0.9642857313156128, 0.976190447807312, 0.9702380895614624, 0.9583333134651184, 0.9523809552192688, 0.9821428656578064, 0.9821428656578064, 0.9821428656578064, 0.9702380895614624, 0.9702380895614624, 0.9583333134651184, 0.9642857313156128, 0.976190447807312, 0.976190447807312, 0.976190447807312, 0.9642857313156128, 1.0, 0.9642857313156128, 0.9583333134651184, 0.976190447807312, 0.9583333134651184, 0.9523809552192688, 0.9523809552192688, 0.9583333134651184, 0.9940476417541504, 0.9702380895614624, 0.9642857313156128, 0.9642857313156128, 0.9642857313156128, 0.9583333134651184, 0.976190447807312, 0.976190447807312, 0.9583333134651184, 0.9523809552192688, 0.976190447807312, 0.9523809552192688, 0.9642857313156128, 0.9642857313156128, 0.9404761791229248, 0.9404761791229248, 0.9404761791229248, 0.9583333134651184, 0.988095223903656, 0.9404761791229248, 0.9583333134651184, 0.9583333134651184, 0.9523809552192688, 0.9583333134651184, 0.976190447807312, 0.9404761791229248, 0.9821428656578064, 0.9821428656578064, 0.9702380895614624, 0.9821428656578064, 0.9583333134651184, 0.976190447807312, 0.9404761791229248, 0.9821428656578064, 0.9523809552192688, 0.976190447807312, 0.9702380895614624, 0.9642857313156128, 0.976190447807312, 0.9523809552192688, 0.9702380895614624, 0.976190447807312, 0.9821428656578064, 0.9583333134651184, 0.9702380895614624, 0.9464285969734192, 0.9583333134651184, 0.988095223903656, 0.9702380895614624, 0.9583333134651184, 0.976190447807312, 0.988095223903656, 0.9940476417541504, 0.9523809552192688, 0.9702380895614624, 0.976190447807312, 0.9583333134651184, 0.988095223903656, 0.976190447807312, 0.9821428656578064, 0.9702380895614624, 0.9821428656578064, 0.9583333134651184, 0.9642857313156128, 0.9642857313156128, 0.9702380895614624, 0.9464285969734192, 0.9821428656578064, 0.9583333134651184, 0.9523809552192688, 0.9821428656578064, 0.9821428656578064, 0.976190447807312, 0.988095223903656, 0.9523809552192688, 0.988095223903656, 0.988095223903656, 0.9345238208770752, 0.9821428656578064, 0.988095223903656, 0.9702380895614624, 0.9821428656578064, 0.976190447807312, 0.9702380895614624, 0.9821428656578064, 0.9821428656578064, 0.9821428656578064, 0.976190447807312, 0.9940476417541504, 0.976190447807312, 0.988095223903656, 0.976190447807312, 0.9940476417541504, 0.976190447807312, 0.9702380895614624, 0.976190447807312, 0.9583333134651184, 0.9702380895614624, 0.988095223903656, 0.9523809552192688, 0.9642857313156128, 0.9702380895614624, 0.9702380895614624, 0.9583333134651184, 0.9702380895614624, 0.9821428656578064, 0.9940476417541504, 0.9821428656578064, 0.988095223903656, 0.9821428656578064, 0.988095223903656, 0.976190447807312, 0.9702380895614624, 0.9702380895614624, 0.976190447807312], 'val_loss': [3.497169017791748, 4.751813888549805, 6.697055339813232, 7.906796455383301, 8.553400039672852, 8.18100357055664, 7.953833103179932, 8.10912799835205, 7.98585319519043, 7.843414306640625, 7.450984001159668, 7.0008978843688965, 6.63129186630249, 6.215754508972168, 6.243070602416992, 6.659975051879883, 6.865379333496094, 6.867955207824707, 6.533573627471924, 6.083880424499512, 5.6343512535095215, 5.096145153045654, 4.604433059692383, 4.026978015899658, 4.010943412780762, 3.828925848007202, 3.583336114883423, 3.742950439453125, 3.9204261302948, 4.454336166381836, 4.809475898742676, 4.825486183166504, 4.814445495605469, 4.519871234893799, 4.094131946563721, 3.9722135066986084, 3.8250200748443604, 3.8151776790618896, 3.814267635345459, 3.4831695556640625, 3.225105047225952, 3.1296370029449463, 3.0697648525238037, 3.1331393718719482, 3.15521502494812, 3.3593051433563232, 3.6244914531707764, 3.1806256771087646, 2.661280393600464, 2.2639474868774414, 1.9437330961227417, 1.5984405279159546, 1.6801364421844482, 1.9435957670211792, 1.9706737995147705, 2.0093953609466553, 2.0350918769836426, 1.989222526550293, 1.9569865465164185, 1.6252119541168213, 1.3833887577056885, 1.125518798828125, 0.9383730292320251, 0.775655210018158, 0.720529317855835, 0.7688813209533691, 0.8877295851707458, 0.8806822896003723, 0.9770921468734741, 1.039704442024231, 1.0283204317092896, 1.073057770729065, 1.0636531114578247, 1.0698481798171997, 1.0765084028244019, 1.0083715915679932, 0.9576654434204102, 0.9522286057472229, 1.0140036344528198, 1.0559412240982056, 1.0316472053527832, 0.9780912399291992, 0.9262323975563049, 0.8627098798751831, 0.8327794671058655, 0.8213645219802856, 0.8005287051200867, 0.8158140182495117, 0.8697170615196228, 0.9601680040359497, 1.0818920135498047, 1.2215447425842285, 1.2794252634048462, 1.2466039657592773, 1.222477674484253, 1.3833826780319214, 1.4762444496154785, 1.4381107091903687, 1.1876633167266846, 0.9736455082893372, 0.8965953588485718, 0.8997477293014526, 1.0022995471954346, 1.10873544216156, 1.217369556427002, 1.2983171939849854, 1.2395108938217163, 1.2409707307815552, 1.241943359375, 1.1744171380996704, 1.1339401006698608, 1.1281787157058716, 1.1140047311782837, 1.0023714303970337, 1.0058804750442505, 1.0704216957092285, 1.0523486137390137, 0.9135949015617371, 0.8316423296928406, 0.8341054320335388, 0.9013264179229736, 0.994749903678894, 1.0897144079208374, 1.1382583379745483, 1.1226638555526733, 1.1897163391113281, 1.2133910655975342, 1.1450915336608887, 1.1560473442077637, 1.1071152687072754, 1.0640873908996582, 1.0345306396484375, 0.9955771565437317, 0.9595299363136292, 0.967591404914856, 0.9555432796478271, 0.8986648321151733, 0.8621735572814941, 0.8441930413246155, 0.8450279235839844, 0.8765869736671448, 0.8775573968887329, 0.9148826003074646, 1.0050272941589355, 1.1476176977157593, 1.3950566053390503, 1.561767578125, 1.49152410030365, 1.3854093551635742, 1.177274227142334, 1.0666173696517944, 1.0289483070373535, 1.0045087337493896, 0.9416553974151611, 0.8906397223472595, 0.8959149122238159, 0.7637788653373718, 0.6974543929100037, 0.6794264316558838, 0.7285071611404419, 0.7935202717781067, 0.8521228432655334, 0.9056558609008789, 0.8619533181190491, 0.8415083289146423, 0.8479847311973572, 0.8792362809181213, 0.8751612305641174, 0.8438869118690491, 0.8011289834976196, 0.7207461595535278, 0.6912460923194885, 0.6963736414909363, 0.708103597164154, 0.6791427731513977, 0.6122744083404541, 0.538436233997345, 0.5087394118309021, 0.5426482558250427, 0.608801543712616, 0.6738711595535278, 0.7415570020675659, 0.8660151958465576, 0.9719902276992798, 1.037100076675415, 1.0067899227142334, 0.9315807819366455, 0.8676940202713013, 0.945284366607666, 1.0756070613861084, 1.2426801919937134, 1.3071495294570923, 1.3214136362075806, 1.3548885583877563, 1.3938077688217163, 1.3608207702636719, 1.2154815196990967, 1.074086308479309, 0.9400714635848999, 0.8345898389816284, 0.8116937279701233, 0.8481636643409729, 0.871274471282959, 0.8577846884727478, 0.8098375201225281, 0.735963761806488, 0.6548557877540588, 0.5637514591217041, 0.4739590883255005, 0.44916272163391113, 0.4659213125705719, 0.5322355628013611, 0.588641881942749, 0.648705780506134, 0.7393666505813599, 0.8234770894050598, 0.9457616806030273, 1.0805779695510864, 1.1876373291015625, 1.2905670404434204, 1.261718511581421, 1.1540404558181763, 1.108747124671936, 1.0286508798599243, 0.9493470788002014, 0.9038023352622986, 0.8509559631347656, 0.7667692303657532, 0.7572569251060486, 0.7891678810119629, 0.8980579972267151, 1.091219186782837, 1.269114375114441, 1.3769904375076294, 1.407771348953247, 1.3729826211929321, 1.20352041721344, 1.0671998262405396, 0.9353298544883728, 0.8682376146316528, 0.9096247553825378, 1.0116689205169678, 1.1183913946151733, 1.1454495191574097, 1.0691877603530884, 1.007880449295044, 0.965956449508667, 0.9671034216880798, 1.0094516277313232, 1.1079610586166382, 1.168434739112854, 1.2100821733474731, 1.266011357307434, 1.343183994293213, 1.3885868787765503, 1.4062212705612183, 1.301068663597107, 1.117570400238037, 0.9440432786941528, 0.863944947719574, 0.8761416077613831, 0.90016108751297, 0.9134448766708374, 0.9467602372169495, 0.9561244249343872, 1.038943886756897, 1.0249742269515991, 1.061890959739685, 18.116050720214844, 4.961177349090576, 3.225058078765869, 2.483335256576538, 1.9315129518508911, 1.696530818939209, 1.4390794038772583, 1.3559342622756958, 1.3774514198303223, 1.3434603214263916, 1.196657419204712, 1.1139289140701294, 1.0304335355758667, 0.9916867017745972, 0.9215365648269653, 0.9115556478500366, 0.9710394740104675, 1.069045901298523, 1.0950626134872437, 1.0998985767364502, 1.0571268796920776, 1.0267127752304077, 1.0155584812164307, 0.9928018450737, 0.9965304732322693, 1.0318636894226074, 1.0290560722351074, 1.0379446744918823, 1.0892835855484009, 1.1815019845962524, 1.2786917686462402, 1.3283425569534302, 1.2669352293014526, 1.220032811164856, 1.2073147296905518, 1.1848517656326294, 1.165808081626892, 1.1284973621368408, 1.0688133239746094, 1.0085028409957886, 1.0660152435302734, 1.1664702892303467, 1.2595635652542114, 1.331827163696289, 1.444858431816101, 1.4786970615386963, 1.43344247341156, 1.2974356412887573, 1.099475622177124, 1.0371348857879639, 1.0653116703033447, 1.0862823724746704, 1.0963078737258911, 1.1857552528381348, 1.1176433563232422, 0.9407787919044495, 0.768671989440918, 0.6929318308830261, 0.6397213339805603, 0.6400108933448792, 0.6384692788124084, 0.6412955522537231, 0.6505481004714966, 0.6994110345840454, 0.787651002407074, 0.852724015712738, 0.9210081100463867, 0.9844442009925842, 1.048182725906372, 1.1707831621170044, 1.2285205125808716, 1.199440360069275, 1.1111938953399658, 1.004324197769165, 0.9865798950195312, 0.9863384962081909, 0.9974879026412964, 1.0394788980484009, 1.1249645948410034, 1.196736454963684, 1.2030481100082397, 1.200066328048706, 1.1904064416885376, 1.1925772428512573, 1.2219558954238892, 1.266873836517334, 1.3042880296707153, 1.3164557218551636, 1.3100404739379883, 1.386222004890442, 1.5227588415145874, 1.44077467918396, 1.382282018661499, 1.2468823194503784, 1.066601276397705, 0.9493679404258728, 0.843496561050415, 0.7029080986976624, 0.623956024646759, 0.5888781547546387, 0.60610431432724, 0.6579374670982361, 0.7306886315345764, 0.8530351519584656, 0.8892844319343567, 0.9415712356567383, 1.0395796298980713, 1.1294810771942139, 1.19512140750885, 1.2310574054718018, 1.2129971981048584, 1.1278855800628662, 1.1449787616729736, 1.131328821182251, 1.0984975099563599, 1.0516326427459717, 1.0106208324432373, 1.0198841094970703, 1.0132418870925903, 1.0142933130264282, 1.0886040925979614, 1.2150293588638306, 1.4039266109466553, 1.512138843536377, 1.5845357179641724, 1.567647933959961, 1.5150519609451294, 1.5339679718017578, 1.5081920623779297, 1.4396779537200928, 1.4402962923049927, 1.4118454456329346, 1.2644473314285278, 1.051481008529663, 0.9490671157836914, 0.879661500453949, 0.9608393311500549, 1.0437735319137573, 1.1013009548187256, 1.0957022905349731, 1.0971266031265259, 1.1899927854537964, 1.2915223836898804, 1.34027898311615, 1.3552688360214233, 1.2509448528289795, 1.1097142696380615, 1.0546401739120483, 1.0488510131835938, 1.105952262878418, 1.1196870803833008, 1.1133801937103271, 13.669007301330566, 5.868127346038818, 4.859933376312256, 3.9208590984344482, 3.9461450576782227, 3.9060420989990234, 3.307511329650879, 2.8196489810943604, 2.5223169326782227, 2.4046077728271484, 2.352341651916504, 2.0407652854919434, 1.7516589164733887, 1.509515404701233, 1.4918968677520752, 1.5300291776657104, 1.5950233936309814, 1.7127596139907837, 1.6871258020401, 1.5096098184585571, 1.382381558418274, 1.3932971954345703, 1.3796794414520264, 1.4716331958770752, 1.5785993337631226, 1.5855587720870972, 1.5089949369430542, 1.3194435834884644, 1.264579176902771, 1.2933058738708496, 1.2810592651367188, 1.2130606174468994, 1.2035328149795532, 1.1744706630706787, 1.084307312965393, 0.9655677080154419, 0.8357290625572205, 0.7987072467803955, 0.7979004979133606, 0.8316943645477295, 0.8924906253814697, 0.9460394978523254, 0.9961580634117126, 1.079397439956665, 1.148047924041748, 1.224826693534851, 1.2718985080718994, 1.3294967412948608, 1.345465064048767, 1.3690075874328613, 1.3723256587982178, 1.3222376108169556, 1.2002520561218262, 1.1530401706695557, 1.1447995901107788, 1.15000319480896, 1.1235218048095703, 1.083912968635559, 1.0552271604537964, 1.094072937965393, 1.257506251335144, 1.513205647468567, 1.7566587924957275, 1.9211698770523071, 2.0155696868896484, 2.0960612297058105, 2.1394846439361572, 2.1005332469940186, 2.032989978790283, 1.9847187995910645, 1.903127908706665, 1.7860063314437866, 1.6474902629852295, 1.5869293212890625, 1.5401045083999634, 1.4452431201934814, 1.3419550657272339, 1.2596721649169922, 1.1914513111114502, 1.145850658416748], 'val_accuracy': [0.20000000298023224, 0.13333334028720856, 0.13333334028720856, 0.13333334028720856, 0.13333334028720856, 0.13333334028720856, 0.13333334028720856, 0.13333334028720856, 0.13333334028720856, 0.13333334028720856, 0.13333334028720856, 0.13333334028720856, 0.13333334028720856, 0.13333334028720856, 0.13333334028720856, 0.13333334028720856, 0.13333334028720856, 0.13333334028720856, 0.13333334028720856, 0.13333334028720856, 0.13333334028720856, 0.17777778208255768, 0.20000000298023224, 0.20000000298023224, 0.20000000298023224, 0.20000000298023224, 0.20000000298023224, 0.20000000298023224, 0.20000000298023224, 0.17777778208255768, 0.20000000298023224, 0.17777778208255768, 0.17777778208255768, 0.20000000298023224, 0.2222222238779068, 0.24444444477558136, 0.24444444477558136, 0.24444444477558136, 0.2666666805744171, 0.2666666805744171, 0.2888889014720917, 0.31111112236976624, 0.31111112236976624, 0.3333333432674408, 0.3333333432674408, 0.2888889014720917, 0.2888889014720917, 0.31111112236976624, 0.35555556416511536, 0.42222222685813904, 0.46666666865348816, 0.5555555820465088, 0.5555555820465088, 0.42222222685813904, 0.4444444477558136, 0.4444444477558136, 0.4444444477558136, 0.46666666865348816, 0.4888888895511627, 0.5111111402511597, 0.5555555820465088, 0.6000000238418579, 0.6888889074325562, 0.7111111283302307, 0.7777777910232544, 0.7333333492279053, 0.7333333492279053, 0.7111111283302307, 0.7333333492279053, 0.7333333492279053, 0.7555555701255798, 0.7333333492279053, 0.7333333492279053, 0.6888889074325562, 0.7111111283302307, 0.7333333492279053, 0.7777777910232544, 0.7777777910232544, 0.800000011920929, 0.7555555701255798, 0.7111111283302307, 0.7333333492279053, 0.7777777910232544, 0.8222222328186035, 0.8444444537162781, 0.8222222328186035, 0.8222222328186035, 0.7777777910232544, 0.7777777910232544, 0.7555555701255798, 0.7333333492279053, 0.7111111283302307, 0.7111111283302307, 0.7111111283302307, 0.7333333492279053, 0.6888889074325562, 0.7111111283302307, 0.7333333492279053, 0.7555555701255798, 0.7777777910232544, 0.8222222328186035, 0.800000011920929, 0.8444444537162781, 0.8444444537162781, 0.8222222328186035, 0.800000011920929, 0.800000011920929, 0.8222222328186035, 0.800000011920929, 0.7777777910232544, 0.800000011920929, 0.800000011920929, 0.800000011920929, 0.8444444537162781, 0.800000011920929, 0.800000011920929, 0.7777777910232544, 0.800000011920929, 0.8444444537162781, 0.8444444537162781, 0.8444444537162781, 0.8444444537162781, 0.800000011920929, 0.8444444537162781, 0.800000011920929, 0.800000011920929, 0.8222222328186035, 0.8222222328186035, 0.800000011920929, 0.8444444537162781, 0.8222222328186035, 0.8222222328186035, 0.8222222328186035, 0.8222222328186035, 0.8222222328186035, 0.8444444537162781, 0.8444444537162781, 0.8444444537162781, 0.8444444537162781, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8222222328186035, 0.8222222328186035, 0.800000011920929, 0.7777777910232544, 0.7555555701255798, 0.7555555701255798, 0.7333333492279053, 0.8444444537162781, 0.8444444537162781, 0.8444444537162781, 0.8444444537162781, 0.8222222328186035, 0.8222222328186035, 0.800000011920929, 0.8444444537162781, 0.8444444537162781, 0.8444444537162781, 0.800000011920929, 0.800000011920929, 0.800000011920929, 0.800000011920929, 0.8222222328186035, 0.800000011920929, 0.7777777910232544, 0.800000011920929, 0.7777777910232544, 0.7777777910232544, 0.8444444537162781, 0.9111111164093018, 0.9111111164093018, 0.8888888955116272, 0.8888888955116272, 0.9111111164093018, 0.9111111164093018, 0.9333333373069763, 0.9333333373069763, 0.8888888955116272, 0.8666666746139526, 0.8666666746139526, 0.8444444537162781, 0.8444444537162781, 0.8444444537162781, 0.8222222328186035, 0.8222222328186035, 0.8444444537162781, 0.8444444537162781, 0.8444444537162781, 0.8444444537162781, 0.800000011920929, 0.800000011920929, 0.8222222328186035, 0.7777777910232544, 0.7777777910232544, 0.8222222328186035, 0.8444444537162781, 0.8666666746139526, 0.8888888955116272, 0.9111111164093018, 0.9111111164093018, 0.8888888955116272, 0.8666666746139526, 0.8666666746139526, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.9111111164093018, 0.9111111164093018, 0.8888888955116272, 0.9111111164093018, 0.9111111164093018, 0.9111111164093018, 0.9111111164093018, 0.8888888955116272, 0.8888888955116272, 0.8666666746139526, 0.8666666746139526, 0.8444444537162781, 0.8666666746139526, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8666666746139526, 0.8666666746139526, 0.8444444537162781, 0.8666666746139526, 0.8888888955116272, 0.8444444537162781, 0.8444444537162781, 0.8444444537162781, 0.8444444537162781, 0.8444444537162781, 0.8444444537162781, 0.8222222328186035, 0.8222222328186035, 0.8444444537162781, 0.8666666746139526, 0.8888888955116272, 0.8888888955116272, 0.8444444537162781, 0.8444444537162781, 0.8666666746139526, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8444444537162781, 0.8222222328186035, 0.7777777910232544, 0.7555555701255798, 0.7777777910232544, 0.800000011920929, 0.800000011920929, 0.8222222328186035, 0.8444444537162781, 0.8222222328186035, 0.8222222328186035, 0.8222222328186035, 0.8444444537162781, 0.8222222328186035, 0.8222222328186035, 0.8222222328186035, 0.8222222328186035, 0.8222222328186035, 0.8222222328186035, 0.31111112236976624, 0.46666666865348816, 0.6000000238418579, 0.7111111283302307, 0.6888889074325562, 0.7111111283302307, 0.7555555701255798, 0.7777777910232544, 0.800000011920929, 0.8222222328186035, 0.800000011920929, 0.800000011920929, 0.8444444537162781, 0.8444444537162781, 0.8444444537162781, 0.8444444537162781, 0.8222222328186035, 0.8222222328186035, 0.800000011920929, 0.8222222328186035, 0.8444444537162781, 0.8444444537162781, 0.8444444537162781, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8444444537162781, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8444444537162781, 0.8222222328186035, 0.800000011920929, 0.800000011920929, 0.800000011920929, 0.800000011920929, 0.8444444537162781, 0.8888888955116272, 0.9111111164093018, 0.8888888955116272, 0.8888888955116272, 0.8666666746139526, 0.8888888955116272, 0.9111111164093018, 0.9111111164093018, 0.9111111164093018, 0.9111111164093018, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.9111111164093018, 0.9111111164093018, 0.9111111164093018, 0.9111111164093018, 0.9111111164093018, 0.9111111164093018, 0.9111111164093018, 0.9111111164093018, 0.8666666746139526, 0.8444444537162781, 0.9111111164093018, 0.9111111164093018, 0.9111111164093018, 0.9111111164093018, 0.8888888955116272, 0.8888888955116272, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8444444537162781, 0.8222222328186035, 0.7777777910232544, 0.800000011920929, 0.8222222328186035, 0.8222222328186035, 0.8666666746139526, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8888888955116272, 0.8666666746139526, 0.8444444537162781, 0.8444444537162781, 0.8444444537162781, 0.8444444537162781, 0.8444444537162781, 0.8666666746139526, 0.8666666746139526, 0.8444444537162781, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8888888955116272, 0.8444444537162781, 0.8222222328186035, 0.8222222328186035, 0.800000011920929, 0.800000011920929, 0.7777777910232544, 0.7555555701255798, 0.7555555701255798, 0.7777777910232544, 0.7777777910232544, 0.7777777910232544, 0.8222222328186035, 0.8222222328186035, 0.8444444537162781, 0.8666666746139526, 0.8888888955116272, 0.8666666746139526, 0.8444444537162781, 0.8444444537162781, 0.8444444537162781, 0.800000011920929, 0.8222222328186035, 0.8222222328186035, 0.8444444537162781, 0.8444444537162781, 0.8444444537162781, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.4000000059604645, 0.5777778029441833, 0.5777778029441833, 0.5333333611488342, 0.5555555820465088, 0.5555555820465088, 0.644444465637207, 0.7333333492279053, 0.7333333492279053, 0.7111111283302307, 0.7111111283302307, 0.7777777910232544, 0.800000011920929, 0.800000011920929, 0.800000011920929, 0.800000011920929, 0.8222222328186035, 0.7777777910232544, 0.7777777910232544, 0.7777777910232544, 0.7777777910232544, 0.7777777910232544, 0.7777777910232544, 0.7555555701255798, 0.7333333492279053, 0.7333333492279053, 0.7333333492279053, 0.7333333492279053, 0.7555555701255798, 0.800000011920929, 0.7777777910232544, 0.800000011920929, 0.8222222328186035, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8666666746139526, 0.8444444537162781, 0.800000011920929, 0.800000011920929, 0.800000011920929, 0.800000011920929, 0.800000011920929, 0.800000011920929, 0.8666666746139526, 0.8666666746139526, 0.8888888955116272, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.800000011920929, 0.800000011920929, 0.800000011920929, 0.800000011920929, 0.800000011920929, 0.800000011920929, 0.800000011920929, 0.7777777910232544, 0.800000011920929, 0.800000011920929, 0.8222222328186035, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8444444537162781, 0.8444444537162781, 0.8444444537162781]}\n",
    "\n",
    "#exp4\n",
    "\n",
    "import tensorflow\n",
    "\n",
    "from keras.layers import Dense,Flatten\n",
    "\n",
    "from keras.applications.vgg16 import VGG16\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "from keras import Sequential\n",
    "\n",
    "from keras.layers import Dense,Conv2D,MaxPooling2D,Flatten,BatchNormalization,Dropout\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam,SGD,RMSprop\n",
    "\n",
    "from tensorflow.keras.layers import Dense,Input,Dropout,GlobalAveragePooling2D,Flatten,Conv2D,BatchNormalization,Activation,MaxPooling2D\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Flatten, BatchNormalization, Dropout\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from tensorflow.keras.applications.vgg19 import VGG19\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Importing Deep Learning Libraries\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "from keras import regularizers\n",
    "\n",
    "from keras.preprocessing import image\n",
    "\n",
    "​\n",
    "\n",
    "#HISTOGRAM CODE\n",
    "\n",
    "#histogram code\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "\n",
    "​\n",
    "\n",
    "emotions = [\"happy\", \"sadness\", \"anger\", \"disgust\", \"neutral\", \"fear\", \"surprise\"]\n",
    "\n",
    "​\n",
    "\n",
    "folder_path = \"Jaffetrainvalidation/train\"\n",
    "\n",
    "# Counting the number of images per emotion\n",
    "\n",
    "counts = [len(os.listdir(os.path.join(folder_path, emotion))) for emotion in emotions]\n",
    "\n",
    "​\n",
    "\n",
    "# Plotting the bar chart\n",
    "\n",
    "colors = ['red', 'yellow', 'black', 'blue', 'orange', 'green', 'pink']\n",
    "\n",
    "plt.bar(emotions, height=counts, color=colors)\n",
    "\n",
    "plt.ylabel('Number')\n",
    "\n",
    "plt.xlabel('Emotions')\n",
    "\n",
    "​\n",
    "\n",
    "​\n",
    "\n",
    "#plt.savefig('hostgoarm.png')\n",
    "\n",
    "#plt.show()\n",
    "\n",
    "​\n",
    "\n",
    "​\n",
    "\n",
    "# Data generators\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "​\n",
    "\n",
    "# Data augmentation for training set\n",
    "\n",
    "datagen_train = ImageDataGenerator(\n",
    "\n",
    "    rescale=1./255,\n",
    "\n",
    "    rotation_range=15,\n",
    "\n",
    "    width_shift_range=0.1,\n",
    "\n",
    "    height_shift_range=0.1,\n",
    "\n",
    "    shear_range=0.2,\n",
    "\n",
    "    zoom_range=0.2,\n",
    "\n",
    "    horizontal_flip=True,\n",
    "\n",
    "    fill_mode='nearest'\n",
    "\n",
    ")\n",
    "\n",
    "​\n",
    "\n",
    "# Normalization for validation set (no augmentation)\n",
    "\n",
    "datagen_val = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "​\n",
    "\n",
    "​\n",
    "\n",
    "​\n",
    "\n",
    "train_ds = datagen_train.flow_from_directory(\"Jaffetrainvalidation/train\",\n",
    "\n",
    "                                             target_size=(256, 256),\n",
    "\n",
    "                                             color_mode=\"rgb\",\n",
    "\n",
    "                                             batch_size=batch_size,\n",
    "\n",
    "                                             class_mode='categorical',\n",
    "\n",
    "                                             shuffle=True)\n",
    "\n",
    "​\n",
    "\n",
    "test_ds = datagen_val.flow_from_directory(\"Jaffetrainvalidation/validation\",\n",
    "\n",
    "                                         target_size=(256, 256),\n",
    "\n",
    "                                         color_mode=\"rgb\",\n",
    "\n",
    "                                         batch_size=batch_size,\n",
    "\n",
    "                                         class_mode='categorical',\n",
    "\n",
    "                                         shuffle=False)\n",
    "\n",
    "print('Train and Validation sets have been created.')\n",
    "\n",
    "​\n",
    "\n",
    "​\n",
    "\n",
    "#model vgg19\n",
    "\n",
    "​\n",
    "\n",
    "from keras.applications.vgg19 import VGG19\n",
    "\n",
    "conv_base = VGG19(weights='imagenet', include_top=False, input_shape=(256, 256, 3))\n",
    "\n",
    "conv_base.summary()\n",
    "\n",
    "​\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(conv_base)\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(256,activation = 'relu'))\n",
    "\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dropout(0.30))\n",
    "\n",
    "# Second fully connected layer  \n",
    "\n",
    "model.add(Dense(512,activation = 'relu'))\n",
    "\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dropout(0.30))\n",
    "\n",
    "​\n",
    "\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "​\n",
    "\n",
    "# model.summary()\n",
    "\n",
    "# Visualize the model.\n",
    "\n",
    "#plot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True)\n",
    "\n",
    "​\n",
    "\n",
    "conv_base.trainable = False\n",
    "\n",
    "​\n",
    "\n",
    "model.compile(optimizer = Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "​\n",
    "\n",
    "print('CNN model has been created you can proceed to train you data with this model.')\n",
    "\n",
    "​\n",
    "\n",
    "​\n",
    "\n",
    "​\n",
    "\n",
    "​\n",
    "\n",
    "# Training the model\n",
    "\n",
    "epochs = 500\n",
    "\n",
    "​\n",
    "\n",
    "history = model.fit(x=train_ds,\n",
    "\n",
    "                    epochs=epochs,\n",
    "\n",
    "                    validation_data=test_ds)\n",
    "\n",
    "​\n",
    "\n",
    "# Print training history\n",
    "\n",
    "print(history.history)\n",
    "\n",
    "​\n",
    "\n",
    "# Plotting training history\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "​\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "\n",
    "plt.title('Loss')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "​\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "\n",
    "plt.title('Accuracy')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "Found 168 images belonging to 7 classes.\n",
    "Found 45 images belonging to 7 classes.\n",
    "Train and Validation sets have been created.\n",
    "Model: \"vgg19\"\n",
    "_________________________________________________________________\n",
    " Layer (type)                Output Shape              Param #   \n",
    "=================================================================\n",
    " input_5 (InputLayer)        [(None, 256, 256, 3)]     0         \n",
    "                                                                 \n",
    " block1_conv1 (Conv2D)       (None, 256, 256, 64)      1792      \n",
    "                                                                 \n",
    " block1_conv2 (Conv2D)       (None, 256, 256, 64)      36928     \n",
    "                                                                 \n",
    " block1_pool (MaxPooling2D)  (None, 128, 128, 64)      0         \n",
    "                                                                 \n",
    " block2_conv1 (Conv2D)       (None, 128, 128, 128)     73856     \n",
    "                                                                 \n",
    " block2_conv2 (Conv2D)       (None, 128, 128, 128)     147584    \n",
    "                                                                 \n",
    " block2_pool (MaxPooling2D)  (None, 64, 64, 128)       0         \n",
    "                                                                 \n",
    " block3_conv1 (Conv2D)       (None, 64, 64, 256)       295168    \n",
    "                                                                 \n",
    " block3_conv2 (Conv2D)       (None, 64, 64, 256)       590080    \n",
    "                                                                 \n",
    " block3_conv3 (Conv2D)       (None, 64, 64, 256)       590080    \n",
    "                                                                 \n",
    " block3_conv4 (Conv2D)       (None, 64, 64, 256)       590080    \n",
    "                                                                 \n",
    " block3_pool (MaxPooling2D)  (None, 32, 32, 256)       0         \n",
    "                                                                 \n",
    " block4_conv1 (Conv2D)       (None, 32, 32, 512)       1180160   \n",
    "                                                                 \n",
    " block4_conv2 (Conv2D)       (None, 32, 32, 512)       2359808   \n",
    "                                                                 \n",
    " block4_conv3 (Conv2D)       (None, 32, 32, 512)       2359808   \n",
    "                                                                 \n",
    " block4_conv4 (Conv2D)       (None, 32, 32, 512)       2359808   \n",
    "                                                                 \n",
    " block4_pool (MaxPooling2D)  (None, 16, 16, 512)       0         \n",
    "                                                                 \n",
    " block5_conv1 (Conv2D)       (None, 16, 16, 512)       2359808   \n",
    "                                                                 \n",
    " block5_conv2 (Conv2D)       (None, 16, 16, 512)       2359808   \n",
    "                                                                 \n",
    " block5_conv3 (Conv2D)       (None, 16, 16, 512)       2359808   \n",
    "                                                                 \n",
    " block5_conv4 (Conv2D)       (None, 16, 16, 512)       2359808   \n",
    "                                                                 \n",
    " block5_pool (MaxPooling2D)  (None, 8, 8, 512)         0         \n",
    "                                                                 \n",
    "=================================================================\n",
    "Total params: 20024384 (76.39 MB)\n",
    "Trainable params: 20024384 (76.39 MB)\n",
    "Non-trainable params: 0 (0.00 Byte)\n",
    "_________________________________________________________________\n",
    "CNN model has been created you can proceed to train you data with this model.\n",
    "Epoch 1/500\n",
    "3/3 [==============================] - 3s 520ms/step - loss: 2.9026 - accuracy: 0.2202 - val_loss: 2.7849 - val_accuracy: 0.1333\n",
    "Epoch 2/500\n",
    "3/3 [==============================] - 2s 453ms/step - loss: 2.3800 - accuracy: 0.2798 - val_loss: 3.9407 - val_accuracy: 0.1333\n",
    "Epoch 3/500\n",
    "3/3 [==============================] - 2s 600ms/step - loss: 2.1080 - accuracy: 0.3393 - val_loss: 5.5374 - val_accuracy: 0.2000\n",
    "Epoch 4/500\n",
    "3/3 [==============================] - 2s 605ms/step - loss: 2.0495 - accuracy: 0.3512 - val_loss: 6.9484 - val_accuracy: 0.1778\n",
    "Epoch 5/500\n",
    "3/3 [==============================] - 2s 448ms/step - loss: 1.9216 - accuracy: 0.3988 - val_loss: 7.5788 - val_accuracy: 0.1333\n",
    "Epoch 6/500\n",
    "3/3 [==============================] - 2s 448ms/step - loss: 1.7120 - accuracy: 0.4286 - val_loss: 7.0294 - val_accuracy: 0.2000\n",
    "Epoch 7/500\n",
    "3/3 [==============================] - 2s 464ms/step - loss: 1.7256 - accuracy: 0.4345 - val_loss: 6.6517 - val_accuracy: 0.2444\n",
    "Epoch 8/500\n",
    "3/3 [==============================] - 2s 606ms/step - loss: 1.4973 - accuracy: 0.4345 - val_loss: 7.0972 - val_accuracy: 0.2444\n",
    "Epoch 9/500\n",
    "3/3 [==============================] - 2s 610ms/step - loss: 1.5498 - accuracy: 0.4524 - val_loss: 7.4514 - val_accuracy: 0.2667\n",
    "Epoch 10/500\n",
    "3/3 [==============================] - 2s 471ms/step - loss: 1.4457 - accuracy: 0.4881 - val_loss: 8.1991 - val_accuracy: 0.1556\n",
    "Epoch 11/500\n",
    "3/3 [==============================] - 2s 453ms/step - loss: 1.4290 - accuracy: 0.4881 - val_loss: 8.5894 - val_accuracy: 0.1556\n",
    "Epoch 12/500\n",
    "3/3 [==============================] - 2s 603ms/step - loss: 1.4132 - accuracy: 0.4940 - val_loss: 7.8782 - val_accuracy: 0.2667\n",
    "Epoch 13/500\n",
    "3/3 [==============================] - 2s 470ms/step - loss: 1.3145 - accuracy: 0.4821 - val_loss: 7.3955 - val_accuracy: 0.2222\n",
    "Epoch 14/500\n",
    "3/3 [==============================] - 2s 609ms/step - loss: 1.1079 - accuracy: 0.5833 - val_loss: 7.8719 - val_accuracy: 0.1556\n",
    "Epoch 15/500\n",
    "3/3 [==============================] - 2s 609ms/step - loss: 1.1476 - accuracy: 0.5357 - val_loss: 7.9400 - val_accuracy: 0.1333\n",
    "Epoch 16/500\n",
    "3/3 [==============================] - 2s 451ms/step - loss: 1.0954 - accuracy: 0.6012 - val_loss: 7.2947 - val_accuracy: 0.1333\n",
    "Epoch 17/500\n",
    "3/3 [==============================] - 2s 618ms/step - loss: 0.9761 - accuracy: 0.6548 - val_loss: 6.5002 - val_accuracy: 0.1556\n",
    "Epoch 18/500\n",
    "3/3 [==============================] - 2s 449ms/step - loss: 0.8933 - accuracy: 0.6726 - val_loss: 6.2807 - val_accuracy: 0.2222\n",
    "Epoch 19/500\n",
    "3/3 [==============================] - 2s 445ms/step - loss: 0.7977 - accuracy: 0.6905 - val_loss: 6.4173 - val_accuracy: 0.2889\n",
    "Epoch 20/500\n",
    "3/3 [==============================] - 2s 469ms/step - loss: 0.9743 - accuracy: 0.6667 - val_loss: 7.0581 - val_accuracy: 0.2889\n",
    "Epoch 21/500\n",
    "3/3 [==============================] - 2s 470ms/step - loss: 0.8897 - accuracy: 0.6488 - val_loss: 7.0741 - val_accuracy: 0.3111\n",
    "Epoch 22/500\n",
    "3/3 [==============================] - 2s 469ms/step - loss: 0.8440 - accuracy: 0.6845 - val_loss: 6.5179 - val_accuracy: 0.3333\n",
    "Epoch 23/500\n",
    "3/3 [==============================] - 2s 453ms/step - loss: 0.8084 - accuracy: 0.6667 - val_loss: 5.8663 - val_accuracy: 0.3333\n",
    "Epoch 24/500\n",
    "3/3 [==============================] - 2s 466ms/step - loss: 0.8400 - accuracy: 0.6667 - val_loss: 5.4575 - val_accuracy: 0.2889\n",
    "Epoch 25/500\n",
    "3/3 [==============================] - 2s 473ms/step - loss: 0.7618 - accuracy: 0.7202 - val_loss: 5.3409 - val_accuracy: 0.2444\n",
    "Epoch 26/500\n",
    "3/3 [==============================] - 2s 453ms/step - loss: 0.8260 - accuracy: 0.7262 - val_loss: 5.6942 - val_accuracy: 0.1556\n",
    "Epoch 27/500\n",
    "3/3 [==============================] - 2s 452ms/step - loss: 0.7025 - accuracy: 0.7143 - val_loss: 5.8771 - val_accuracy: 0.1333\n",
    "Epoch 28/500\n",
    "3/3 [==============================] - 2s 596ms/step - loss: 0.7854 - accuracy: 0.6964 - val_loss: 6.0047 - val_accuracy: 0.1333\n",
    "Epoch 29/500\n",
    "3/3 [==============================] - 2s 474ms/step - loss: 0.7156 - accuracy: 0.7083 - val_loss: 5.9574 - val_accuracy: 0.1333\n",
    "Epoch 30/500\n",
    "3/3 [==============================] - 2s 453ms/step - loss: 0.8452 - accuracy: 0.6786 - val_loss: 5.9845 - val_accuracy: 0.1333\n",
    "Epoch 31/500\n",
    "3/3 [==============================] - 2s 448ms/step - loss: 0.7252 - accuracy: 0.7381 - val_loss: 5.6460 - val_accuracy: 0.1333\n",
    "Epoch 32/500\n",
    "3/3 [==============================] - 2s 458ms/step - loss: 0.6144 - accuracy: 0.7798 - val_loss: 4.9925 - val_accuracy: 0.1333\n",
    "Epoch 33/500\n",
    "3/3 [==============================] - 2s 480ms/step - loss: 0.4889 - accuracy: 0.8214 - val_loss: 4.8844 - val_accuracy: 0.1778\n",
    "\n",
    "Epoch 34/500\n",
    "3/3 [==============================] - 2s 450ms/step - loss: 0.4864 - accuracy: 0.8155 - val_loss: 4.8010 - val_accuracy: 0.2000\n",
    "Epoch 35/500\n",
    "3/3 [==============================] - 2s 591ms/step - loss: 0.5664 - accuracy: 0.7917 - val_loss: 4.5969 - val_accuracy: 0.1778\n",
    "Epoch 36/500\n",
    "3/3 [==============================] - 2s 472ms/step - loss: 0.6126 - accuracy: 0.8333 - val_loss: 4.2932 - val_accuracy: 0.2000\n",
    "Epoch 37/500\n",
    "3/3 [==============================] - 2s 587ms/step - loss: 0.4651 - accuracy: 0.8393 - val_loss: 4.0232 - val_accuracy: 0.2222\n",
    "Epoch 38/500\n",
    "3/3 [==============================] - 2s 449ms/step - loss: 0.4674 - accuracy: 0.8036 - val_loss: 3.7232 - val_accuracy: 0.2667\n",
    "Epoch 39/500\n",
    "3/3 [==============================] - 2s 453ms/step - loss: 0.5025 - accuracy: 0.8036 - val_loss: 3.4086 - val_accuracy: 0.3111\n",
    "Epoch 40/500\n",
    "3/3 [==============================] - 2s 587ms/step - loss: 0.5155 - accuracy: 0.7857 - val_loss: 2.9186 - val_accuracy: 0.3333\n",
    "Epoch 41/500\n",
    "3/3 [==============================] - 2s 450ms/step - loss: 0.4549 - accuracy: 0.8274 - val_loss: 2.3974 - val_accuracy: 0.4000\n",
    "Epoch 42/500\n",
    "3/3 [==============================] - 2s 585ms/step - loss: 0.4504 - accuracy: 0.8155 - val_loss: 2.1406 - val_accuracy: 0.5333\n",
    "Epoch 43/500\n",
    "3/3 [==============================] - 2s 448ms/step - loss: 0.4686 - accuracy: 0.8333 - val_loss: 2.0814 - val_accuracy: 0.5333\n",
    "Epoch 44/500\n",
    "3/3 [==============================] - 2s 596ms/step - loss: 0.4108 - accuracy: 0.8512 - val_loss: 2.2376 - val_accuracy: 0.5333\n",
    "Epoch 45/500\n",
    "3/3 [==============================] - 2s 461ms/step - loss: 0.3540 - accuracy: 0.8810 - val_loss: 2.2757 - val_accuracy: 0.4667\n",
    "Epoch 46/500\n",
    "3/3 [==============================] - 2s 589ms/step - loss: 0.3362 - accuracy: 0.8810 - val_loss: 2.5004 - val_accuracy: 0.4222\n",
    "Epoch 47/500\n",
    "3/3 [==============================] - 2s 458ms/step - loss: 0.5231 - accuracy: 0.8452 - val_loss: 2.3149 - val_accuracy: 0.4000\n",
    "Epoch 48/500\n",
    "3/3 [==============================] - 2s 588ms/step - loss: 0.3736 - accuracy: 0.8750 - val_loss: 2.3242 - val_accuracy: 0.4222\n",
    "Epoch 49/500\n",
    "3/3 [==============================] - 2s 454ms/step - loss: 0.4335 - accuracy: 0.8631 - val_loss: 2.2039 - val_accuracy: 0.4000\n",
    "Epoch 50/500\n",
    "3/3 [==============================] - 2s 474ms/step - loss: 0.3605 - accuracy: 0.8750 - val_loss: 2.0370 - val_accuracy: 0.4222\n",
    "Epoch 51/500\n",
    "3/3 [==============================] - 2s 586ms/step - loss: 0.2747 - accuracy: 0.8869 - val_loss: 2.0240 - val_accuracy: 0.5333\n",
    "Epoch 52/500\n",
    "3/3 [==============================] - 2s 450ms/step - loss: 0.2754 - accuracy: 0.9107 - val_loss: 2.0961 - val_accuracy: 0.5556\n",
    "Epoch 53/500\n",
    "3/3 [==============================] - 2s 598ms/step - loss: 0.3376 - accuracy: 0.8929 - val_loss: 2.2598 - val_accuracy: 0.5333\n",
    "Epoch 54/500\n",
    "3/3 [==============================] - 2s 591ms/step - loss: 0.3288 - accuracy: 0.8810 - val_loss: 2.0700 - val_accuracy: 0.5333\n",
    "Epoch 55/500\n",
    "3/3 [==============================] - 2s 469ms/step - loss: 0.4200 - accuracy: 0.8274 - val_loss: 2.5632 - val_accuracy: 0.5556\n",
    "Epoch 56/500\n",
    "3/3 [==============================] - 2s 597ms/step - loss: 0.3621 - accuracy: 0.8869 - val_loss: 3.2638 - val_accuracy: 0.4222\n",
    "Epoch 57/500\n",
    "3/3 [==============================] - 2s 449ms/step - loss: 0.3383 - accuracy: 0.8690 - val_loss: 3.5708 - val_accuracy: 0.4222\n",
    "Epoch 58/500\n",
    "3/3 [==============================] - 2s 451ms/step - loss: 0.3057 - accuracy: 0.8690 - val_loss: 3.6118 - val_accuracy: 0.3333\n",
    "Epoch 59/500\n",
    "3/3 [==============================] - 2s 452ms/step - loss: 0.3792 - accuracy: 0.8690 - val_loss: 3.1432 - val_accuracy: 0.3556\n",
    "Epoch 60/500\n",
    "3/3 [==============================] - 2s 446ms/step - loss: 0.4046 - accuracy: 0.8571 - val_loss: 2.5510 - val_accuracy: 0.3556\n",
    "Epoch 61/500\n",
    "3/3 [==============================] - 2s 590ms/step - loss: 0.3181 - accuracy: 0.8750 - val_loss: 2.1289 - val_accuracy: 0.5111\n",
    "Epoch 62/500\n",
    "3/3 [==============================] - 2s 670ms/step - loss: 0.3641 - accuracy: 0.8810 - val_loss: 2.2079 - val_accuracy: 0.4889\n",
    "Epoch 63/500\n",
    "3/3 [==============================] - 2s 669ms/step - loss: 0.3594 - accuracy: 0.8631 - val_loss: 2.2615 - val_accuracy: 0.5111\n",
    "Epoch 64/500\n",
    "3/3 [==============================] - 2s 468ms/step - loss: 0.2724 - accuracy: 0.8750 - val_loss: 2.1373 - val_accuracy: 0.5333\n",
    "Epoch 65/500\n",
    "3/3 [==============================] - 2s 607ms/step - loss: 0.3058 - accuracy: 0.8869 - val_loss: 2.2676 - val_accuracy: 0.5333\n",
    "Epoch 66/500\n",
    "3/3 [==============================] - 2s 650ms/step - loss: 0.2787 - accuracy: 0.9107 - val_loss: 2.2421 - val_accuracy: 0.5556\n",
    "Epoch 67/500\n",
    "3/3 [==============================] - 2s 636ms/step - loss: 0.3350 - accuracy: 0.8571 - val_loss: 2.2701 - val_accuracy: 0.5556\n",
    "Epoch 68/500\n",
    "3/3 [==============================] - 2s 510ms/step - loss: 0.2264 - accuracy: 0.9167 - val_loss: 2.1478 - val_accuracy: 0.5333\n",
    "Epoch 69/500\n",
    "3/3 [==============================] - 2s 676ms/step - loss: 0.3342 - accuracy: 0.9048 - val_loss: 2.0218 - val_accuracy: 0.5556\n",
    "Epoch 70/500\n",
    "3/3 [==============================] - 2s 520ms/step - loss: 0.2332 - accuracy: 0.8869 - val_loss: 1.8289 - val_accuracy: 0.5333\n",
    "Epoch 71/500\n",
    "3/3 [==============================] - 2s 633ms/step - loss: 0.2732 - accuracy: 0.9048 - val_loss: 1.8573 - val_accuracy: 0.5556\n",
    "Epoch 72/500\n",
    "3/3 [==============================] - 2s 620ms/step - loss: 0.2399 - accuracy: 0.9107 - val_loss: 1.9566 - val_accuracy: 0.5333\n",
    "Epoch 73/500\n",
    "3/3 [==============================] - 2s 497ms/step - loss: 0.3901 - accuracy: 0.8631 - val_loss: 1.9229 - val_accuracy: 0.5111\n",
    "Epoch 74/500\n",
    "3/3 [==============================] - 2s 454ms/step - loss: 0.2785 - accuracy: 0.9048 - val_loss: 1.9380 - val_accuracy: 0.5556\n",
    "Epoch 75/500\n",
    "3/3 [==============================] - 2s 481ms/step - loss: 0.2545 - accuracy: 0.8929 - val_loss: 1.9823 - val_accuracy: 0.5556\n",
    "Epoch 76/500\n",
    "3/3 [==============================] - 2s 525ms/step - loss: 0.4079 - accuracy: 0.8452 - val_loss: 1.7898 - val_accuracy: 0.5778\n",
    "Epoch 77/500\n",
    "3/3 [==============================] - 2s 620ms/step - loss: 0.3254 - accuracy: 0.8690 - val_loss: 1.9591 - val_accuracy: 0.5556\n",
    "Epoch 78/500\n",
    "3/3 [==============================] - 2s 597ms/step - loss: 0.2227 - accuracy: 0.9167 - val_loss: 1.8958 - val_accuracy: 0.5778\n",
    "Epoch 79/500\n",
    "3/3 [==============================] - 2s 473ms/step - loss: 0.3185 - accuracy: 0.8869 - val_loss: 1.8479 - val_accuracy: 0.5778\n",
    "Epoch 80/500\n",
    "3/3 [==============================] - 2s 585ms/step - loss: 0.2236 - accuracy: 0.8988 - val_loss: 1.8028 - val_accuracy: 0.5778\n",
    "Epoch 81/500\n",
    "3/3 [==============================] - 2s 594ms/step - loss: 0.2556 - accuracy: 0.9107 - val_loss: 1.8486 - val_accuracy: 0.5778\n",
    "Epoch 82/500\n",
    "3/3 [==============================] - 2s 466ms/step - loss: 0.3779 - accuracy: 0.8631 - val_loss: 2.2088 - val_accuracy: 0.5556\n",
    "Epoch 83/500\n",
    "3/3 [==============================] - 2s 593ms/step - loss: 0.2718 - accuracy: 0.8750 - val_loss: 2.5158 - val_accuracy: 0.5778\n",
    "Epoch 84/500\n",
    "3/3 [==============================] - 2s 596ms/step - loss: 0.2363 - accuracy: 0.9226 - val_loss: 2.6609 - val_accuracy: 0.5556\n",
    "Epoch 85/500\n",
    "3/3 [==============================] - 2s 587ms/step - loss: 0.2750 - accuracy: 0.8988 - val_loss: 2.4723 - val_accuracy: 0.5333\n",
    "Epoch 86/500\n",
    "3/3 [==============================] - 2s 450ms/step - loss: 0.3081 - accuracy: 0.8571 - val_loss: 1.9575 - val_accuracy: 0.6889\n",
    "Epoch 87/500\n",
    "3/3 [==============================] - 2s 470ms/step - loss: 0.2605 - accuracy: 0.9048 - val_loss: 1.3945 - val_accuracy: 0.7333\n",
    "Epoch 88/500\n",
    "3/3 [==============================] - 2s 452ms/step - loss: 0.2324 - accuracy: 0.9286 - val_loss: 1.0082 - val_accuracy: 0.7778\n",
    "Epoch 89/500\n",
    "3/3 [==============================] - 2s 446ms/step - loss: 0.3035 - accuracy: 0.8869 - val_loss: 0.7915 - val_accuracy: 0.8444\n",
    "Epoch 90/500\n",
    "3/3 [==============================] - 2s 453ms/step - loss: 0.2519 - accuracy: 0.9107 - val_loss: 0.7259 - val_accuracy: 0.8667\n",
    "Epoch 91/500\n",
    "3/3 [==============================] - 2s 467ms/step - loss: 0.2257 - accuracy: 0.9226 - val_loss: 0.7786 - val_accuracy: 0.8222\n",
    "\n",
    "Epoch 92/500\n",
    "3/3 [==============================] - 2s 478ms/step - loss: 0.2042 - accuracy: 0.9167 - val_loss: 0.8157 - val_accuracy: 0.8000\n",
    "Epoch 93/500\n",
    "3/3 [==============================] - 2s 590ms/step - loss: 0.2263 - accuracy: 0.9107 - val_loss: 0.8446 - val_accuracy: 0.7778\n",
    "Epoch 94/500\n",
    "3/3 [==============================] - 2s 596ms/step - loss: 0.2807 - accuracy: 0.9167 - val_loss: 0.8326 - val_accuracy: 0.8222\n",
    "Epoch 95/500\n",
    "3/3 [==============================] - 2s 598ms/step - loss: 0.2297 - accuracy: 0.9286 - val_loss: 0.8456 - val_accuracy: 0.8000\n",
    "Epoch 96/500\n",
    "3/3 [==============================] - 2s 472ms/step - loss: 0.1405 - accuracy: 0.9286 - val_loss: 0.8718 - val_accuracy: 0.8222\n",
    "Epoch 97/500\n",
    "3/3 [==============================] - 2s 590ms/step - loss: 0.3350 - accuracy: 0.8631 - val_loss: 0.9302 - val_accuracy: 0.8222\n",
    "Epoch 98/500\n",
    "3/3 [==============================] - 2s 470ms/step - loss: 0.1453 - accuracy: 0.9583 - val_loss: 0.9349 - val_accuracy: 0.8444\n",
    "Epoch 99/500\n",
    "3/3 [==============================] - 2s 464ms/step - loss: 0.2848 - accuracy: 0.9107 - val_loss: 1.0242 - val_accuracy: 0.8667\n",
    "Epoch 100/500\n",
    "3/3 [==============================] - 2s 444ms/step - loss: 0.2327 - accuracy: 0.9048 - val_loss: 1.1000 - val_accuracy: 0.8222\n",
    "Epoch 101/500\n",
    "3/3 [==============================] - 2s 451ms/step - loss: 0.1664 - accuracy: 0.9405 - val_loss: 1.1050 - val_accuracy: 0.8000\n",
    "Epoch 102/500\n",
    "3/3 [==============================] - 2s 591ms/step - loss: 0.2569 - accuracy: 0.9167 - val_loss: 0.9835 - val_accuracy: 0.8667\n",
    "Epoch 103/500\n",
    "3/3 [==============================] - 2s 464ms/step - loss: 0.1406 - accuracy: 0.9524 - val_loss: 1.0084 - val_accuracy: 0.8444\n",
    "Epoch 104/500\n",
    "3/3 [==============================] - 2s 454ms/step - loss: 0.2483 - accuracy: 0.9167 - val_loss: 1.0889 - val_accuracy: 0.8444\n",
    "Epoch 105/500\n",
    "3/3 [==============================] - 2s 596ms/step - loss: 0.2043 - accuracy: 0.9464 - val_loss: 1.1554 - val_accuracy: 0.8222\n",
    "Epoch 106/500\n",
    "3/3 [==============================] - 2s 473ms/step - loss: 0.1962 - accuracy: 0.9226 - val_loss: 1.1779 - val_accuracy: 0.8222\n",
    "Epoch 107/500\n",
    "3/3 [==============================] - 2s 458ms/step - loss: 0.1735 - accuracy: 0.9226 - val_loss: 1.1434 - val_accuracy: 0.8444\n",
    "Epoch 108/500\n",
    "3/3 [==============================] - 2s 447ms/step - loss: 0.2320 - accuracy: 0.9524 - val_loss: 1.1245 - val_accuracy: 0.8444\n",
    "Epoch 109/500\n",
    "3/3 [==============================] - 2s 444ms/step - loss: 0.2549 - accuracy: 0.8988 - val_loss: 1.1678 - val_accuracy: 0.8444\n",
    "Epoch 110/500\n",
    "3/3 [==============================] - 2s 450ms/step - loss: 0.2160 - accuracy: 0.9048 - val_loss: 1.2945 - val_accuracy: 0.8000\n",
    "Epoch 111/500\n",
    "3/3 [==============================] - 2s 446ms/step - loss: 0.1780 - accuracy: 0.9345 - val_loss: 1.4825 - val_accuracy: 0.8000\n",
    "Epoch 112/500\n",
    "3/3 [==============================] - 2s 589ms/step - loss: 0.1896 - accuracy: 0.9286 - val_loss: 1.6723 - val_accuracy: 0.7778\n",
    "Epoch 113/500\n",
    "3/3 [==============================] - 2s 473ms/step - loss: 0.1351 - accuracy: 0.9464 - val_loss: 1.8552 - val_accuracy: 0.6889\n",
    "Epoch 114/500\n",
    "3/3 [==============================] - 2s 589ms/step - loss: 0.1993 - accuracy: 0.9226 - val_loss: 2.0090 - val_accuracy: 0.6667\n",
    "Epoch 115/500\n",
    "3/3 [==============================] - 2s 474ms/step - loss: 0.1248 - accuracy: 0.9583 - val_loss: 2.0172 - val_accuracy: 0.6667\n",
    "Epoch 116/500\n",
    "3/3 [==============================] - 2s 450ms/step - loss: 0.2087 - accuracy: 0.9345 - val_loss: 1.9196 - val_accuracy: 0.6889\n",
    "Epoch 117/500\n",
    "3/3 [==============================] - 2s 448ms/step - loss: 0.1528 - accuracy: 0.9524 - val_loss: 1.6315 - val_accuracy: 0.7556\n",
    "Epoch 118/500\n",
    "3/3 [==============================] - 2s 603ms/step - loss: 0.1502 - accuracy: 0.9524 - val_loss: 1.2985 - val_accuracy: 0.8000\n",
    "Epoch 119/500\n",
    "3/3 [==============================] - 2s 593ms/step - loss: 0.2355 - accuracy: 0.9167 - val_loss: 1.0933 - val_accuracy: 0.8222\n",
    "Epoch 120/500\n",
    "3/3 [==============================] - 2s 587ms/step - loss: 0.1374 - accuracy: 0.9643 - val_loss: 0.9742 - val_accuracy: 0.8444\n",
    "Epoch 121/500\n",
    "3/3 [==============================] - 2s 452ms/step - loss: 0.1284 - accuracy: 0.9643 - val_loss: 0.9822 - val_accuracy: 0.8444\n",
    "Epoch 122/500\n",
    "3/3 [==============================] - 2s 464ms/step - loss: 0.1828 - accuracy: 0.9048 - val_loss: 1.0318 - val_accuracy: 0.8444\n",
    "Epoch 123/500\n",
    "3/3 [==============================] - 2s 588ms/step - loss: 0.1446 - accuracy: 0.9524 - val_loss: 1.1446 - val_accuracy: 0.8222\n",
    "Epoch 124/500\n",
    "3/3 [==============================] - 2s 592ms/step - loss: 0.1826 - accuracy: 0.9524 - val_loss: 1.2352 - val_accuracy: 0.8000\n",
    "Epoch 125/500\n",
    "3/3 [==============================] - 2s 472ms/step - loss: 0.1795 - accuracy: 0.9345 - val_loss: 1.3048 - val_accuracy: 0.8000\n",
    "Epoch 126/500\n",
    "3/3 [==============================] - 2s 452ms/step - loss: 0.1566 - accuracy: 0.9524 - val_loss: 1.3490 - val_accuracy: 0.8000\n",
    "Epoch 127/500\n",
    "3/3 [==============================] - 2s 449ms/step - loss: 0.1415 - accuracy: 0.9405 - val_loss: 1.3091 - val_accuracy: 0.8222\n",
    "Epoch 128/500\n",
    "3/3 [==============================] - 2s 453ms/step - loss: 0.1373 - accuracy: 0.9762 - val_loss: 1.1161 - val_accuracy: 0.8444\n",
    "Epoch 129/500\n",
    "3/3 [==============================] - 2s 445ms/step - loss: 0.1451 - accuracy: 0.9524 - val_loss: 0.9639 - val_accuracy: 0.8889\n",
    "Epoch 130/500\n",
    "3/3 [==============================] - 2s 467ms/step - loss: 0.3210 - accuracy: 0.9107 - val_loss: 0.8968 - val_accuracy: 0.8667\n",
    "Epoch 131/500\n",
    "3/3 [==============================] - 2s 594ms/step - loss: 0.1535 - accuracy: 0.9464 - val_loss: 0.9299 - val_accuracy: 0.8667\n",
    "Epoch 132/500\n",
    "3/3 [==============================] - 2s 468ms/step - loss: 0.1275 - accuracy: 0.9464 - val_loss: 0.9756 - val_accuracy: 0.8444\n",
    "Epoch 133/500\n",
    "3/3 [==============================] - 2s 585ms/step - loss: 0.1592 - accuracy: 0.9524 - val_loss: 1.0121 - val_accuracy: 0.8444\n",
    "Epoch 134/500\n",
    "3/3 [==============================] - 2s 592ms/step - loss: 0.2103 - accuracy: 0.9107 - val_loss: 1.0620 - val_accuracy: 0.8444\n",
    "Epoch 135/500\n",
    "3/3 [==============================] - 2s 595ms/step - loss: 0.1217 - accuracy: 0.9524 - val_loss: 1.1832 - val_accuracy: 0.7778\n",
    "Epoch 136/500\n",
    "3/3 [==============================] - 2s 450ms/step - loss: 0.1794 - accuracy: 0.9167 - val_loss: 1.3084 - val_accuracy: 0.7778\n",
    "Epoch 137/500\n",
    "3/3 [==============================] - 2s 450ms/step - loss: 0.1755 - accuracy: 0.9286 - val_loss: 1.3121 - val_accuracy: 0.8000\n",
    "Epoch 138/500\n",
    "3/3 [==============================] - 2s 452ms/step - loss: 0.2615 - accuracy: 0.9345 - val_loss: 1.2422 - val_accuracy: 0.7778\n",
    "Epoch 139/500\n",
    "3/3 [==============================] - 2s 592ms/step - loss: 0.2006 - accuracy: 0.9107 - val_loss: 1.1597 - val_accuracy: 0.8222\n",
    "Epoch 140/500\n",
    "3/3 [==============================] - 2s 598ms/step - loss: 0.1039 - accuracy: 0.9702 - val_loss: 1.1606 - val_accuracy: 0.8444\n",
    "Epoch 141/500\n",
    "3/3 [==============================] - 2s 468ms/step - loss: 0.2530 - accuracy: 0.9226 - val_loss: 1.1378 - val_accuracy: 0.8444\n",
    "Epoch 142/500\n",
    "3/3 [==============================] - 2s 595ms/step - loss: 0.1608 - accuracy: 0.9464 - val_loss: 1.1380 - val_accuracy: 0.8222\n",
    "Epoch 143/500\n",
    "3/3 [==============================] - 2s 468ms/step - loss: 0.1594 - accuracy: 0.9345 - val_loss: 1.1406 - val_accuracy: 0.8444\n",
    "Epoch 144/500\n",
    "3/3 [==============================] - 2s 448ms/step - loss: 0.1255 - accuracy: 0.9524 - val_loss: 1.2157 - val_accuracy: 0.8000\n",
    "Epoch 145/500\n",
    "3/3 [==============================] - 2s 587ms/step - loss: 0.1087 - accuracy: 0.9583 - val_loss: 1.3373 - val_accuracy: 0.8222\n",
    "Epoch 146/500\n",
    "3/3 [==============================] - 2s 478ms/step - loss: 0.1731 - accuracy: 0.9226 - val_loss: 1.3427 - val_accuracy: 0.8222\n",
    "Epoch 147/500\n",
    "3/3 [==============================] - 2s 592ms/step - loss: 0.1353 - accuracy: 0.9464 - val_loss: 1.2804 - val_accuracy: 0.8444\n",
    "Epoch 148/500\n",
    "3/3 [==============================] - 2s 593ms/step - loss: 0.1076 - accuracy: 0.9762 - val_loss: 1.2287 - val_accuracy: 0.8667\n",
    "Epoch 149/500\n",
    "\n",
    "3/3 [==============================] - 2s 592ms/step - loss: 0.1757 - accuracy: 0.9345 - val_loss: 1.1273 - val_accuracy: 0.8889\n",
    "Epoch 150/500\n",
    "3/3 [==============================] - 2s 598ms/step - loss: 0.1551 - accuracy: 0.9583 - val_loss: 1.0668 - val_accuracy: 0.8889\n",
    "Epoch 151/500\n",
    "3/3 [==============================] - 2s 446ms/step - loss: 0.1650 - accuracy: 0.9345 - val_loss: 1.0659 - val_accuracy: 0.8667\n",
    "Epoch 152/500\n",
    "3/3 [==============================] - 2s 469ms/step - loss: 0.1355 - accuracy: 0.9583 - val_loss: 1.0916 - val_accuracy: 0.8667\n",
    "Epoch 153/500\n",
    "3/3 [==============================] - 2s 455ms/step - loss: 0.1589 - accuracy: 0.9464 - val_loss: 1.0776 - val_accuracy: 0.8667\n",
    "Epoch 154/500\n",
    "3/3 [==============================] - 2s 451ms/step - loss: 0.1847 - accuracy: 0.9345 - val_loss: 1.1141 - val_accuracy: 0.8667\n",
    "Epoch 155/500\n",
    "3/3 [==============================] - 2s 472ms/step - loss: 0.1831 - accuracy: 0.9226 - val_loss: 1.1792 - val_accuracy: 0.8444\n",
    "Epoch 156/500\n",
    "3/3 [==============================] - 2s 448ms/step - loss: 0.1954 - accuracy: 0.9286 - val_loss: 1.1968 - val_accuracy: 0.8444\n",
    "Epoch 157/500\n",
    "3/3 [==============================] - 2s 464ms/step - loss: 0.1625 - accuracy: 0.9286 - val_loss: 1.1758 - val_accuracy: 0.8667\n",
    "Epoch 158/500\n",
    "3/3 [==============================] - 2s 596ms/step - loss: 0.0904 - accuracy: 0.9702 - val_loss: 1.1655 - val_accuracy: 0.8889\n",
    "Epoch 159/500\n",
    "3/3 [==============================] - 2s 474ms/step - loss: 0.0883 - accuracy: 0.9702 - val_loss: 1.1251 - val_accuracy: 0.8667\n",
    "Epoch 160/500\n",
    "3/3 [==============================] - 2s 593ms/step - loss: 0.1986 - accuracy: 0.9583 - val_loss: 1.0899 - val_accuracy: 0.8889\n",
    "Epoch 161/500\n",
    "3/3 [==============================] - 2s 475ms/step - loss: 0.1586 - accuracy: 0.9583 - val_loss: 1.0289 - val_accuracy: 0.8889\n",
    "Epoch 162/500\n",
    "3/3 [==============================] - 2s 469ms/step - loss: 0.1397 - accuracy: 0.9464 - val_loss: 0.9500 - val_accuracy: 0.8889\n",
    "Epoch 163/500\n",
    "3/3 [==============================] - 2s 474ms/step - loss: 0.1752 - accuracy: 0.9286 - val_loss: 0.8705 - val_accuracy: 0.9111\n",
    "Epoch 164/500\n",
    "3/3 [==============================] - 2s 470ms/step - loss: 0.2403 - accuracy: 0.9464 - val_loss: 0.7558 - val_accuracy: 0.9111\n",
    "Epoch 165/500\n",
    "3/3 [==============================] - 2s 470ms/step - loss: 0.2090 - accuracy: 0.9286 - val_loss: 0.7037 - val_accuracy: 0.8889\n",
    "Epoch 166/500\n",
    "3/3 [==============================] - 2s 468ms/step - loss: 0.1386 - accuracy: 0.9524 - val_loss: 0.7407 - val_accuracy: 0.8889\n",
    "Epoch 167/500\n",
    "3/3 [==============================] - 2s 591ms/step - loss: 0.1679 - accuracy: 0.9345 - val_loss: 0.7952 - val_accuracy: 0.8667\n",
    "Epoch 168/500\n",
    "3/3 [==============================] - 2s 455ms/step - loss: 0.1813 - accuracy: 0.9405 - val_loss: 5.9707 - val_accuracy: 0.3556\n",
    "Epoch 169/500\n",
    "3/3 [==============================] - 2s 467ms/step - loss: 0.1104 - accuracy: 0.9762 - val_loss: 3.4874 - val_accuracy: 0.4444\n",
    "Epoch 170/500\n",
    "3/3 [==============================] - 2s 589ms/step - loss: 0.1647 - accuracy: 0.9405 - val_loss: 2.3514 - val_accuracy: 0.5778\n",
    "Epoch 171/500\n",
    "3/3 [==============================] - 2s 451ms/step - loss: 0.1354 - accuracy: 0.9524 - val_loss: 1.8235 - val_accuracy: 0.6000\n",
    "Epoch 172/500\n",
    "3/3 [==============================] - 2s 593ms/step - loss: 0.1449 - accuracy: 0.9464 - val_loss: 1.8006 - val_accuracy: 0.6667\n",
    "Epoch 173/500\n",
    "3/3 [==============================] - 2s 592ms/step - loss: 0.1469 - accuracy: 0.9464 - val_loss: 1.8786 - val_accuracy: 0.6444\n",
    "Epoch 174/500\n",
    "3/3 [==============================] - 2s 449ms/step - loss: 0.0912 - accuracy: 0.9643 - val_loss: 1.7850 - val_accuracy: 0.6667\n",
    "Epoch 175/500\n",
    "3/3 [==============================] - 2s 473ms/step - loss: 0.1020 - accuracy: 0.9643 - val_loss: 1.5367 - val_accuracy: 0.7111\n",
    "Epoch 176/500\n",
    "3/3 [==============================] - 2s 465ms/step - loss: 0.1446 - accuracy: 0.9583 - val_loss: 1.4011 - val_accuracy: 0.7333\n",
    "Epoch 177/500\n",
    "3/3 [==============================] - 2s 588ms/step - loss: 0.1487 - accuracy: 0.9226 - val_loss: 1.2098 - val_accuracy: 0.7333\n",
    "Epoch 178/500\n",
    "3/3 [==============================] - 2s 595ms/step - loss: 0.1026 - accuracy: 0.9524 - val_loss: 1.0910 - val_accuracy: 0.7778\n",
    "Epoch 179/500\n",
    "3/3 [==============================] - 2s 448ms/step - loss: 0.1253 - accuracy: 0.9643 - val_loss: 1.0772 - val_accuracy: 0.8000\n",
    "Epoch 180/500\n",
    "3/3 [==============================] - 2s 454ms/step - loss: 0.1140 - accuracy: 0.9583 - val_loss: 1.0726 - val_accuracy: 0.8000\n",
    "Epoch 181/500\n",
    "3/3 [==============================] - 2s 444ms/step - loss: 0.2027 - accuracy: 0.9107 - val_loss: 1.0378 - val_accuracy: 0.7778\n",
    "Epoch 182/500\n",
    "3/3 [==============================] - 2s 452ms/step - loss: 0.1217 - accuracy: 0.9464 - val_loss: 1.0133 - val_accuracy: 0.7778\n",
    "Epoch 183/500\n",
    "3/3 [==============================] - 2s 450ms/step - loss: 0.0772 - accuracy: 0.9762 - val_loss: 0.9913 - val_accuracy: 0.7778\n",
    "Epoch 184/500\n",
    "3/3 [==============================] - 2s 593ms/step - loss: 0.1279 - accuracy: 0.9702 - val_loss: 0.9582 - val_accuracy: 0.7778\n",
    "Epoch 185/500\n",
    "3/3 [==============================] - 2s 459ms/step - loss: 0.1597 - accuracy: 0.9524 - val_loss: 0.9711 - val_accuracy: 0.7778\n",
    "Epoch 186/500\n",
    "3/3 [==============================] - 2s 475ms/step - loss: 0.1728 - accuracy: 0.9405 - val_loss: 1.0517 - val_accuracy: 0.7778\n",
    "Epoch 187/500\n",
    "3/3 [==============================] - 2s 454ms/step - loss: 0.1532 - accuracy: 0.9286 - val_loss: 1.0781 - val_accuracy: 0.8000\n",
    "Epoch 188/500\n",
    "3/3 [==============================] - 2s 454ms/step - loss: 0.1668 - accuracy: 0.9286 - val_loss: 1.0895 - val_accuracy: 0.8222\n",
    "Epoch 189/500\n",
    "3/3 [==============================] - 2s 588ms/step - loss: 0.1417 - accuracy: 0.9583 - val_loss: 1.0681 - val_accuracy: 0.8222\n",
    "Epoch 190/500\n",
    "3/3 [==============================] - 2s 597ms/step - loss: 0.0903 - accuracy: 0.9702 - val_loss: 1.0623 - val_accuracy: 0.8444\n",
    "Epoch 191/500\n",
    "3/3 [==============================] - 2s 454ms/step - loss: 0.1430 - accuracy: 0.9524 - val_loss: 1.1140 - val_accuracy: 0.8444\n",
    "Epoch 192/500\n",
    "3/3 [==============================] - 2s 469ms/step - loss: 0.1233 - accuracy: 0.9643 - val_loss: 1.1629 - val_accuracy: 0.8444\n",
    "Epoch 193/500\n",
    "3/3 [==============================] - 2s 593ms/step - loss: 0.1653 - accuracy: 0.9345 - val_loss: 1.1680 - val_accuracy: 0.8444\n",
    "Epoch 194/500\n",
    "3/3 [==============================] - 2s 593ms/step - loss: 0.0897 - accuracy: 0.9762 - val_loss: 1.1321 - val_accuracy: 0.8222\n",
    "Epoch 195/500\n",
    "3/3 [==============================] - 2s 477ms/step - loss: 0.1011 - accuracy: 0.9583 - val_loss: 1.0648 - val_accuracy: 0.8444\n",
    "Epoch 196/500\n",
    "3/3 [==============================] - 2s 591ms/step - loss: 0.0947 - accuracy: 0.9643 - val_loss: 0.9638 - val_accuracy: 0.8444\n",
    "Epoch 197/500\n",
    "3/3 [==============================] - 2s 453ms/step - loss: 0.1215 - accuracy: 0.9762 - val_loss: 0.8690 - val_accuracy: 0.8667\n",
    "Epoch 198/500\n",
    "3/3 [==============================] - 2s 453ms/step - loss: 0.1578 - accuracy: 0.9226 - val_loss: 0.8118 - val_accuracy: 0.8667\n",
    "Epoch 199/500\n",
    "3/3 [==============================] - 2s 473ms/step - loss: 0.1218 - accuracy: 0.9583 - val_loss: 0.8077 - val_accuracy: 0.8667\n",
    "Epoch 200/500\n",
    "3/3 [==============================] - 2s 451ms/step - loss: 0.0899 - accuracy: 0.9702 - val_loss: 0.8740 - val_accuracy: 0.8667\n",
    "Epoch 201/500\n",
    "3/3 [==============================] - 2s 476ms/step - loss: 0.1582 - accuracy: 0.9583 - val_loss: 0.9340 - val_accuracy: 0.8889\n",
    "Epoch 202/500\n",
    "3/3 [==============================] - 2s 594ms/step - loss: 0.1603 - accuracy: 0.9524 - val_loss: 0.9768 - val_accuracy: 0.8667\n",
    "Epoch 203/500\n",
    "3/3 [==============================] - 2s 466ms/step - loss: 0.1642 - accuracy: 0.9345 - val_loss: 1.0505 - val_accuracy: 0.8444\n",
    "Epoch 204/500\n",
    "3/3 [==============================] - 2s 592ms/step - loss: 0.1719 - accuracy: 0.9345 - val_loss: 1.1646 - val_accuracy: 0.8222\n",
    "Epoch 205/500\n",
    "3/3 [==============================] - 2s 453ms/step - loss: 0.1028 - accuracy: 0.9702 - val_loss: 1.2730 - val_accuracy: 0.7556\n",
    "Epoch 206/500\n",
    "\n",
    "3/3 [==============================] - 2s 467ms/step - loss: 0.0868 - accuracy: 0.9583 - val_loss: 1.4391 - val_accuracy: 0.7111\n",
    "Epoch 207/500\n",
    "3/3 [==============================] - 2s 449ms/step - loss: 0.1086 - accuracy: 0.9643 - val_loss: 1.5880 - val_accuracy: 0.7333\n",
    "Epoch 208/500\n",
    "3/3 [==============================] - 2s 596ms/step - loss: 0.1367 - accuracy: 0.9583 - val_loss: 1.4533 - val_accuracy: 0.7333\n",
    "Epoch 209/500\n",
    "3/3 [==============================] - 2s 474ms/step - loss: 0.1387 - accuracy: 0.9286 - val_loss: 1.4130 - val_accuracy: 0.7778\n",
    "Epoch 210/500\n",
    "3/3 [==============================] - 2s 453ms/step - loss: 0.1149 - accuracy: 0.9583 - val_loss: 1.3192 - val_accuracy: 0.7778\n",
    "Epoch 211/500\n",
    "3/3 [==============================] - 2s 451ms/step - loss: 0.0902 - accuracy: 0.9821 - val_loss: 1.2424 - val_accuracy: 0.7778\n",
    "Epoch 212/500\n",
    "3/3 [==============================] - 2s 591ms/step - loss: 0.1406 - accuracy: 0.9583 - val_loss: 1.2495 - val_accuracy: 0.7556\n",
    "Epoch 213/500\n",
    "3/3 [==============================] - 2s 455ms/step - loss: 0.0773 - accuracy: 0.9762 - val_loss: 1.1904 - val_accuracy: 0.7778\n",
    "Epoch 214/500\n",
    "3/3 [==============================] - 2s 469ms/step - loss: 0.0887 - accuracy: 0.9762 - val_loss: 1.1445 - val_accuracy: 0.8222\n",
    "Epoch 215/500\n",
    "3/3 [==============================] - 2s 451ms/step - loss: 0.1797 - accuracy: 0.9286 - val_loss: 1.0075 - val_accuracy: 0.8667\n",
    "Epoch 216/500\n",
    "3/3 [==============================] - 2s 593ms/step - loss: 0.1748 - accuracy: 0.9524 - val_loss: 0.8518 - val_accuracy: 0.8667\n",
    "Epoch 217/500\n",
    "3/3 [==============================] - 2s 470ms/step - loss: 0.1855 - accuracy: 0.9583 - val_loss: 0.7909 - val_accuracy: 0.8444\n",
    "Epoch 218/500\n",
    "3/3 [==============================] - 2s 456ms/step - loss: 0.1208 - accuracy: 0.9643 - val_loss: 0.8060 - val_accuracy: 0.8444\n",
    "Epoch 219/500\n",
    "3/3 [==============================] - 2s 478ms/step - loss: 0.1720 - accuracy: 0.9226 - val_loss: 0.9153 - val_accuracy: 0.8222\n",
    "Epoch 220/500\n",
    "3/3 [==============================] - 2s 590ms/step - loss: 0.2008 - accuracy: 0.9464 - val_loss: 0.9123 - val_accuracy: 0.8000\n",
    "Epoch 221/500\n",
    "3/3 [==============================] - 2s 451ms/step - loss: 0.0973 - accuracy: 0.9643 - val_loss: 0.9408 - val_accuracy: 0.7778\n",
    "Epoch 222/500\n",
    "3/3 [==============================] - 2s 455ms/step - loss: 0.1845 - accuracy: 0.9405 - val_loss: 1.0049 - val_accuracy: 0.7556\n",
    "Epoch 223/500\n",
    "3/3 [==============================] - 2s 598ms/step - loss: 0.0946 - accuracy: 0.9524 - val_loss: 0.9832 - val_accuracy: 0.7556\n",
    "Epoch 224/500\n",
    "3/3 [==============================] - 2s 596ms/step - loss: 0.1027 - accuracy: 0.9702 - val_loss: 0.9452 - val_accuracy: 0.7556\n",
    "Epoch 225/500\n",
    "3/3 [==============================] - 2s 446ms/step - loss: 0.1977 - accuracy: 0.9345 - val_loss: 0.9108 - val_accuracy: 0.8000\n",
    "Epoch 226/500\n",
    "3/3 [==============================] - 2s 452ms/step - loss: 0.0869 - accuracy: 0.9762 - val_loss: 0.9340 - val_accuracy: 0.7778\n",
    "Epoch 227/500\n",
    "3/3 [==============================] - 2s 594ms/step - loss: 0.2075 - accuracy: 0.9405 - val_loss: 1.0167 - val_accuracy: 0.8222\n",
    "Epoch 228/500\n",
    "3/3 [==============================] - 2s 450ms/step - loss: 0.1617 - accuracy: 0.9524 - val_loss: 1.1198 - val_accuracy: 0.8222\n",
    "Epoch 229/500\n",
    "3/3 [==============================] - 2s 475ms/step - loss: 0.1527 - accuracy: 0.9524 - val_loss: 1.0164 - val_accuracy: 0.8444\n",
    "Epoch 230/500\n",
    "3/3 [==============================] - 2s 472ms/step - loss: 0.0718 - accuracy: 0.9643 - val_loss: 1.0028 - val_accuracy: 0.8889\n",
    "Epoch 231/500\n",
    "3/3 [==============================] - 2s 594ms/step - loss: 0.0981 - accuracy: 0.9702 - val_loss: 1.0118 - val_accuracy: 0.9111\n",
    "Epoch 232/500\n",
    "3/3 [==============================] - 2s 446ms/step - loss: 0.1298 - accuracy: 0.9464 - val_loss: 1.0502 - val_accuracy: 0.9111\n",
    "Epoch 233/500\n",
    "3/3 [==============================] - 2s 449ms/step - loss: 0.1112 - accuracy: 0.9524 - val_loss: 1.1252 - val_accuracy: 0.9111\n",
    "Epoch 234/500\n",
    "3/3 [==============================] - 2s 588ms/step - loss: 0.1185 - accuracy: 0.9643 - val_loss: 1.2556 - val_accuracy: 0.9111\n",
    "Epoch 235/500\n",
    "3/3 [==============================] - 2s 467ms/step - loss: 0.2186 - accuracy: 0.9048 - val_loss: 1.2822 - val_accuracy: 0.9111\n",
    "Epoch 236/500\n",
    "3/3 [==============================] - 2s 467ms/step - loss: 0.1490 - accuracy: 0.9524 - val_loss: 1.2430 - val_accuracy: 0.9111\n",
    "Epoch 237/500\n",
    "3/3 [==============================] - 2s 446ms/step - loss: 0.0622 - accuracy: 0.9821 - val_loss: 1.2028 - val_accuracy: 0.8889\n",
    "Epoch 238/500\n",
    "3/3 [==============================] - 2s 468ms/step - loss: 0.1285 - accuracy: 0.9643 - val_loss: 1.2282 - val_accuracy: 0.8889\n",
    "Epoch 239/500\n",
    "3/3 [==============================] - 2s 477ms/step - loss: 0.0439 - accuracy: 0.9821 - val_loss: 1.2496 - val_accuracy: 0.8667\n",
    "Epoch 240/500\n",
    "3/3 [==============================] - 2s 469ms/step - loss: 0.1284 - accuracy: 0.9643 - val_loss: 1.2218 - val_accuracy: 0.8444\n",
    "Epoch 241/500\n",
    "3/3 [==============================] - 2s 472ms/step - loss: 0.2276 - accuracy: 0.9464 - val_loss: 1.1455 - val_accuracy: 0.8667\n",
    "Epoch 242/500\n",
    "3/3 [==============================] - 2s 474ms/step - loss: 0.1043 - accuracy: 0.9702 - val_loss: 1.0853 - val_accuracy: 0.8222\n",
    "Epoch 243/500\n",
    "3/3 [==============================] - 2s 482ms/step - loss: 0.1337 - accuracy: 0.9583 - val_loss: 0.9952 - val_accuracy: 0.8444\n",
    "Epoch 244/500\n",
    "3/3 [==============================] - 2s 596ms/step - loss: 0.0912 - accuracy: 0.9702 - val_loss: 0.9237 - val_accuracy: 0.9111\n",
    "Epoch 245/500\n",
    "3/3 [==============================] - 2s 467ms/step - loss: 0.0715 - accuracy: 0.9762 - val_loss: 0.9091 - val_accuracy: 0.8889\n",
    "Epoch 246/500\n",
    "3/3 [==============================] - 2s 592ms/step - loss: 0.2192 - accuracy: 0.9167 - val_loss: 0.8628 - val_accuracy: 0.8889\n",
    "Epoch 247/500\n",
    "3/3 [==============================] - 2s 473ms/step - loss: 0.1119 - accuracy: 0.9583 - val_loss: 0.8423 - val_accuracy: 0.8889\n",
    "Epoch 248/500\n",
    "3/3 [==============================] - 2s 469ms/step - loss: 0.1604 - accuracy: 0.9583 - val_loss: 0.8724 - val_accuracy: 0.8667\n",
    "Epoch 249/500\n",
    "3/3 [==============================] - 2s 458ms/step - loss: 0.1609 - accuracy: 0.9405 - val_loss: 0.9502 - val_accuracy: 0.8444\n",
    "Epoch 250/500\n",
    "3/3 [==============================] - 2s 454ms/step - loss: 0.2278 - accuracy: 0.9464 - val_loss: 0.9441 - val_accuracy: 0.8444\n",
    "Epoch 251/500\n",
    "3/3 [==============================] - 2s 457ms/step - loss: 0.0907 - accuracy: 0.9762 - val_loss: 0.9326 - val_accuracy: 0.8667\n",
    "Epoch 252/500\n",
    "3/3 [==============================] - 2s 454ms/step - loss: 0.1317 - accuracy: 0.9702 - val_loss: 0.9550 - val_accuracy: 0.8667\n",
    "Epoch 253/500\n",
    "3/3 [==============================] - 2s 472ms/step - loss: 0.1460 - accuracy: 0.9345 - val_loss: 0.9959 - val_accuracy: 0.8444\n",
    "Epoch 254/500\n",
    "3/3 [==============================] - 2s 464ms/step - loss: 0.1430 - accuracy: 0.9524 - val_loss: 1.0095 - val_accuracy: 0.8444\n",
    "Epoch 255/500\n",
    "3/3 [==============================] - 2s 470ms/step - loss: 0.1051 - accuracy: 0.9702 - val_loss: 0.9893 - val_accuracy: 0.8444\n",
    "Epoch 256/500\n",
    "3/3 [==============================] - 2s 476ms/step - loss: 0.1876 - accuracy: 0.9643 - val_loss: 0.9908 - val_accuracy: 0.8444\n",
    "Epoch 257/500\n",
    "3/3 [==============================] - 2s 447ms/step - loss: 0.1036 - accuracy: 0.9702 - val_loss: 1.0211 - val_accuracy: 0.8444\n",
    "Epoch 258/500\n",
    "3/3 [==============================] - 2s 449ms/step - loss: 0.1379 - accuracy: 0.9583 - val_loss: 1.0282 - val_accuracy: 0.8444\n",
    "Epoch 259/500\n",
    "3/3 [==============================] - 2s 465ms/step - loss: 0.1186 - accuracy: 0.9583 - val_loss: 0.9894 - val_accuracy: 0.8444\n",
    "Epoch 260/500\n",
    "3/3 [==============================] - 2s 447ms/step - loss: 0.1078 - accuracy: 0.9524 - val_loss: 0.9390 - val_accuracy: 0.8667\n",
    "Epoch 261/500\n",
    "3/3 [==============================] - 2s 464ms/step - loss: 0.1129 - accuracy: 0.9702 - val_loss: 0.9026 - val_accuracy: 0.8667\n",
    "Epoch 262/500\n",
    "3/3 [==============================] - 2s 594ms/step - loss: 0.0351 - accuracy: 0.9881 - val_loss: 0.8701 - val_accuracy: 0.8667\n",
    "Epoch 263/500\n",
    "\n",
    "3/3 [==============================] - 2s 594ms/step - loss: 0.0755 - accuracy: 0.9821 - val_loss: 0.8686 - val_accuracy: 0.8667\n",
    "Epoch 264/500\n",
    "3/3 [==============================] - 2s 446ms/step - loss: 0.0441 - accuracy: 0.9940 - val_loss: 0.8650 - val_accuracy: 0.8667\n",
    "Epoch 265/500\n",
    "3/3 [==============================] - 2s 454ms/step - loss: 0.1112 - accuracy: 0.9405 - val_loss: 0.9169 - val_accuracy: 0.8667\n",
    "Epoch 266/500\n",
    "3/3 [==============================] - 2s 448ms/step - loss: 0.1017 - accuracy: 0.9643 - val_loss: 1.0602 - val_accuracy: 0.8222\n",
    "Epoch 267/500\n",
    "3/3 [==============================] - 2s 593ms/step - loss: 0.1021 - accuracy: 0.9702 - val_loss: 1.2061 - val_accuracy: 0.8000\n",
    "Epoch 268/500\n",
    "3/3 [==============================] - 2s 593ms/step - loss: 0.0876 - accuracy: 0.9643 - val_loss: 1.2993 - val_accuracy: 0.7778\n",
    "Epoch 269/500\n",
    "3/3 [==============================] - 2s 471ms/step - loss: 0.0797 - accuracy: 0.9702 - val_loss: 1.4101 - val_accuracy: 0.7333\n",
    "Epoch 270/500\n",
    "3/3 [==============================] - 2s 462ms/step - loss: 0.1136 - accuracy: 0.9702 - val_loss: 1.4073 - val_accuracy: 0.7111\n",
    "Epoch 271/500\n",
    "3/3 [==============================] - 2s 586ms/step - loss: 0.1080 - accuracy: 0.9583 - val_loss: 1.4112 - val_accuracy: 0.7556\n",
    "Epoch 272/500\n",
    "3/3 [==============================] - 2s 602ms/step - loss: 0.0690 - accuracy: 0.9702 - val_loss: 1.3383 - val_accuracy: 0.7778\n",
    "Epoch 273/500\n",
    "3/3 [==============================] - 2s 598ms/step - loss: 0.1335 - accuracy: 0.9464 - val_loss: 1.2851 - val_accuracy: 0.7778\n",
    "Epoch 274/500\n",
    "3/3 [==============================] - 2s 465ms/step - loss: 0.0988 - accuracy: 0.9702 - val_loss: 1.2100 - val_accuracy: 0.8000\n",
    "Epoch 275/500\n",
    "3/3 [==============================] - 2s 447ms/step - loss: 0.0884 - accuracy: 0.9524 - val_loss: 1.2320 - val_accuracy: 0.8000\n",
    "Epoch 276/500\n",
    "3/3 [==============================] - 2s 602ms/step - loss: 0.1065 - accuracy: 0.9464 - val_loss: 1.1893 - val_accuracy: 0.8000\n",
    "Epoch 277/500\n",
    "3/3 [==============================] - 2s 589ms/step - loss: 0.1236 - accuracy: 0.9702 - val_loss: 1.1731 - val_accuracy: 0.8000\n",
    "Epoch 278/500\n",
    "3/3 [==============================] - 2s 592ms/step - loss: 0.1422 - accuracy: 0.9524 - val_loss: 1.1222 - val_accuracy: 0.8000\n",
    "Epoch 279/500\n",
    "3/3 [==============================] - 2s 451ms/step - loss: 0.0991 - accuracy: 0.9524 - val_loss: 1.0152 - val_accuracy: 0.8222\n",
    "Epoch 280/500\n",
    "3/3 [==============================] - 2s 451ms/step - loss: 0.0772 - accuracy: 0.9821 - val_loss: 0.9125 - val_accuracy: 0.8444\n",
    "Epoch 281/500\n",
    "3/3 [==============================] - 2s 452ms/step - loss: 0.1217 - accuracy: 0.9643 - val_loss: 0.8563 - val_accuracy: 0.8444\n",
    "Epoch 282/500\n",
    "3/3 [==============================] - 2s 594ms/step - loss: 0.0666 - accuracy: 0.9762 - val_loss: 0.8426 - val_accuracy: 0.8667\n",
    "Epoch 283/500\n",
    "3/3 [==============================] - 2s 593ms/step - loss: 0.1241 - accuracy: 0.9643 - val_loss: 0.8378 - val_accuracy: 0.8444\n",
    "Epoch 284/500\n",
    "3/3 [==============================] - 2s 595ms/step - loss: 0.1288 - accuracy: 0.9583 - val_loss: 0.9038 - val_accuracy: 0.8444\n",
    "Epoch 285/500\n",
    "3/3 [==============================] - 2s 462ms/step - loss: 0.0404 - accuracy: 0.9881 - val_loss: 0.9260 - val_accuracy: 0.8444\n",
    "Epoch 286/500\n",
    "3/3 [==============================] - 2s 447ms/step - loss: 0.0774 - accuracy: 0.9643 - val_loss: 0.9216 - val_accuracy: 0.8444\n",
    "Epoch 287/500\n",
    "3/3 [==============================] - 2s 471ms/step - loss: 0.1057 - accuracy: 0.9583 - val_loss: 0.8506 - val_accuracy: 0.8444\n",
    "Epoch 288/500\n",
    "3/3 [==============================] - 2s 468ms/step - loss: 0.0555 - accuracy: 0.9821 - val_loss: 0.8012 - val_accuracy: 0.8667\n",
    "Epoch 289/500\n",
    "3/3 [==============================] - 2s 451ms/step - loss: 0.0877 - accuracy: 0.9583 - val_loss: 0.7653 - val_accuracy: 0.8667\n",
    "Epoch 290/500\n",
    "3/3 [==============================] - 2s 477ms/step - loss: 0.0810 - accuracy: 0.9762 - val_loss: 0.7627 - val_accuracy: 0.8889\n",
    "Epoch 291/500\n",
    "3/3 [==============================] - 2s 452ms/step - loss: 0.0701 - accuracy: 0.9762 - val_loss: 0.7797 - val_accuracy: 0.8889\n",
    "Epoch 292/500\n",
    "3/3 [==============================] - 2s 475ms/step - loss: 0.0827 - accuracy: 0.9702 - val_loss: 0.7952 - val_accuracy: 0.8889\n",
    "Epoch 293/500\n",
    "3/3 [==============================] - 2s 591ms/step - loss: 0.1229 - accuracy: 0.9524 - val_loss: 0.7944 - val_accuracy: 0.8889\n",
    "Epoch 294/500\n",
    "3/3 [==============================] - 2s 594ms/step - loss: 0.0904 - accuracy: 0.9821 - val_loss: 0.8377 - val_accuracy: 0.9111\n",
    "Epoch 295/500\n",
    "3/3 [==============================] - 2s 472ms/step - loss: 0.1212 - accuracy: 0.9583 - val_loss: 0.9195 - val_accuracy: 0.8889\n",
    "Epoch 296/500\n",
    "3/3 [==============================] - 2s 454ms/step - loss: 0.1187 - accuracy: 0.9405 - val_loss: 1.0130 - val_accuracy: 0.8667\n",
    "Epoch 297/500\n",
    "3/3 [==============================] - 2s 450ms/step - loss: 0.0825 - accuracy: 0.9881 - val_loss: 1.0429 - val_accuracy: 0.8889\n",
    "Epoch 298/500\n",
    "3/3 [==============================] - 2s 466ms/step - loss: 0.1254 - accuracy: 0.9583 - val_loss: 1.0645 - val_accuracy: 0.8889\n",
    "Epoch 299/500\n",
    "3/3 [==============================] - 2s 466ms/step - loss: 0.0580 - accuracy: 0.9762 - val_loss: 1.1389 - val_accuracy: 0.8667\n",
    "Epoch 300/500\n",
    "3/3 [==============================] - 2s 470ms/step - loss: 0.0493 - accuracy: 0.9881 - val_loss: 1.1492 - val_accuracy: 0.8667\n",
    "Epoch 301/500\n",
    "3/3 [==============================] - 2s 602ms/step - loss: 0.1360 - accuracy: 0.9583 - val_loss: 1.1895 - val_accuracy: 0.8667\n",
    "Epoch 302/500\n",
    "3/3 [==============================] - 2s 598ms/step - loss: 0.0832 - accuracy: 0.9583 - val_loss: 1.2267 - val_accuracy: 0.8667\n",
    "Epoch 303/500\n",
    "3/3 [==============================] - 2s 453ms/step - loss: 0.1880 - accuracy: 0.9405 - val_loss: 1.2128 - val_accuracy: 0.8667\n",
    "Epoch 304/500\n",
    "3/3 [==============================] - 2s 473ms/step - loss: 0.0742 - accuracy: 0.9762 - val_loss: 1.1881 - val_accuracy: 0.8667\n",
    "Epoch 305/500\n",
    "3/3 [==============================] - 2s 450ms/step - loss: 0.0733 - accuracy: 0.9583 - val_loss: 1.1963 - val_accuracy: 0.8444\n",
    "Epoch 306/500\n",
    "3/3 [==============================] - 2s 450ms/step - loss: 0.0640 - accuracy: 0.9762 - val_loss: 1.2210 - val_accuracy: 0.8667\n",
    "Epoch 307/500\n",
    "3/3 [==============================] - 2s 471ms/step - loss: 0.1084 - accuracy: 0.9643 - val_loss: 1.2447 - val_accuracy: 0.8667\n",
    "Epoch 308/500\n",
    "3/3 [==============================] - 2s 467ms/step - loss: 0.1187 - accuracy: 0.9583 - val_loss: 1.2041 - val_accuracy: 0.8667\n",
    "Epoch 309/500\n",
    "3/3 [==============================] - 2s 599ms/step - loss: 0.0933 - accuracy: 0.9702 - val_loss: 1.1316 - val_accuracy: 0.8667\n",
    "Epoch 310/500\n",
    "3/3 [==============================] - 2s 595ms/step - loss: 0.1396 - accuracy: 0.9524 - val_loss: 1.0748 - val_accuracy: 0.8222\n",
    "Epoch 311/500\n",
    "3/3 [==============================] - 2s 608ms/step - loss: 0.0843 - accuracy: 0.9702 - val_loss: 1.1497 - val_accuracy: 0.8222\n",
    "Epoch 312/500\n",
    "3/3 [==============================] - 2s 592ms/step - loss: 0.0635 - accuracy: 0.9762 - val_loss: 1.2474 - val_accuracy: 0.8000\n",
    "Epoch 313/500\n",
    "3/3 [==============================] - 2s 591ms/step - loss: 0.1114 - accuracy: 0.9702 - val_loss: 1.3175 - val_accuracy: 0.8000\n",
    "Epoch 314/500\n",
    "3/3 [==============================] - 2s 598ms/step - loss: 0.0536 - accuracy: 0.9762 - val_loss: 1.3538 - val_accuracy: 0.8222\n",
    "Epoch 315/500\n",
    "3/3 [==============================] - 2s 454ms/step - loss: 0.0541 - accuracy: 0.9702 - val_loss: 1.2905 - val_accuracy: 0.8222\n",
    "Epoch 316/500\n",
    "3/3 [==============================] - 2s 449ms/step - loss: 0.0680 - accuracy: 0.9762 - val_loss: 1.1934 - val_accuracy: 0.8222\n",
    "Epoch 317/500\n",
    "3/3 [==============================] - 2s 456ms/step - loss: 0.0782 - accuracy: 0.9821 - val_loss: 1.1786 - val_accuracy: 0.8444\n",
    "Epoch 318/500\n",
    "3/3 [==============================] - 2s 477ms/step - loss: 0.0869 - accuracy: 0.9583 - val_loss: 1.2460 - val_accuracy: 0.8222\n",
    "Epoch 319/500\n",
    "3/3 [==============================] - 2s 598ms/step - loss: 0.1246 - accuracy: 0.9583 - val_loss: 1.3134 - val_accuracy: 0.8000\n",
    "Epoch 320/500\n",
    "\n",
    "3/3 [==============================] - 2s 471ms/step - loss: 0.1048 - accuracy: 0.9524 - val_loss: 1.1997 - val_accuracy: 0.8000\n",
    "Epoch 321/500\n",
    "3/3 [==============================] - 2s 475ms/step - loss: 0.1217 - accuracy: 0.9524 - val_loss: 1.0358 - val_accuracy: 0.8222\n",
    "Epoch 322/500\n",
    "3/3 [==============================] - 2s 609ms/step - loss: 0.0919 - accuracy: 0.9702 - val_loss: 0.9720 - val_accuracy: 0.8444\n",
    "Epoch 323/500\n",
    "3/3 [==============================] - 2s 482ms/step - loss: 0.1243 - accuracy: 0.9643 - val_loss: 0.9249 - val_accuracy: 0.8444\n",
    "Epoch 324/500\n",
    "3/3 [==============================] - 2s 594ms/step - loss: 0.0531 - accuracy: 0.9762 - val_loss: 0.9355 - val_accuracy: 0.8444\n",
    "Epoch 325/500\n",
    "3/3 [==============================] - 2s 444ms/step - loss: 0.0543 - accuracy: 0.9762 - val_loss: 0.9080 - val_accuracy: 0.8444\n",
    "Epoch 326/500\n",
    "3/3 [==============================] - 2s 455ms/step - loss: 0.1103 - accuracy: 0.9583 - val_loss: 0.8483 - val_accuracy: 0.8444\n",
    "Epoch 327/500\n",
    "3/3 [==============================] - 2s 468ms/step - loss: 0.1040 - accuracy: 0.9464 - val_loss: 0.8045 - val_accuracy: 0.8667\n",
    "Epoch 328/500\n",
    "3/3 [==============================] - 2s 474ms/step - loss: 0.0466 - accuracy: 0.9881 - val_loss: 0.7826 - val_accuracy: 0.8667\n",
    "Epoch 329/500\n",
    "3/3 [==============================] - 2s 463ms/step - loss: 0.0728 - accuracy: 0.9702 - val_loss: 0.7934 - val_accuracy: 0.8667\n",
    "Epoch 330/500\n",
    "3/3 [==============================] - 2s 596ms/step - loss: 0.1489 - accuracy: 0.9524 - val_loss: 0.8357 - val_accuracy: 0.8667\n",
    "Epoch 331/500\n",
    "3/3 [==============================] - 2s 446ms/step - loss: 0.0744 - accuracy: 0.9762 - val_loss: 0.8488 - val_accuracy: 0.8667\n",
    "Epoch 332/500\n",
    "3/3 [==============================] - 2s 453ms/step - loss: 0.1215 - accuracy: 0.9405 - val_loss: 0.8942 - val_accuracy: 0.8889\n",
    "Epoch 333/500\n",
    "3/3 [==============================] - 2s 472ms/step - loss: 0.0425 - accuracy: 0.9881 - val_loss: 0.9290 - val_accuracy: 0.8889\n",
    "Epoch 334/500\n",
    "3/3 [==============================] - 2s 476ms/step - loss: 0.0772 - accuracy: 0.9702 - val_loss: 0.9653 - val_accuracy: 0.8889\n",
    "Epoch 335/500\n",
    "3/3 [==============================] - 2s 591ms/step - loss: 0.0888 - accuracy: 0.9643 - val_loss: 0.9688 - val_accuracy: 0.8889\n",
    "Epoch 336/500\n",
    "3/3 [==============================] - 2s 591ms/step - loss: 0.0935 - accuracy: 0.9643 - val_loss: 0.9388 - val_accuracy: 0.8667\n",
    "Epoch 337/500\n",
    "3/3 [==============================] - 2s 593ms/step - loss: 0.0978 - accuracy: 0.9583 - val_loss: 0.9266 - val_accuracy: 0.8667\n",
    "Epoch 338/500\n",
    "3/3 [==============================] - 2s 595ms/step - loss: 0.0881 - accuracy: 0.9643 - val_loss: 0.8687 - val_accuracy: 0.8667\n",
    "Epoch 339/500\n",
    "3/3 [==============================] - 2s 470ms/step - loss: 0.1190 - accuracy: 0.9524 - val_loss: 0.8826 - val_accuracy: 0.8667\n",
    "Epoch 340/500\n",
    "3/3 [==============================] - 2s 596ms/step - loss: 0.1064 - accuracy: 0.9643 - val_loss: 0.8654 - val_accuracy: 0.8667\n",
    "Epoch 341/500\n",
    "3/3 [==============================] - 2s 449ms/step - loss: 0.1064 - accuracy: 0.9583 - val_loss: 0.8315 - val_accuracy: 0.8667\n",
    "Epoch 342/500\n",
    "3/3 [==============================] - 2s 472ms/step - loss: 0.0690 - accuracy: 0.9762 - val_loss: 0.8303 - val_accuracy: 0.8667\n",
    "Epoch 343/500\n",
    "3/3 [==============================] - 2s 595ms/step - loss: 0.0804 - accuracy: 0.9643 - val_loss: 0.7811 - val_accuracy: 0.8667\n",
    "Epoch 344/500\n",
    "3/3 [==============================] - 2s 472ms/step - loss: 0.0612 - accuracy: 0.9821 - val_loss: 0.7469 - val_accuracy: 0.8667\n",
    "Epoch 345/500\n",
    "3/3 [==============================] - 2s 472ms/step - loss: 0.1172 - accuracy: 0.9583 - val_loss: 0.7301 - val_accuracy: 0.8667\n",
    "Epoch 346/500\n",
    "3/3 [==============================] - 2s 463ms/step - loss: 0.0695 - accuracy: 0.9702 - val_loss: 0.6810 - val_accuracy: 0.8889\n",
    "Epoch 347/500\n",
    "3/3 [==============================] - 2s 451ms/step - loss: 0.0881 - accuracy: 0.9643 - val_loss: 0.6682 - val_accuracy: 0.8889\n",
    "Epoch 348/500\n",
    "3/3 [==============================] - 2s 594ms/step - loss: 0.0943 - accuracy: 0.9762 - val_loss: 0.6775 - val_accuracy: 0.9111\n",
    "Epoch 349/500\n",
    "3/3 [==============================] - 2s 470ms/step - loss: 0.0591 - accuracy: 0.9821 - val_loss: 0.7225 - val_accuracy: 0.8889\n",
    "Epoch 350/500\n",
    "3/3 [==============================] - 2s 468ms/step - loss: 0.1663 - accuracy: 0.9345 - val_loss: 0.7771 - val_accuracy: 0.8667\n",
    "Epoch 351/500\n",
    "3/3 [==============================] - 2s 595ms/step - loss: 0.0797 - accuracy: 0.9762 - val_loss: 0.8275 - val_accuracy: 0.8667\n",
    "Epoch 352/500\n",
    "3/3 [==============================] - 2s 449ms/step - loss: 0.0601 - accuracy: 0.9702 - val_loss: 0.8665 - val_accuracy: 0.8889\n",
    "Epoch 353/500\n",
    "3/3 [==============================] - 2s 589ms/step - loss: 0.0806 - accuracy: 0.9643 - val_loss: 0.8675 - val_accuracy: 0.8667\n",
    "Epoch 354/500\n",
    "3/3 [==============================] - 2s 448ms/step - loss: 0.0503 - accuracy: 0.9762 - val_loss: 0.8372 - val_accuracy: 0.8667\n",
    "Epoch 355/500\n",
    "3/3 [==============================] - 2s 448ms/step - loss: 0.0638 - accuracy: 0.9702 - val_loss: 0.8244 - val_accuracy: 0.8667\n",
    "Epoch 356/500\n",
    "3/3 [==============================] - 2s 450ms/step - loss: 0.0688 - accuracy: 0.9702 - val_loss: 0.8522 - val_accuracy: 0.8667\n",
    "Epoch 357/500\n",
    "3/3 [==============================] - 2s 452ms/step - loss: 0.1087 - accuracy: 0.9643 - val_loss: 0.9296 - val_accuracy: 0.8667\n",
    "Epoch 358/500\n",
    "3/3 [==============================] - 2s 594ms/step - loss: 0.1094 - accuracy: 0.9524 - val_loss: 0.9505 - val_accuracy: 0.8444\n",
    "Epoch 359/500\n",
    "3/3 [==============================] - 2s 452ms/step - loss: 0.0675 - accuracy: 0.9821 - val_loss: 1.0011 - val_accuracy: 0.8444\n",
    "Epoch 360/500\n",
    "3/3 [==============================] - 2s 475ms/step - loss: 0.0706 - accuracy: 0.9702 - val_loss: 0.9976 - val_accuracy: 0.8444\n",
    "Epoch 361/500\n",
    "3/3 [==============================] - 2s 455ms/step - loss: 0.1054 - accuracy: 0.9583 - val_loss: 0.8937 - val_accuracy: 0.8444\n",
    "Epoch 362/500\n",
    "3/3 [==============================] - 2s 599ms/step - loss: 0.0636 - accuracy: 0.9881 - val_loss: 0.8383 - val_accuracy: 0.8444\n",
    "Epoch 363/500\n",
    "3/3 [==============================] - 2s 470ms/step - loss: 0.1411 - accuracy: 0.9524 - val_loss: 0.8760 - val_accuracy: 0.8444\n",
    "Epoch 364/500\n",
    "3/3 [==============================] - 2s 465ms/step - loss: 0.1242 - accuracy: 0.9345 - val_loss: 0.9433 - val_accuracy: 0.8667\n",
    "Epoch 365/500\n",
    "3/3 [==============================] - 2s 593ms/step - loss: 0.0296 - accuracy: 0.9881 - val_loss: 1.0388 - val_accuracy: 0.8667\n",
    "Epoch 366/500\n",
    "3/3 [==============================] - 2s 470ms/step - loss: 0.0854 - accuracy: 0.9821 - val_loss: 1.0785 - val_accuracy: 0.8667\n",
    "Epoch 367/500\n",
    "3/3 [==============================] - 2s 587ms/step - loss: 0.1812 - accuracy: 0.9464 - val_loss: 1.0947 - val_accuracy: 0.8444\n",
    "Epoch 368/500\n",
    "3/3 [==============================] - 2s 455ms/step - loss: 0.1261 - accuracy: 0.9524 - val_loss: 1.0318 - val_accuracy: 0.8444\n",
    "Epoch 369/500\n",
    "3/3 [==============================] - 2s 475ms/step - loss: 0.1135 - accuracy: 0.9583 - val_loss: 0.9962 - val_accuracy: 0.8889\n",
    "Epoch 370/500\n",
    "3/3 [==============================] - 2s 448ms/step - loss: 0.1036 - accuracy: 0.9583 - val_loss: 0.9697 - val_accuracy: 0.8889\n",
    "Epoch 371/500\n",
    "3/3 [==============================] - 2s 588ms/step - loss: 0.0614 - accuracy: 0.9643 - val_loss: 0.9338 - val_accuracy: 0.9111\n",
    "Epoch 372/500\n",
    "3/3 [==============================] - 2s 463ms/step - loss: 0.1233 - accuracy: 0.9524 - val_loss: 0.9434 - val_accuracy: 0.8889\n",
    "Epoch 373/500\n",
    "3/3 [==============================] - 2s 473ms/step - loss: 0.0518 - accuracy: 0.9702 - val_loss: 0.9898 - val_accuracy: 0.8889\n",
    "Epoch 374/500\n",
    "3/3 [==============================] - 2s 455ms/step - loss: 0.0699 - accuracy: 0.9643 - val_loss: 1.0494 - val_accuracy: 0.8889\n",
    "Epoch 375/500\n",
    "3/3 [==============================] - 2s 593ms/step - loss: 0.1067 - accuracy: 0.9583 - val_loss: 0.9876 - val_accuracy: 0.8889\n",
    "Epoch 376/500\n",
    "3/3 [==============================] - 2s 453ms/step - loss: 0.1079 - accuracy: 0.9702 - val_loss: 0.9444 - val_accuracy: 0.9111\n",
    "Epoch 377/500\n",
    "\n",
    "3/3 [==============================] - 2s 450ms/step - loss: 0.1837 - accuracy: 0.9583 - val_loss: 0.9481 - val_accuracy: 0.9111\n",
    "Epoch 378/500\n",
    "3/3 [==============================] - 2s 468ms/step - loss: 0.0594 - accuracy: 0.9881 - val_loss: 0.9453 - val_accuracy: 0.9111\n",
    "Epoch 379/500\n",
    "3/3 [==============================] - 2s 592ms/step - loss: 0.1034 - accuracy: 0.9583 - val_loss: 0.9490 - val_accuracy: 0.8889\n",
    "Epoch 380/500\n",
    "3/3 [==============================] - 2s 472ms/step - loss: 0.0717 - accuracy: 0.9643 - val_loss: 0.9464 - val_accuracy: 0.8889\n",
    "Epoch 381/500\n",
    "3/3 [==============================] - 2s 449ms/step - loss: 0.0872 - accuracy: 0.9524 - val_loss: 0.9576 - val_accuracy: 0.8889\n",
    "Epoch 382/500\n",
    "3/3 [==============================] - 2s 471ms/step - loss: 0.0915 - accuracy: 0.9583 - val_loss: 1.0466 - val_accuracy: 0.8889\n",
    "Epoch 383/500\n",
    "3/3 [==============================] - 2s 471ms/step - loss: 0.0966 - accuracy: 0.9762 - val_loss: 1.1763 - val_accuracy: 0.8444\n",
    "Epoch 384/500\n",
    "3/3 [==============================] - 2s 587ms/step - loss: 0.0750 - accuracy: 0.9702 - val_loss: 1.1688 - val_accuracy: 0.8444\n",
    "Epoch 385/500\n",
    "3/3 [==============================] - 2s 476ms/step - loss: 0.1351 - accuracy: 0.9583 - val_loss: 1.1220 - val_accuracy: 0.8444\n",
    "Epoch 386/500\n",
    "3/3 [==============================] - 2s 472ms/step - loss: 0.1095 - accuracy: 0.9524 - val_loss: 1.1876 - val_accuracy: 0.8444\n",
    "Epoch 387/500\n",
    "3/3 [==============================] - 2s 472ms/step - loss: 0.1763 - accuracy: 0.9583 - val_loss: 1.2756 - val_accuracy: 0.8444\n",
    "Epoch 388/500\n",
    "3/3 [==============================] - 2s 594ms/step - loss: 0.0926 - accuracy: 0.9583 - val_loss: 1.3245 - val_accuracy: 0.8444\n",
    "Epoch 389/500\n",
    "3/3 [==============================] - 2s 466ms/step - loss: 0.1021 - accuracy: 0.9464 - val_loss: 1.3628 - val_accuracy: 0.8444\n",
    "Epoch 390/500\n",
    "3/3 [==============================] - 2s 471ms/step - loss: 0.1141 - accuracy: 0.9524 - val_loss: 1.3865 - val_accuracy: 0.8444\n",
    "Epoch 391/500\n",
    "3/3 [==============================] - 2s 468ms/step - loss: 0.0775 - accuracy: 0.9702 - val_loss: 1.3832 - val_accuracy: 0.8444\n",
    "Epoch 392/500\n",
    "3/3 [==============================] - 2s 595ms/step - loss: 0.1188 - accuracy: 0.9702 - val_loss: 1.3108 - val_accuracy: 0.8444\n",
    "Epoch 393/500\n",
    "3/3 [==============================] - 2s 449ms/step - loss: 0.0846 - accuracy: 0.9643 - val_loss: 1.2579 - val_accuracy: 0.8667\n",
    "Epoch 394/500\n",
    "3/3 [==============================] - 2s 464ms/step - loss: 0.0908 - accuracy: 0.9702 - val_loss: 1.1709 - val_accuracy: 0.8667\n",
    "Epoch 395/500\n",
    "3/3 [==============================] - 2s 592ms/step - loss: 0.1091 - accuracy: 0.9583 - val_loss: 1.0384 - val_accuracy: 0.8667\n",
    "Epoch 396/500\n",
    "3/3 [==============================] - 2s 469ms/step - loss: 0.1168 - accuracy: 0.9702 - val_loss: 0.9563 - val_accuracy: 0.8667\n",
    "Epoch 397/500\n",
    "3/3 [==============================] - 2s 595ms/step - loss: 0.1688 - accuracy: 0.9464 - val_loss: 0.8863 - val_accuracy: 0.8667\n",
    "Epoch 398/500\n",
    "3/3 [==============================] - 2s 478ms/step - loss: 0.0630 - accuracy: 0.9702 - val_loss: 0.8708 - val_accuracy: 0.8667\n",
    "Epoch 399/500\n",
    "3/3 [==============================] - 2s 469ms/step - loss: 0.1102 - accuracy: 0.9702 - val_loss: 0.9167 - val_accuracy: 0.8667\n",
    "Epoch 400/500\n",
    "3/3 [==============================] - 2s 585ms/step - loss: 0.0816 - accuracy: 0.9643 - val_loss: 0.9632 - val_accuracy: 0.8667\n",
    "Epoch 401/500\n",
    "3/3 [==============================] - 2s 595ms/step - loss: 0.0878 - accuracy: 0.9702 - val_loss: 1.0403 - val_accuracy: 0.8667\n",
    "Epoch 402/500\n",
    "3/3 [==============================] - 2s 587ms/step - loss: 0.1059 - accuracy: 0.9762 - val_loss: 1.1011 - val_accuracy: 0.8667\n",
    "Epoch 403/500\n",
    "3/3 [==============================] - 2s 452ms/step - loss: 0.0781 - accuracy: 0.9702 - val_loss: 1.1719 - val_accuracy: 0.8667\n",
    "Epoch 404/500\n",
    "3/3 [==============================] - 2s 587ms/step - loss: 0.1224 - accuracy: 0.9345 - val_loss: 1.2238 - val_accuracy: 0.8444\n",
    "Epoch 405/500\n",
    "3/3 [==============================] - 2s 469ms/step - loss: 0.1348 - accuracy: 0.9524 - val_loss: 1.2643 - val_accuracy: 0.8444\n",
    "Epoch 406/500\n",
    "3/3 [==============================] - 2s 593ms/step - loss: 0.0623 - accuracy: 0.9702 - val_loss: 1.2413 - val_accuracy: 0.8444\n",
    "Epoch 407/500\n",
    "3/3 [==============================] - 2s 449ms/step - loss: 0.0701 - accuracy: 0.9702 - val_loss: 1.1978 - val_accuracy: 0.8444\n",
    "Epoch 408/500\n",
    "3/3 [==============================] - 2s 450ms/step - loss: 0.1005 - accuracy: 0.9762 - val_loss: 1.1409 - val_accuracy: 0.8667\n",
    "Epoch 409/500\n",
    "3/3 [==============================] - 2s 468ms/step - loss: 0.0635 - accuracy: 0.9762 - val_loss: 1.0263 - val_accuracy: 0.8667\n",
    "Epoch 410/500\n",
    "3/3 [==============================] - 2s 591ms/step - loss: 0.1232 - accuracy: 0.9583 - val_loss: 0.8834 - val_accuracy: 0.8667\n",
    "Epoch 411/500\n",
    "3/3 [==============================] - 2s 464ms/step - loss: 0.0754 - accuracy: 0.9702 - val_loss: 0.8014 - val_accuracy: 0.8667\n",
    "Epoch 412/500\n",
    "3/3 [==============================] - 2s 450ms/step - loss: 0.1055 - accuracy: 0.9702 - val_loss: 0.7589 - val_accuracy: 0.8667\n",
    "Epoch 413/500\n",
    "3/3 [==============================] - 2s 467ms/step - loss: 0.1065 - accuracy: 0.9702 - val_loss: 0.7669 - val_accuracy: 0.8444\n",
    "Epoch 414/500\n",
    "3/3 [==============================] - 2s 444ms/step - loss: 0.0641 - accuracy: 0.9940 - val_loss: 0.7955 - val_accuracy: 0.8222\n",
    "Epoch 415/500\n",
    "3/3 [==============================] - 2s 476ms/step - loss: 0.0846 - accuracy: 0.9702 - val_loss: 0.8361 - val_accuracy: 0.8222\n",
    "Epoch 416/500\n",
    "3/3 [==============================] - 2s 598ms/step - loss: 0.1249 - accuracy: 0.9583 - val_loss: 0.8292 - val_accuracy: 0.8444\n",
    "Epoch 417/500\n",
    "3/3 [==============================] - 2s 589ms/step - loss: 0.1392 - accuracy: 0.9524 - val_loss: 0.9069 - val_accuracy: 0.8444\n",
    "Epoch 418/500\n",
    "3/3 [==============================] - 2s 596ms/step - loss: 0.0834 - accuracy: 0.9762 - val_loss: 1.0184 - val_accuracy: 0.8444\n",
    "Epoch 419/500\n",
    "3/3 [==============================] - 2s 590ms/step - loss: 0.0993 - accuracy: 0.9464 - val_loss: 1.0476 - val_accuracy: 0.8667\n",
    "Epoch 420/500\n",
    "3/3 [==============================] - 2s 451ms/step - loss: 0.0704 - accuracy: 0.9702 - val_loss: 1.0064 - val_accuracy: 0.8667\n",
    "Epoch 421/500\n",
    "3/3 [==============================] - 2s 584ms/step - loss: 0.1140 - accuracy: 0.9643 - val_loss: 0.9469 - val_accuracy: 0.8667\n",
    "Epoch 422/500\n",
    "3/3 [==============================] - 2s 597ms/step - loss: 0.0797 - accuracy: 0.9643 - val_loss: 0.7762 - val_accuracy: 0.8667\n",
    "Epoch 423/500\n",
    "3/3 [==============================] - 2s 446ms/step - loss: 0.0925 - accuracy: 0.9643 - val_loss: 0.6802 - val_accuracy: 0.8667\n",
    "Epoch 424/500\n",
    "3/3 [==============================] - 2s 466ms/step - loss: 0.0749 - accuracy: 0.9762 - val_loss: 0.6515 - val_accuracy: 0.8667\n",
    "Epoch 425/500\n",
    "3/3 [==============================] - 2s 471ms/step - loss: 0.0682 - accuracy: 0.9821 - val_loss: 0.6463 - val_accuracy: 0.8667\n",
    "Epoch 426/500\n",
    "3/3 [==============================] - 2s 472ms/step - loss: 0.0272 - accuracy: 0.9881 - val_loss: 0.6666 - val_accuracy: 0.8667\n",
    "Epoch 427/500\n",
    "3/3 [==============================] - 2s 469ms/step - loss: 0.0862 - accuracy: 0.9762 - val_loss: 0.7015 - val_accuracy: 0.8889\n",
    "Epoch 428/500\n",
    "3/3 [==============================] - 2s 462ms/step - loss: 0.0450 - accuracy: 0.9881 - val_loss: 0.7355 - val_accuracy: 0.9111\n",
    "Epoch 429/500\n",
    "3/3 [==============================] - 2s 592ms/step - loss: 0.0470 - accuracy: 0.9702 - val_loss: 0.7727 - val_accuracy: 0.8889\n",
    "Epoch 430/500\n",
    "3/3 [==============================] - 2s 470ms/step - loss: 0.0898 - accuracy: 0.9821 - val_loss: 0.8304 - val_accuracy: 0.8889\n",
    "Epoch 431/500\n",
    "3/3 [==============================] - 2s 595ms/step - loss: 0.0261 - accuracy: 0.9940 - val_loss: 0.8238 - val_accuracy: 0.8889\n",
    "Epoch 432/500\n",
    "3/3 [==============================] - 2s 596ms/step - loss: 0.0565 - accuracy: 0.9821 - val_loss: 0.8358 - val_accuracy: 0.8889\n",
    "Epoch 433/500\n",
    "3/3 [==============================] - 2s 454ms/step - loss: 0.0382 - accuracy: 0.9940 - val_loss: 0.8643 - val_accuracy: 0.8889\n",
    "Epoch 434/500\n",
    "\n",
    "3/3 [==============================] - 2s 455ms/step - loss: 0.0971 - accuracy: 0.9583 - val_loss: 0.8418 - val_accuracy: 0.8889\n",
    "Epoch 435/500\n",
    "3/3 [==============================] - 2s 447ms/step - loss: 0.0493 - accuracy: 0.9821 - val_loss: 0.7337 - val_accuracy: 0.9111\n",
    "Epoch 436/500\n",
    "3/3 [==============================] - 2s 452ms/step - loss: 0.1102 - accuracy: 0.9464 - val_loss: 0.6906 - val_accuracy: 0.8889\n",
    "Epoch 437/500\n",
    "3/3 [==============================] - 2s 454ms/step - loss: 0.1077 - accuracy: 0.9583 - val_loss: 0.6855 - val_accuracy: 0.8444\n",
    "Epoch 438/500\n",
    "3/3 [==============================] - 2s 594ms/step - loss: 0.1111 - accuracy: 0.9643 - val_loss: 0.6673 - val_accuracy: 0.8667\n",
    "Epoch 439/500\n",
    "3/3 [==============================] - 2s 450ms/step - loss: 0.0793 - accuracy: 0.9643 - val_loss: 0.6474 - val_accuracy: 0.8889\n",
    "Epoch 440/500\n",
    "3/3 [==============================] - 2s 589ms/step - loss: 0.0968 - accuracy: 0.9643 - val_loss: 0.6430 - val_accuracy: 0.9333\n",
    "Epoch 441/500\n",
    "3/3 [==============================] - 2s 592ms/step - loss: 0.0517 - accuracy: 0.9762 - val_loss: 0.6589 - val_accuracy: 0.9111\n",
    "Epoch 442/500\n",
    "3/3 [==============================] - 2s 446ms/step - loss: 0.0923 - accuracy: 0.9583 - val_loss: 0.6413 - val_accuracy: 0.9111\n",
    "Epoch 443/500\n",
    "3/3 [==============================] - 2s 469ms/step - loss: 0.0427 - accuracy: 0.9821 - val_loss: 0.6613 - val_accuracy: 0.8889\n",
    "Epoch 444/500\n",
    "3/3 [==============================] - 2s 464ms/step - loss: 0.0821 - accuracy: 0.9821 - val_loss: 0.7099 - val_accuracy: 0.8889\n",
    "Epoch 445/500\n",
    "3/3 [==============================] - 2s 467ms/step - loss: 0.0517 - accuracy: 0.9881 - val_loss: 0.7567 - val_accuracy: 0.8889\n",
    "Epoch 446/500\n",
    "3/3 [==============================] - 2s 471ms/step - loss: 0.1006 - accuracy: 0.9702 - val_loss: 0.7948 - val_accuracy: 0.8889\n",
    "Epoch 447/500\n",
    "3/3 [==============================] - 2s 447ms/step - loss: 0.0251 - accuracy: 0.9881 - val_loss: 0.8479 - val_accuracy: 0.8667\n",
    "Epoch 448/500\n",
    "3/3 [==============================] - 2s 469ms/step - loss: 0.0421 - accuracy: 0.9821 - val_loss: 0.9141 - val_accuracy: 0.8444\n",
    "Epoch 449/500\n",
    "3/3 [==============================] - 2s 593ms/step - loss: 0.0734 - accuracy: 0.9762 - val_loss: 0.9536 - val_accuracy: 0.8444\n",
    "Epoch 450/500\n",
    "3/3 [==============================] - 2s 602ms/step - loss: 0.0443 - accuracy: 0.9821 - val_loss: 0.9509 - val_accuracy: 0.8444\n",
    "Epoch 451/500\n",
    "3/3 [==============================] - 2s 593ms/step - loss: 0.0444 - accuracy: 0.9881 - val_loss: 0.9154 - val_accuracy: 0.8222\n",
    "Epoch 452/500\n",
    "3/3 [==============================] - 2s 589ms/step - loss: 0.0371 - accuracy: 0.9940 - val_loss: 0.8943 - val_accuracy: 0.8667\n",
    "Epoch 453/500\n",
    "3/3 [==============================] - 2s 456ms/step - loss: 0.0685 - accuracy: 0.9762 - val_loss: 0.9089 - val_accuracy: 0.8444\n",
    "Epoch 454/500\n",
    "3/3 [==============================] - 2s 446ms/step - loss: 0.0577 - accuracy: 0.9643 - val_loss: 0.9492 - val_accuracy: 0.8222\n",
    "Epoch 455/500\n",
    "3/3 [==============================] - 2s 470ms/step - loss: 0.0821 - accuracy: 0.9643 - val_loss: 0.9223 - val_accuracy: 0.8222\n",
    "Epoch 456/500\n",
    "3/3 [==============================] - 2s 471ms/step - loss: 0.0645 - accuracy: 0.9881 - val_loss: 0.9057 - val_accuracy: 0.8889\n",
    "Epoch 457/500\n",
    "3/3 [==============================] - 2s 452ms/step - loss: 0.0953 - accuracy: 0.9643 - val_loss: 0.9149 - val_accuracy: 0.8667\n",
    "Epoch 458/500\n",
    "3/3 [==============================] - 2s 590ms/step - loss: 0.0099 - accuracy: 1.0000 - val_loss: 0.9523 - val_accuracy: 0.8667\n",
    "Epoch 459/500\n",
    "3/3 [==============================] - 2s 449ms/step - loss: 0.0385 - accuracy: 0.9762 - val_loss: 0.9888 - val_accuracy: 0.8667\n",
    "Epoch 460/500\n",
    "3/3 [==============================] - 2s 593ms/step - loss: 0.0635 - accuracy: 0.9702 - val_loss: 0.8917 - val_accuracy: 0.8667\n",
    "Epoch 461/500\n",
    "3/3 [==============================] - 2s 452ms/step - loss: 0.0450 - accuracy: 0.9702 - val_loss: 0.8522 - val_accuracy: 0.8667\n",
    "Epoch 462/500\n",
    "3/3 [==============================] - 2s 593ms/step - loss: 0.0642 - accuracy: 0.9702 - val_loss: 0.8535 - val_accuracy: 0.8667\n",
    "Epoch 463/500\n",
    "3/3 [==============================] - 2s 453ms/step - loss: 0.1121 - accuracy: 0.9643 - val_loss: 0.8481 - val_accuracy: 0.8667\n",
    "Epoch 464/500\n",
    "3/3 [==============================] - 2s 451ms/step - loss: 0.0399 - accuracy: 0.9821 - val_loss: 0.8634 - val_accuracy: 0.8667\n",
    "Epoch 465/500\n",
    "3/3 [==============================] - 2s 450ms/step - loss: 0.0642 - accuracy: 0.9583 - val_loss: 0.9143 - val_accuracy: 0.8667\n",
    "Epoch 466/500\n",
    "3/3 [==============================] - 2s 599ms/step - loss: 0.0284 - accuracy: 0.9821 - val_loss: 0.9437 - val_accuracy: 0.8667\n",
    "Epoch 467/500\n",
    "3/3 [==============================] - 2s 462ms/step - loss: 0.0878 - accuracy: 0.9821 - val_loss: 0.9435 - val_accuracy: 0.8667\n",
    "Epoch 468/500\n",
    "3/3 [==============================] - 2s 474ms/step - loss: 0.0419 - accuracy: 0.9881 - val_loss: 0.9162 - val_accuracy: 0.8889\n",
    "Epoch 469/500\n",
    "3/3 [==============================] - 2s 454ms/step - loss: 0.0635 - accuracy: 0.9702 - val_loss: 0.8752 - val_accuracy: 0.8889\n",
    "Epoch 470/500\n",
    "3/3 [==============================] - 2s 587ms/step - loss: 0.0229 - accuracy: 0.9940 - val_loss: 0.8489 - val_accuracy: 0.9111\n",
    "Epoch 471/500\n",
    "3/3 [==============================] - 2s 466ms/step - loss: 0.0883 - accuracy: 0.9702 - val_loss: 0.8432 - val_accuracy: 0.8889\n",
    "Epoch 472/500\n",
    "3/3 [==============================] - 2s 469ms/step - loss: 0.0604 - accuracy: 0.9821 - val_loss: 0.8622 - val_accuracy: 0.8889\n",
    "Epoch 473/500\n",
    "3/3 [==============================] - 2s 586ms/step - loss: 0.0774 - accuracy: 0.9702 - val_loss: 0.9078 - val_accuracy: 0.8889\n",
    "Epoch 474/500\n",
    "3/3 [==============================] - 2s 467ms/step - loss: 0.1163 - accuracy: 0.9524 - val_loss: 0.9798 - val_accuracy: 0.8889\n",
    "Epoch 475/500\n",
    "3/3 [==============================] - 2s 469ms/step - loss: 0.0848 - accuracy: 0.9702 - val_loss: 1.0784 - val_accuracy: 0.8889\n",
    "Epoch 476/500\n",
    "3/3 [==============================] - 2s 467ms/step - loss: 0.1114 - accuracy: 0.9583 - val_loss: 1.1702 - val_accuracy: 0.8889\n",
    "Epoch 477/500\n",
    "3/3 [==============================] - 2s 594ms/step - loss: 0.0723 - accuracy: 0.9762 - val_loss: 1.2394 - val_accuracy: 0.8889\n",
    "Epoch 478/500\n",
    "3/3 [==============================] - 2s 455ms/step - loss: 0.0952 - accuracy: 0.9702 - val_loss: 1.2587 - val_accuracy: 0.8667\n",
    "Epoch 479/500\n",
    "3/3 [==============================] - 2s 469ms/step - loss: 0.0867 - accuracy: 0.9821 - val_loss: 1.2300 - val_accuracy: 0.8444\n",
    "Epoch 480/500\n",
    "3/3 [==============================] - 2s 447ms/step - loss: 0.1867 - accuracy: 0.9345 - val_loss: 1.2759 - val_accuracy: 0.8000\n",
    "Epoch 481/500\n",
    "3/3 [==============================] - 2s 472ms/step - loss: 0.0352 - accuracy: 0.9821 - val_loss: 1.4065 - val_accuracy: 0.7778\n",
    "Epoch 482/500\n",
    "3/3 [==============================] - 2s 478ms/step - loss: 0.2134 - accuracy: 0.9405 - val_loss: 1.6354 - val_accuracy: 0.7556\n",
    "Epoch 483/500\n",
    "3/3 [==============================] - 2s 449ms/step - loss: 0.0918 - accuracy: 0.9762 - val_loss: 1.7308 - val_accuracy: 0.7556\n",
    "Epoch 484/500\n",
    "3/3 [==============================] - 2s 588ms/step - loss: 0.0652 - accuracy: 0.9762 - val_loss: 1.7040 - val_accuracy: 0.8000\n",
    "Epoch 485/500\n",
    "3/3 [==============================] - 2s 475ms/step - loss: 0.0977 - accuracy: 0.9643 - val_loss: 1.5987 - val_accuracy: 0.8222\n",
    "Epoch 486/500\n",
    "3/3 [==============================] - 2s 590ms/step - loss: 0.1058 - accuracy: 0.9583 - val_loss: 1.4423 - val_accuracy: 0.8444\n",
    "Epoch 487/500\n",
    "3/3 [==============================] - 2s 470ms/step - loss: 0.1990 - accuracy: 0.9405 - val_loss: 1.3551 - val_accuracy: 0.8444\n",
    "Epoch 488/500\n",
    "3/3 [==============================] - 2s 444ms/step - loss: 0.1148 - accuracy: 0.9702 - val_loss: 1.2828 - val_accuracy: 0.8667\n",
    "Epoch 489/500\n",
    "3/3 [==============================] - 2s 453ms/step - loss: 0.1806 - accuracy: 0.9405 - val_loss: 1.2139 - val_accuracy: 0.8667\n",
    "Epoch 490/500\n",
    "3/3 [==============================] - 2s 591ms/step - loss: 0.1699 - accuracy: 0.9583 - val_loss: 1.1822 - val_accuracy: 0.8667\n",
    "Epoch 491/500\n",
    "\n",
    "3/3 [==============================] - 2s 446ms/step - loss: 0.0946 - accuracy: 0.9702 - val_loss: 1.2115 - val_accuracy: 0.8667\n",
    "Epoch 492/500\n",
    "3/3 [==============================] - 2s 588ms/step - loss: 0.0967 - accuracy: 0.9762 - val_loss: 1.3498 - val_accuracy: 0.8667\n",
    "Epoch 493/500\n",
    "3/3 [==============================] - 2s 594ms/step - loss: 0.0902 - accuracy: 0.9643 - val_loss: 1.5661 - val_accuracy: 0.8000\n",
    "Epoch 494/500\n",
    "3/3 [==============================] - 2s 593ms/step - loss: 0.0664 - accuracy: 0.9762 - val_loss: 1.6130 - val_accuracy: 0.7778\n",
    "Epoch 495/500\n",
    "3/3 [==============================] - 2s 603ms/step - loss: 0.0310 - accuracy: 0.9881 - val_loss: 1.6383 - val_accuracy: 0.7778\n",
    "Epoch 496/500\n",
    "3/3 [==============================] - 2s 588ms/step - loss: 0.0363 - accuracy: 0.9881 - val_loss: 1.6326 - val_accuracy: 0.7556\n",
    "Epoch 497/500\n",
    "3/3 [==============================] - 2s 446ms/step - loss: 0.0716 - accuracy: 0.9762 - val_loss: 1.5846 - val_accuracy: 0.7778\n",
    "Epoch 498/500\n",
    "3/3 [==============================] - 2s 452ms/step - loss: 0.0627 - accuracy: 0.9702 - val_loss: 1.5074 - val_accuracy: 0.8222\n",
    "Epoch 499/500\n",
    "3/3 [==============================] - 2s 475ms/step - loss: 0.1033 - accuracy: 0.9643 - val_loss: 1.4160 - val_accuracy: 0.8444\n",
    "Epoch 500/500\n",
    "3/3 [==============================] - 2s 595ms/step - loss: 0.0552 - accuracy: 0.9881 - val_loss: 1.3293 - val_accuracy: 0.8667\n",
    "{'loss': [2.902595043182373, 2.380047559738159, 2.107966423034668, 2.0494768619537354, 1.9216450452804565, 1.711968183517456, 1.7256284952163696, 1.4973359107971191, 1.5497934818267822, 1.4457032680511475, 1.4289823770523071, 1.4132394790649414, 1.314518928527832, 1.1078962087631226, 1.1476353406906128, 1.0954108238220215, 0.9760926365852356, 0.8932982087135315, 0.7977258563041687, 0.9743236303329468, 0.8896631598472595, 0.8439539074897766, 0.8083810210227966, 0.8400141596794128, 0.7617804408073425, 0.8260048627853394, 0.7024803161621094, 0.785434901714325, 0.7155746221542358, 0.8451884388923645, 0.7252389788627625, 0.6143702268600464, 0.48891153931617737, 0.4864279329776764, 0.5664175152778625, 0.6125801801681519, 0.4650919735431671, 0.46738046407699585, 0.5025293827056885, 0.5155008435249329, 0.4549211859703064, 0.45043763518333435, 0.468644917011261, 0.4108177721500397, 0.3540239930152893, 0.33621275424957275, 0.5230563282966614, 0.37361112236976624, 0.43353861570358276, 0.3604947626590729, 0.27469930052757263, 0.27542340755462646, 0.33759161829948425, 0.3287855386734009, 0.4200270175933838, 0.3620518147945404, 0.33827003836631775, 0.3057444989681244, 0.3792344629764557, 0.40463918447494507, 0.318061888217926, 0.36413460969924927, 0.3593693673610687, 0.27244889736175537, 0.30578821897506714, 0.27865496277809143, 0.3349844217300415, 0.22638337314128876, 0.3341633379459381, 0.23322109878063202, 0.2731897830963135, 0.23992785811424255, 0.39005210995674133, 0.27853864431381226, 0.2544536590576172, 0.40792015194892883, 0.32542189955711365, 0.22268445789813995, 0.3184719979763031, 0.2236345112323761, 0.2555950880050659, 0.3779228925704956, 0.2717750668525696, 0.23628827929496765, 0.2750423550605774, 0.30811771750450134, 0.2604997158050537, 0.23244228959083557, 0.30349573493003845, 0.2519315183162689, 0.2256755828857422, 0.20420034229755402, 0.22634129226207733, 0.2807310223579407, 0.22967961430549622, 0.14045710861682892, 0.334975004196167, 0.14531481266021729, 0.2848396897315979, 0.23272913694381714, 0.1664087176322937, 0.25688377022743225, 0.1405700445175171, 0.2482721209526062, 0.20434501767158508, 0.19615086913108826, 0.17345000803470612, 0.2320145219564438, 0.25485754013061523, 0.21604399383068085, 0.17797666788101196, 0.18958811461925507, 0.1351354420185089, 0.1993168443441391, 0.12477187812328339, 0.20871864259243011, 0.15283101797103882, 0.1502152681350708, 0.2355007380247116, 0.13739846646785736, 0.12839218974113464, 0.18276235461235046, 0.14461401104927063, 0.1825750470161438, 0.1795429289340973, 0.15658438205718994, 0.14147819578647614, 0.1373242437839508, 0.14513321220874786, 0.32100799679756165, 0.15350952744483948, 0.1275063008069992, 0.15920031070709229, 0.21029150485992432, 0.12171398848295212, 0.17944996058940887, 0.1755339503288269, 0.26145586371421814, 0.20059671998023987, 0.1038828119635582, 0.2529541552066803, 0.16079623997211456, 0.15938150882720947, 0.12554772198200226, 0.10871832817792892, 0.17312046885490417, 0.13529399037361145, 0.10761123895645142, 0.1756935566663742, 0.1551067978143692, 0.16501043736934662, 0.135468989610672, 0.15894430875778198, 0.18465487658977509, 0.1830749809741974, 0.19535067677497864, 0.16251105070114136, 0.09040912985801697, 0.0882643461227417, 0.1986498087644577, 0.15855681896209717, 0.13966023921966553, 0.17523185908794403, 0.2403409332036972, 0.20902208983898163, 0.13860662281513214, 0.16789133846759796, 0.18127669394016266, 0.11038728803396225, 0.1646600067615509, 0.13538344204425812, 0.1449170708656311, 0.14694525301456451, 0.09119872003793716, 0.1019584983587265, 0.14456777274608612, 0.14865221083164215, 0.10262524336576462, 0.12532636523246765, 0.11398369818925858, 0.20273077487945557, 0.1217426136136055, 0.07724584639072418, 0.1278633326292038, 0.15968084335327148, 0.17282140254974365, 0.15317323803901672, 0.16677232086658478, 0.14171871542930603, 0.09026411920785904, 0.14296483993530273, 0.12327811866998672, 0.1652984917163849, 0.08971426635980606, 0.10105276107788086, 0.0946672111749649, 0.12153428792953491, 0.15775629878044128, 0.12175991386175156, 0.08988890051841736, 0.15815013647079468, 0.16025276482105255, 0.16423462331295013, 0.17192962765693665, 0.10280568152666092, 0.08681544661521912, 0.10863234102725983, 0.13672535121440887, 0.13871872425079346, 0.11492935568094254, 0.0902252122759819, 0.14064793288707733, 0.07734004408121109, 0.08874767273664474, 0.1796785444021225, 0.17481839656829834, 0.18545813858509064, 0.12075446546077728, 0.17202433943748474, 0.200776144862175, 0.09730260819196701, 0.18449319899082184, 0.09456901252269745, 0.10266131162643433, 0.19766707718372345, 0.08686112612485886, 0.20745745301246643, 0.1617313176393509, 0.1527404934167862, 0.07178131490945816, 0.09813044220209122, 0.12984837591648102, 0.11115750670433044, 0.11854501068592072, 0.21858873963356018, 0.14904212951660156, 0.062228430062532425, 0.12850269675254822, 0.04389835149049759, 0.12844273447990417, 0.22762428224086761, 0.1043323203921318, 0.13370737433433533, 0.09121757745742798, 0.07149582356214523, 0.21919818222522736, 0.11194677650928497, 0.16037465631961823, 0.16088047623634338, 0.22781385481357574, 0.09070020914077759, 0.1316525638103485, 0.14604029059410095, 0.14299938082695007, 0.10507027059793472, 0.1875789910554886, 0.10363301634788513, 0.13785260915756226, 0.11856114119291306, 0.10779111832380295, 0.11289980262517929, 0.03514597564935684, 0.07552669197320938, 0.044061511754989624, 0.11118556559085846, 0.10174684226512909, 0.1021471694111824, 0.0876283347606659, 0.07968330383300781, 0.1136135458946228, 0.10800246894359589, 0.06895789504051208, 0.13352099061012268, 0.09881353378295898, 0.08840566128492355, 0.10648460686206818, 0.12358526885509491, 0.1422049105167389, 0.09911757707595825, 0.07720092684030533, 0.12170886993408203, 0.06659657508134842, 0.1241106390953064, 0.12877078354358673, 0.04038798809051514, 0.07736644148826599, 0.10567834228277206, 0.05552935600280762, 0.0876539871096611, 0.08095625787973404, 0.07011550664901733, 0.08271343261003494, 0.1228971853852272, 0.09044213593006134, 0.12124785035848618, 0.11866658180952072, 0.08253570646047592, 0.12538155913352966, 0.058026671409606934, 0.049263518303632736, 0.13597114384174347, 0.08323436975479126, 0.18803855776786804, 0.0742189809679985, 0.07331305742263794, 0.06397735327482224, 0.10836303979158401, 0.11867304891347885, 0.09327305853366852, 0.1396011859178543, 0.08428007364273071, 0.0635022521018982, 0.11138202995061874, 0.05363868549466133, 0.05414316803216934, 0.0679558739066124, 0.07818147540092468, 0.08692687004804611, 0.12463275343179703, 0.10476680099964142, 0.121735118329525, 0.09188872575759888, 0.12425672262907028, 0.05311823636293411, 0.05433734878897667, 0.11029592901468277, 0.10395103693008423, 0.046567611396312714, 0.07277213037014008, 0.1489369422197342, 0.07437831908464432, 0.12150009721517563, 0.04245135188102722, 0.0772327333688736, 0.08881810307502747, 0.09354457259178162, 0.09776876121759415, 0.0881485790014267, 0.1190347820520401, 0.10635364800691605, 0.10635989904403687, 0.0690213292837143, 0.080428846180439, 0.06122121959924698, 0.11718743294477463, 0.0694604367017746, 0.08805420994758606, 0.09430068731307983, 0.059084437787532806, 0.1663142293691635, 0.07973349839448929, 0.06008818745613098, 0.08062280714511871, 0.05031673610210419, 0.0638207271695137, 0.06884641945362091, 0.10869496315717697, 0.10935758054256439, 0.06754716485738754, 0.07056177407503128, 0.10540515929460526, 0.06355097889900208, 0.14107100665569305, 0.12417016923427582, 0.029567649587988853, 0.08536170423030853, 0.18115034699440002, 0.12606923282146454, 0.11353055387735367, 0.10359273850917816, 0.06140505522489548, 0.12330228090286255, 0.051765765994787216, 0.06988121569156647, 0.10674840211868286, 0.10789766907691956, 0.18365709483623505, 0.0593680702149868, 0.10340892523527145, 0.07170981913805008, 0.08716265112161636, 0.09152732789516449, 0.09662420302629471, 0.0750073716044426, 0.13513140380382538, 0.10946657508611679, 0.17633377015590668, 0.09256545454263687, 0.10213253647089005, 0.1141110509634018, 0.07750219106674194, 0.11883369833230972, 0.08462285250425339, 0.09075156599283218, 0.10908853262662888, 0.11675218492746353, 0.16880512237548828, 0.06296641379594803, 0.1101786270737648, 0.08164125680923462, 0.08783923834562302, 0.105934277176857, 0.07806843519210815, 0.12237431108951569, 0.13479387760162354, 0.06226310133934021, 0.07006777822971344, 0.10046714544296265, 0.063518226146698, 0.1232488751411438, 0.0753709003329277, 0.10548528283834457, 0.10646940767765045, 0.06412938237190247, 0.08459261804819107, 0.12489472329616547, 0.13916443288326263, 0.08337362110614777, 0.09931768476963043, 0.07043498754501343, 0.1140422597527504, 0.0796663910150528, 0.09245780110359192, 0.0748562440276146, 0.06819847971200943, 0.027239760383963585, 0.08623182773590088, 0.04495729133486748, 0.04701361060142517, 0.08975425362586975, 0.0260960403829813, 0.056469567120075226, 0.03818012773990631, 0.0970938578248024, 0.0492648109793663, 0.11015591025352478, 0.10770121216773987, 0.11107828468084335, 0.07933255285024643, 0.09682133048772812, 0.05170765519142151, 0.09229282289743423, 0.04269777983427048, 0.08207845687866211, 0.051651161164045334, 0.10060010850429535, 0.025093061849474907, 0.04208023101091385, 0.07340679317712784, 0.04431937262415886, 0.04444335028529167, 0.03706689551472664, 0.06850225478410721, 0.057663578540086746, 0.08206640928983688, 0.06451369822025299, 0.09532913565635681, 0.00987289845943451, 0.038504280149936676, 0.06346601247787476, 0.044977039098739624, 0.06416606903076172, 0.11209041625261307, 0.03990171477198601, 0.06417063623666763, 0.02843216434121132, 0.08780497312545776, 0.041872378438711166, 0.06347686797380447, 0.022870346903800964, 0.08825117349624634, 0.060443196445703506, 0.07744838297367096, 0.11627767235040665, 0.08479077368974686, 0.11139967292547226, 0.07233954966068268, 0.09519531577825546, 0.08667395263910294, 0.1866738200187683, 0.03519875183701515, 0.21335910260677338, 0.09183970093727112, 0.06517954170703888, 0.09773451834917068, 0.10581211745738983, 0.19900581240653992, 0.11482955515384674, 0.18058599531650543, 0.1699276864528656, 0.09458011388778687, 0.09668979048728943, 0.09016598016023636, 0.06640134751796722, 0.030977748334407806, 0.036312635987997055, 0.07159719616174698, 0.06270299851894379, 0.10334641486406326, 0.05521385371685028], 'accuracy': [0.2202380895614624, 0.2797619104385376, 0.3392857015132904, 0.3511904776096344, 0.3988095223903656, 0.4285714328289032, 0.4345238208770752, 0.4345238208770752, 0.4523809552192688, 0.488095223903656, 0.488095223903656, 0.494047611951828, 0.4821428656578064, 0.5833333134651184, 0.5357142686843872, 0.601190447807312, 0.6547619104385376, 0.6726190447807312, 0.6904761791229248, 0.6666666865348816, 0.648809552192688, 0.6845238208770752, 0.6666666865348816, 0.6666666865348816, 0.7202380895614624, 0.726190447807312, 0.7142857313156128, 0.6964285969734192, 0.7083333134651184, 0.6785714030265808, 0.738095223903656, 0.7797619104385376, 0.8214285969734192, 0.8154761791229248, 0.7916666865348816, 0.8333333134651184, 0.8392857313156128, 0.8035714030265808, 0.8035714030265808, 0.7857142686843872, 0.8273809552192688, 0.8154761791229248, 0.8333333134651184, 0.851190447807312, 0.8809523582458496, 0.8809523582458496, 0.8452380895614624, 0.875, 0.863095223903656, 0.875, 0.886904776096344, 0.9107142686843872, 0.8928571343421936, 0.8809523582458496, 0.8273809552192688, 0.886904776096344, 0.8690476417541504, 0.8690476417541504, 0.8690476417541504, 0.8571428656578064, 0.875, 0.8809523582458496, 0.863095223903656, 0.875, 0.886904776096344, 0.9107142686843872, 0.8571428656578064, 0.9166666865348816, 0.9047619104385376, 0.886904776096344, 0.9047619104385376, 0.9107142686843872, 0.863095223903656, 0.9047619104385376, 0.8928571343421936, 0.8452380895614624, 0.8690476417541504, 0.9166666865348816, 0.886904776096344, 0.898809552192688, 0.9107142686843872, 0.863095223903656, 0.875, 0.9226190447807312, 0.898809552192688, 0.8571428656578064, 0.9047619104385376, 0.9285714030265808, 0.886904776096344, 0.9107142686843872, 0.9226190447807312, 0.9166666865348816, 0.9107142686843872, 0.9166666865348816, 0.9285714030265808, 0.9285714030265808, 0.863095223903656, 0.9583333134651184, 0.9107142686843872, 0.9047619104385376, 0.9404761791229248, 0.9166666865348816, 0.9523809552192688, 0.9166666865348816, 0.9464285969734192, 0.9226190447807312, 0.9226190447807312, 0.9523809552192688, 0.898809552192688, 0.9047619104385376, 0.9345238208770752, 0.9285714030265808, 0.9464285969734192, 0.9226190447807312, 0.9583333134651184, 0.9345238208770752, 0.9523809552192688, 0.9523809552192688, 0.9166666865348816, 0.9642857313156128, 0.9642857313156128, 0.9047619104385376, 0.9523809552192688, 0.9523809552192688, 0.9345238208770752, 0.9523809552192688, 0.9404761791229248, 0.976190447807312, 0.9523809552192688, 0.9107142686843872, 0.9464285969734192, 0.9464285969734192, 0.9523809552192688, 0.9107142686843872, 0.9523809552192688, 0.9166666865348816, 0.9285714030265808, 0.9345238208770752, 0.9107142686843872, 0.9702380895614624, 0.9226190447807312, 0.9464285969734192, 0.9345238208770752, 0.9523809552192688, 0.9583333134651184, 0.9226190447807312, 0.9464285969734192, 0.976190447807312, 0.9345238208770752, 0.9583333134651184, 0.9345238208770752, 0.9583333134651184, 0.9464285969734192, 0.9345238208770752, 0.9226190447807312, 0.9285714030265808, 0.9285714030265808, 0.9702380895614624, 0.9702380895614624, 0.9583333134651184, 0.9583333134651184, 0.9464285969734192, 0.9285714030265808, 0.9464285969734192, 0.9285714030265808, 0.9523809552192688, 0.9345238208770752, 0.9404761791229248, 0.976190447807312, 0.9404761791229248, 0.9523809552192688, 0.9464285969734192, 0.9464285969734192, 0.9642857313156128, 0.9642857313156128, 0.9583333134651184, 0.9226190447807312, 0.9523809552192688, 0.9642857313156128, 0.9583333134651184, 0.9107142686843872, 0.9464285969734192, 0.976190447807312, 0.9702380895614624, 0.9523809552192688, 0.9404761791229248, 0.9285714030265808, 0.9285714030265808, 0.9583333134651184, 0.9702380895614624, 0.9523809552192688, 0.9642857313156128, 0.9345238208770752, 0.976190447807312, 0.9583333134651184, 0.9642857313156128, 0.976190447807312, 0.9226190447807312, 0.9583333134651184, 0.9702380895614624, 0.9583333134651184, 0.9523809552192688, 0.9345238208770752, 0.9345238208770752, 0.9702380895614624, 0.9583333134651184, 0.9642857313156128, 0.9583333134651184, 0.9285714030265808, 0.9583333134651184, 0.9821428656578064, 0.9583333134651184, 0.976190447807312, 0.976190447807312, 0.9285714030265808, 0.9523809552192688, 0.9583333134651184, 0.9642857313156128, 0.9226190447807312, 0.9464285969734192, 0.9642857313156128, 0.9404761791229248, 0.9523809552192688, 0.9702380895614624, 0.9345238208770752, 0.976190447807312, 0.9404761791229248, 0.9523809552192688, 0.9523809552192688, 0.9642857313156128, 0.9702380895614624, 0.9464285969734192, 0.9523809552192688, 0.9642857313156128, 0.9047619104385376, 0.9523809552192688, 0.9821428656578064, 0.9642857313156128, 0.9821428656578064, 0.9642857313156128, 0.9464285969734192, 0.9702380895614624, 0.9583333134651184, 0.9702380895614624, 0.976190447807312, 0.9166666865348816, 0.9583333134651184, 0.9583333134651184, 0.9404761791229248, 0.9464285969734192, 0.976190447807312, 0.9702380895614624, 0.9345238208770752, 0.9523809552192688, 0.9702380895614624, 0.9642857313156128, 0.9702380895614624, 0.9583333134651184, 0.9583333134651184, 0.9523809552192688, 0.9702380895614624, 0.988095223903656, 0.9821428656578064, 0.9940476417541504, 0.9404761791229248, 0.9642857313156128, 0.9702380895614624, 0.9642857313156128, 0.9702380895614624, 0.9702380895614624, 0.9583333134651184, 0.9702380895614624, 0.9464285969734192, 0.9702380895614624, 0.9523809552192688, 0.9464285969734192, 0.9702380895614624, 0.9523809552192688, 0.9523809552192688, 0.9821428656578064, 0.9642857313156128, 0.976190447807312, 0.9642857313156128, 0.9583333134651184, 0.988095223903656, 0.9642857313156128, 0.9583333134651184, 0.9821428656578064, 0.9583333134651184, 0.976190447807312, 0.976190447807312, 0.9702380895614624, 0.9523809552192688, 0.9821428656578064, 0.9583333134651184, 0.9404761791229248, 0.988095223903656, 0.9583333134651184, 0.976190447807312, 0.988095223903656, 0.9583333134651184, 0.9583333134651184, 0.9404761791229248, 0.976190447807312, 0.9583333134651184, 0.976190447807312, 0.9642857313156128, 0.9583333134651184, 0.9702380895614624, 0.9523809552192688, 0.9702380895614624, 0.976190447807312, 0.9702380895614624, 0.976190447807312, 0.9702380895614624, 0.976190447807312, 0.9821428656578064, 0.9583333134651184, 0.9583333134651184, 0.9523809552192688, 0.9523809552192688, 0.9702380895614624, 0.9642857313156128, 0.976190447807312, 0.976190447807312, 0.9583333134651184, 0.9464285969734192, 0.988095223903656, 0.9702380895614624, 0.9523809552192688, 0.976190447807312, 0.9404761791229248, 0.988095223903656, 0.9702380895614624, 0.9642857313156128, 0.9642857313156128, 0.9583333134651184, 0.9642857313156128, 0.9523809552192688, 0.9642857313156128, 0.9583333134651184, 0.976190447807312, 0.9642857313156128, 0.9821428656578064, 0.9583333134651184, 0.9702380895614624, 0.9642857313156128, 0.976190447807312, 0.9821428656578064, 0.9345238208770752, 0.976190447807312, 0.9702380895614624, 0.9642857313156128, 0.976190447807312, 0.9702380895614624, 0.9702380895614624, 0.9642857313156128, 0.9523809552192688, 0.9821428656578064, 0.9702380895614624, 0.9583333134651184, 0.988095223903656, 0.9523809552192688, 0.9345238208770752, 0.988095223903656, 0.9821428656578064, 0.9464285969734192, 0.9523809552192688, 0.9583333134651184, 0.9583333134651184, 0.9642857313156128, 0.9523809552192688, 0.9702380895614624, 0.9642857313156128, 0.9583333134651184, 0.9702380895614624, 0.9583333134651184, 0.988095223903656, 0.9583333134651184, 0.9642857313156128, 0.9523809552192688, 0.9583333134651184, 0.976190447807312, 0.9702380895614624, 0.9583333134651184, 0.9523809552192688, 0.9583333134651184, 0.9583333134651184, 0.9464285969734192, 0.9523809552192688, 0.9702380895614624, 0.9702380895614624, 0.9642857313156128, 0.9702380895614624, 0.9583333134651184, 0.9702380895614624, 0.9464285969734192, 0.9702380895614624, 0.9702380895614624, 0.9642857313156128, 0.9702380895614624, 0.976190447807312, 0.9702380895614624, 0.9345238208770752, 0.9523809552192688, 0.9702380895614624, 0.9702380895614624, 0.976190447807312, 0.976190447807312, 0.9583333134651184, 0.9702380895614624, 0.9702380895614624, 0.9702380895614624, 0.9940476417541504, 0.9702380895614624, 0.9583333134651184, 0.9523809552192688, 0.976190447807312, 0.9464285969734192, 0.9702380895614624, 0.9642857313156128, 0.9642857313156128, 0.9642857313156128, 0.976190447807312, 0.9821428656578064, 0.988095223903656, 0.976190447807312, 0.988095223903656, 0.9702380895614624, 0.9821428656578064, 0.9940476417541504, 0.9821428656578064, 0.9940476417541504, 0.9583333134651184, 0.9821428656578064, 0.9464285969734192, 0.9583333134651184, 0.9642857313156128, 0.9642857313156128, 0.9642857313156128, 0.976190447807312, 0.9583333134651184, 0.9821428656578064, 0.9821428656578064, 0.988095223903656, 0.9702380895614624, 0.988095223903656, 0.9821428656578064, 0.976190447807312, 0.9821428656578064, 0.988095223903656, 0.9940476417541504, 0.976190447807312, 0.9642857313156128, 0.9642857313156128, 0.988095223903656, 0.9642857313156128, 1.0, 0.976190447807312, 0.9702380895614624, 0.9702380895614624, 0.9702380895614624, 0.9642857313156128, 0.9821428656578064, 0.9583333134651184, 0.9821428656578064, 0.9821428656578064, 0.988095223903656, 0.9702380895614624, 0.9940476417541504, 0.9702380895614624, 0.9821428656578064, 0.9702380895614624, 0.9523809552192688, 0.9702380895614624, 0.9583333134651184, 0.976190447807312, 0.9702380895614624, 0.9821428656578064, 0.9345238208770752, 0.9821428656578064, 0.9404761791229248, 0.976190447807312, 0.976190447807312, 0.9642857313156128, 0.9583333134651184, 0.9404761791229248, 0.9702380895614624, 0.9404761791229248, 0.9583333134651184, 0.9702380895614624, 0.976190447807312, 0.9642857313156128, 0.976190447807312, 0.988095223903656, 0.988095223903656, 0.976190447807312, 0.9702380895614624, 0.9642857313156128, 0.988095223903656], 'val_loss': [2.784907341003418, 3.940690517425537, 5.5374274253845215, 6.948410987854004, 7.578820705413818, 7.029396057128906, 6.6516547203063965, 7.097206115722656, 7.4514288902282715, 8.199071884155273, 8.5894136428833, 7.878190040588379, 7.3954949378967285, 7.871914386749268, 7.939959526062012, 7.294728755950928, 6.500166893005371, 6.280681133270264, 6.417303562164307, 7.058086395263672, 7.074130535125732, 6.517917156219482, 5.866269111633301, 5.457453727722168, 5.340940475463867, 5.694196701049805, 5.877066612243652, 6.004672527313232, 5.957387924194336, 5.984500408172607, 5.645975589752197, 4.99248743057251, 4.884418487548828, 4.801022052764893, 4.596856594085693, 4.293177604675293, 4.0231709480285645, 3.7232024669647217, 3.408576726913452, 2.9185917377471924, 2.39741849899292, 2.140622138977051, 2.0814127922058105, 2.2376041412353516, 2.2757201194763184, 2.500399112701416, 2.314946413040161, 2.32422137260437, 2.203948497772217, 2.0370383262634277, 2.0240206718444824, 2.096069574356079, 2.259751081466675, 2.069997787475586, 2.5631535053253174, 3.263786554336548, 3.570770025253296, 3.6117913722991943, 3.1432244777679443, 2.55096173286438, 2.1288678646087646, 2.207926034927368, 2.2614569664001465, 2.1373465061187744, 2.267608404159546, 2.2421138286590576, 2.2701430320739746, 2.1478443145751953, 2.0218286514282227, 1.8289124965667725, 1.85728120803833, 1.9566073417663574, 1.9229122400283813, 1.938006043434143, 1.9822890758514404, 1.789796233177185, 1.9590860605239868, 1.8957518339157104, 1.8478727340698242, 1.8027844429016113, 1.8485994338989258, 2.2087996006011963, 2.5157737731933594, 2.660923480987549, 2.4723453521728516, 1.9575144052505493, 1.394493818283081, 1.0082366466522217, 0.7915341854095459, 0.7259002923965454, 0.7785897254943848, 0.8157342672348022, 0.8445616960525513, 0.8326333165168762, 0.8456015586853027, 0.8717860579490662, 0.9302412271499634, 0.9348519444465637, 1.0241827964782715, 1.0999560356140137, 1.1049538850784302, 0.9834635257720947, 1.008366346359253, 1.0888663530349731, 1.15537691116333, 1.1779067516326904, 1.1434240341186523, 1.1245037317276, 1.1678311824798584, 1.2945250272750854, 1.4825241565704346, 1.6723195314407349, 1.855184555053711, 2.0089941024780273, 2.0172455310821533, 1.919623613357544, 1.6314641237258911, 1.298513650894165, 1.0932700634002686, 0.9742333292961121, 0.9822394251823425, 1.0318019390106201, 1.1446311473846436, 1.23524808883667, 1.3048142194747925, 1.3490487337112427, 1.3090918064117432, 1.1161197423934937, 0.9638529419898987, 0.8967922925949097, 0.9299116134643555, 0.9755966067314148, 1.012094497680664, 1.0619875192642212, 1.1832458972930908, 1.308445692062378, 1.312099575996399, 1.2422281503677368, 1.1596637964248657, 1.1605968475341797, 1.1377506256103516, 1.1379921436309814, 1.1406265497207642, 1.2157102823257446, 1.3372907638549805, 1.3426834344863892, 1.2804172039031982, 1.2286587953567505, 1.1272825002670288, 1.0667904615402222, 1.065862774848938, 1.0915882587432861, 1.0775913000106812, 1.1141314506530762, 1.1791545152664185, 1.1967564821243286, 1.175846815109253, 1.1654698848724365, 1.1251444816589355, 1.0898911952972412, 1.028916597366333, 0.95001620054245, 0.8705253005027771, 0.7557802200317383, 0.7037416100502014, 0.7407455444335938, 0.7951996922492981, 5.97070837020874, 3.4874398708343506, 2.3514461517333984, 1.823492407798767, 1.8005744218826294, 1.8786475658416748, 1.7849868535995483, 1.536717414855957, 1.4010859727859497, 1.2097619771957397, 1.0909978151321411, 1.0772082805633545, 1.0726392269134521, 1.037829875946045, 1.0132917165756226, 0.9912807941436768, 0.9581817388534546, 0.9711223840713501, 1.05168879032135, 1.078094244003296, 1.0895321369171143, 1.0680838823318481, 1.0623124837875366, 1.1139941215515137, 1.1628708839416504, 1.1680456399917603, 1.1321220397949219, 1.0647536516189575, 0.9637969732284546, 0.8689597249031067, 0.8118056654930115, 0.8076564073562622, 0.8740304708480835, 0.9340182542800903, 0.976802408695221, 1.0505234003067017, 1.1646335124969482, 1.2729649543762207, 1.439107894897461, 1.5879874229431152, 1.4533096551895142, 1.4129928350448608, 1.3191673755645752, 1.242374300956726, 1.2495206594467163, 1.1904208660125732, 1.144515872001648, 1.0074659585952759, 0.8518381714820862, 0.79091477394104, 0.805975615978241, 0.9153060913085938, 0.9122962951660156, 0.9408040642738342, 1.0048604011535645, 0.9831830263137817, 0.9452390670776367, 0.9107542037963867, 0.9340235590934753, 1.016682744026184, 1.1197599172592163, 1.0164211988449097, 1.002834439277649, 1.0117849111557007, 1.0502434968948364, 1.1251763105392456, 1.2556108236312866, 1.2821543216705322, 1.242973804473877, 1.2028014659881592, 1.2282220125198364, 1.2495555877685547, 1.2218097448349, 1.145455002784729, 1.085256814956665, 0.9952160120010376, 0.9237090945243835, 0.9090903401374817, 0.8628379106521606, 0.8423202633857727, 0.8724142909049988, 0.950229823589325, 0.9441213011741638, 0.9326290488243103, 0.9550411701202393, 0.9959186315536499, 1.0094561576843262, 0.9892562627792358, 0.9908377528190613, 1.0210750102996826, 1.0282158851623535, 0.9894370436668396, 0.9389825463294983, 0.9025624394416809, 0.870089054107666, 0.8685529828071594, 0.8650169968605042, 0.9168733358383179, 1.0601502656936646, 1.2061342000961304, 1.2993130683898926, 1.4101412296295166, 1.407260775566101, 1.4112110137939453, 1.3383182287216187, 1.2850885391235352, 1.210018277168274, 1.231975793838501, 1.1892549991607666, 1.1731396913528442, 1.122239351272583, 1.0151721239089966, 0.9124786257743835, 0.8563498854637146, 0.8426210284233093, 0.837781548500061, 0.9037977457046509, 0.9259753823280334, 0.9216262698173523, 0.8506188988685608, 0.801150918006897, 0.7652816772460938, 0.7626511454582214, 0.7797072529792786, 0.7951720952987671, 0.7943954467773438, 0.8377214074134827, 0.9194744825363159, 1.0130490064620972, 1.0429480075836182, 1.0645033121109009, 1.138866901397705, 1.1492048501968384, 1.1895450353622437, 1.226694941520691, 1.212820291519165, 1.1881293058395386, 1.1962894201278687, 1.221029281616211, 1.2447214126586914, 1.2041481733322144, 1.1316287517547607, 1.0747684240341187, 1.149730920791626, 1.247434139251709, 1.3175129890441895, 1.3537838459014893, 1.2905477285385132, 1.1934151649475098, 1.1785691976547241, 1.246017575263977, 1.313366413116455, 1.199662446975708, 1.035842776298523, 0.9720295667648315, 0.9248992800712585, 0.9354605078697205, 0.9079952836036682, 0.8483174443244934, 0.8044636845588684, 0.7826070785522461, 0.7933971881866455, 0.8356508612632751, 0.8488264083862305, 0.8942270874977112, 0.9289765954017639, 0.9652947187423706, 0.968772828578949, 0.9387537837028503, 0.9265847206115723, 0.8686798214912415, 0.8826361894607544, 0.8654016852378845, 0.8314622640609741, 0.8302549719810486, 0.7811363339424133, 0.7469223141670227, 0.7301225662231445, 0.6810330748558044, 0.6681919693946838, 0.6775109171867371, 0.7224671244621277, 0.77714604139328, 0.8274751305580139, 0.8664583563804626, 0.8675467371940613, 0.837224006652832, 0.8243530988693237, 0.8521679639816284, 0.9296337366104126, 0.950465977191925, 1.0010688304901123, 0.9976006150245667, 0.8937370181083679, 0.8383090496063232, 0.8760443925857544, 0.9432965517044067, 1.0387955904006958, 1.0785496234893799, 1.0947352647781372, 1.0318132638931274, 0.9962027072906494, 0.9696543216705322, 0.933767557144165, 0.9434317946434021, 0.9897743463516235, 1.0493887662887573, 0.9876152276992798, 0.9443908929824829, 0.9480870366096497, 0.9452934861183167, 0.9489525556564331, 0.9463824033737183, 0.9576356410980225, 1.0465891361236572, 1.1763179302215576, 1.1687633991241455, 1.1219983100891113, 1.1876424551010132, 1.275561809539795, 1.3245103359222412, 1.362764835357666, 1.3865301609039307, 1.3831820487976074, 1.3108142614364624, 1.2579265832901, 1.1708894968032837, 1.0383955240249634, 0.956326961517334, 0.8862852454185486, 0.8708162307739258, 0.9166725277900696, 0.9632331132888794, 1.040321946144104, 1.1011204719543457, 1.1718965768814087, 1.2237640619277954, 1.2643381357192993, 1.24129319190979, 1.1977604627609253, 1.1408915519714355, 1.026314616203308, 0.8834075331687927, 0.8013880252838135, 0.7589166164398193, 0.7668525576591492, 0.7954684495925903, 0.8360589146614075, 0.8292222619056702, 0.9069193601608276, 1.018428921699524, 1.047617793083191, 1.0063692331314087, 0.9469423294067383, 0.7761701941490173, 0.6801926493644714, 0.6515190005302429, 0.646300733089447, 0.6666341423988342, 0.7014825344085693, 0.7354780435562134, 0.7726880311965942, 0.8304283022880554, 0.8238081932067871, 0.8358052372932434, 0.8643477559089661, 0.841805100440979, 0.7336987257003784, 0.6905937790870667, 0.6855215430259705, 0.6673122048377991, 0.647449254989624, 0.6430102586746216, 0.6589462161064148, 0.6412696242332458, 0.6613165140151978, 0.709863007068634, 0.7566582560539246, 0.7948435544967651, 0.8479437232017517, 0.9140881299972534, 0.9535697102546692, 0.9508770704269409, 0.9154289364814758, 0.894291341304779, 0.9089175462722778, 0.9492346048355103, 0.9222524166107178, 0.9057342410087585, 0.9148645401000977, 0.9522775411605835, 0.9887593388557434, 0.8916685581207275, 0.8522017002105713, 0.8534759283065796, 0.8481178283691406, 0.86336350440979, 0.914262056350708, 0.9437005519866943, 0.9434810876846313, 0.9162030816078186, 0.8752080202102661, 0.8488970994949341, 0.8432019352912903, 0.8622165322303772, 0.9077991843223572, 0.9798397421836853, 1.0783815383911133, 1.170203685760498, 1.2394033670425415, 1.2586865425109863, 1.230005145072937, 1.2758806943893433, 1.4065392017364502, 1.635396122932434, 1.7308486700057983, 1.7039581537246704, 1.5987212657928467, 1.4423216581344604, 1.3551253080368042, 1.2828304767608643, 1.2138676643371582, 1.1821630001068115, 1.2114806175231934, 1.349845051765442, 1.5661218166351318, 1.6130026578903198, 1.6382635831832886, 1.6326239109039307, 1.5845935344696045, 1.5074025392532349, 1.4160079956054688, 1.3293339014053345], 'val_accuracy': [0.13333334028720856, 0.13333334028720856, 0.20000000298023224, 0.17777778208255768, 0.13333334028720856, 0.20000000298023224, 0.24444444477558136, 0.24444444477558136, 0.2666666805744171, 0.15555556118488312, 0.15555556118488312, 0.2666666805744171, 0.2222222238779068, 0.15555556118488312, 0.13333334028720856, 0.13333334028720856, 0.15555556118488312, 0.2222222238779068, 0.2888889014720917, 0.2888889014720917, 0.31111112236976624, 0.3333333432674408, 0.3333333432674408, 0.2888889014720917, 0.24444444477558136, 0.15555556118488312, 0.13333334028720856, 0.13333334028720856, 0.13333334028720856, 0.13333334028720856, 0.13333334028720856, 0.13333334028720856, 0.17777778208255768, 0.20000000298023224, 0.17777778208255768, 0.20000000298023224, 0.2222222238779068, 0.2666666805744171, 0.31111112236976624, 0.3333333432674408, 0.4000000059604645, 0.5333333611488342, 0.5333333611488342, 0.5333333611488342, 0.46666666865348816, 0.42222222685813904, 0.4000000059604645, 0.42222222685813904, 0.4000000059604645, 0.42222222685813904, 0.5333333611488342, 0.5555555820465088, 0.5333333611488342, 0.5333333611488342, 0.5555555820465088, 0.42222222685813904, 0.42222222685813904, 0.3333333432674408, 0.35555556416511536, 0.35555556416511536, 0.5111111402511597, 0.4888888895511627, 0.5111111402511597, 0.5333333611488342, 0.5333333611488342, 0.5555555820465088, 0.5555555820465088, 0.5333333611488342, 0.5555555820465088, 0.5333333611488342, 0.5555555820465088, 0.5333333611488342, 0.5111111402511597, 0.5555555820465088, 0.5555555820465088, 0.5777778029441833, 0.5555555820465088, 0.5777778029441833, 0.5777778029441833, 0.5777778029441833, 0.5777778029441833, 0.5555555820465088, 0.5777778029441833, 0.5555555820465088, 0.5333333611488342, 0.6888889074325562, 0.7333333492279053, 0.7777777910232544, 0.8444444537162781, 0.8666666746139526, 0.8222222328186035, 0.800000011920929, 0.7777777910232544, 0.8222222328186035, 0.800000011920929, 0.8222222328186035, 0.8222222328186035, 0.8444444537162781, 0.8666666746139526, 0.8222222328186035, 0.800000011920929, 0.8666666746139526, 0.8444444537162781, 0.8444444537162781, 0.8222222328186035, 0.8222222328186035, 0.8444444537162781, 0.8444444537162781, 0.8444444537162781, 0.800000011920929, 0.800000011920929, 0.7777777910232544, 0.6888889074325562, 0.6666666865348816, 0.6666666865348816, 0.6888889074325562, 0.7555555701255798, 0.800000011920929, 0.8222222328186035, 0.8444444537162781, 0.8444444537162781, 0.8444444537162781, 0.8222222328186035, 0.800000011920929, 0.800000011920929, 0.800000011920929, 0.8222222328186035, 0.8444444537162781, 0.8888888955116272, 0.8666666746139526, 0.8666666746139526, 0.8444444537162781, 0.8444444537162781, 0.8444444537162781, 0.7777777910232544, 0.7777777910232544, 0.800000011920929, 0.7777777910232544, 0.8222222328186035, 0.8444444537162781, 0.8444444537162781, 0.8222222328186035, 0.8444444537162781, 0.800000011920929, 0.8222222328186035, 0.8222222328186035, 0.8444444537162781, 0.8666666746139526, 0.8888888955116272, 0.8888888955116272, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8444444537162781, 0.8444444537162781, 0.8666666746139526, 0.8888888955116272, 0.8666666746139526, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.9111111164093018, 0.9111111164093018, 0.8888888955116272, 0.8888888955116272, 0.8666666746139526, 0.35555556416511536, 0.4444444477558136, 0.5777778029441833, 0.6000000238418579, 0.6666666865348816, 0.644444465637207, 0.6666666865348816, 0.7111111283302307, 0.7333333492279053, 0.7333333492279053, 0.7777777910232544, 0.800000011920929, 0.800000011920929, 0.7777777910232544, 0.7777777910232544, 0.7777777910232544, 0.7777777910232544, 0.7777777910232544, 0.7777777910232544, 0.800000011920929, 0.8222222328186035, 0.8222222328186035, 0.8444444537162781, 0.8444444537162781, 0.8444444537162781, 0.8444444537162781, 0.8222222328186035, 0.8444444537162781, 0.8444444537162781, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8888888955116272, 0.8666666746139526, 0.8444444537162781, 0.8222222328186035, 0.7555555701255798, 0.7111111283302307, 0.7333333492279053, 0.7333333492279053, 0.7777777910232544, 0.7777777910232544, 0.7777777910232544, 0.7555555701255798, 0.7777777910232544, 0.8222222328186035, 0.8666666746139526, 0.8666666746139526, 0.8444444537162781, 0.8444444537162781, 0.8222222328186035, 0.800000011920929, 0.7777777910232544, 0.7555555701255798, 0.7555555701255798, 0.7555555701255798, 0.800000011920929, 0.7777777910232544, 0.8222222328186035, 0.8222222328186035, 0.8444444537162781, 0.8888888955116272, 0.9111111164093018, 0.9111111164093018, 0.9111111164093018, 0.9111111164093018, 0.9111111164093018, 0.9111111164093018, 0.8888888955116272, 0.8888888955116272, 0.8666666746139526, 0.8444444537162781, 0.8666666746139526, 0.8222222328186035, 0.8444444537162781, 0.9111111164093018, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8666666746139526, 0.8444444537162781, 0.8444444537162781, 0.8666666746139526, 0.8666666746139526, 0.8444444537162781, 0.8444444537162781, 0.8444444537162781, 0.8444444537162781, 0.8444444537162781, 0.8444444537162781, 0.8444444537162781, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8222222328186035, 0.800000011920929, 0.7777777910232544, 0.7333333492279053, 0.7111111283302307, 0.7555555701255798, 0.7777777910232544, 0.7777777910232544, 0.800000011920929, 0.800000011920929, 0.800000011920929, 0.800000011920929, 0.800000011920929, 0.8222222328186035, 0.8444444537162781, 0.8444444537162781, 0.8666666746139526, 0.8444444537162781, 0.8444444537162781, 0.8444444537162781, 0.8444444537162781, 0.8444444537162781, 0.8666666746139526, 0.8666666746139526, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.9111111164093018, 0.8888888955116272, 0.8666666746139526, 0.8888888955116272, 0.8888888955116272, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8444444537162781, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8222222328186035, 0.8222222328186035, 0.800000011920929, 0.800000011920929, 0.8222222328186035, 0.8222222328186035, 0.8222222328186035, 0.8444444537162781, 0.8222222328186035, 0.800000011920929, 0.800000011920929, 0.8222222328186035, 0.8444444537162781, 0.8444444537162781, 0.8444444537162781, 0.8444444537162781, 0.8444444537162781, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8888888955116272, 0.8888888955116272, 0.9111111164093018, 0.8888888955116272, 0.8666666746139526, 0.8666666746139526, 0.8888888955116272, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8444444537162781, 0.8444444537162781, 0.8444444537162781, 0.8444444537162781, 0.8444444537162781, 0.8444444537162781, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8444444537162781, 0.8444444537162781, 0.8888888955116272, 0.8888888955116272, 0.9111111164093018, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.9111111164093018, 0.9111111164093018, 0.9111111164093018, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8444444537162781, 0.8444444537162781, 0.8444444537162781, 0.8444444537162781, 0.8444444537162781, 0.8444444537162781, 0.8444444537162781, 0.8444444537162781, 0.8444444537162781, 0.8444444537162781, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8444444537162781, 0.8444444537162781, 0.8444444537162781, 0.8444444537162781, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8444444537162781, 0.8222222328186035, 0.8222222328186035, 0.8444444537162781, 0.8444444537162781, 0.8444444537162781, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8888888955116272, 0.9111111164093018, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.9111111164093018, 0.8888888955116272, 0.8444444537162781, 0.8666666746139526, 0.8888888955116272, 0.9333333373069763, 0.9111111164093018, 0.9111111164093018, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8666666746139526, 0.8444444537162781, 0.8444444537162781, 0.8444444537162781, 0.8222222328186035, 0.8666666746139526, 0.8444444537162781, 0.8222222328186035, 0.8222222328186035, 0.8888888955116272, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8888888955116272, 0.8888888955116272, 0.9111111164093018, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8888888955116272, 0.8666666746139526, 0.8444444537162781, 0.800000011920929, 0.7777777910232544, 0.7555555701255798, 0.7555555701255798, 0.800000011920929, 0.8222222328186035, 0.8444444537162781, 0.8444444537162781, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.8666666746139526, 0.800000011920929, 0.7777777910232544, 0.7777777910232544, 0.7555555701255798, 0.7777777910232544, 0.8222222328186035, 0.8444444537162781, 0.8666666746139526]}\n",
    "\n",
    "#exp5\n",
    "\n",
    "import tensorflow\n",
    "\n",
    "from keras.layers import Dense,Flatten\n",
    "\n",
    "from keras.applications.vgg16 import VGG16\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "from keras import Sequential\n",
    "\n",
    "from keras.layers import Dense,Conv2D,MaxPooling2D,Flatten,BatchNormalization,Dropout\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam,SGD,RMSprop\n",
    "\n",
    "from tensorflow.keras.layers import Dense,Input,Dropout,GlobalAveragePooling2D,Flatten,Conv2D,BatchNormalization,Activation,MaxPooling2D\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Flatten, BatchNormalization, Dropout\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from tensorflow.keras.applications.vgg19 import VGG19\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Importing Deep Learning Libraries\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "from keras import regularizers\n",
    "\n",
    "from keras.preprocessing import image\n",
    "\n",
    "​\n",
    "\n",
    "#HISTOGRAM CODE\n",
    "\n",
    "#histogram code\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "\n",
    "​\n",
    "\n",
    "emotions = [\"happy\", \"sadness\", \"anger\", \"disgust\", \"neutral\", \"fear\", \"surprise\"]\n",
    "\n",
    "​\n",
    "\n",
    "folder_path = \"Jaffetrainvalidation/train\"\n",
    "\n",
    "# Counting the number of images per emotion\n",
    "\n",
    "counts = [len(os.listdir(os.path.join(folder_path, emotion))) for emotion in emotions]\n",
    "\n",
    "​\n",
    "\n",
    "# Plotting the bar chart\n",
    "\n",
    "colors = ['red', 'yellow', 'black', 'blue', 'orange', 'green', 'pink']\n",
    "\n",
    "plt.bar(emotions, height=counts, color=colors)\n",
    "\n",
    "plt.ylabel('Number')\n",
    "\n",
    "plt.xlabel('Emotions')\n",
    "\n",
    "​\n",
    "\n",
    "​\n",
    "\n",
    "#plt.savefig('hostgoarm.png')\n",
    "\n",
    "#plt.show()\n",
    "\n",
    "​\n",
    "\n",
    "​\n",
    "\n",
    "# Data generators\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "​\n",
    "\n",
    "# Data augmentation for training set\n",
    "\n",
    "datagen_train = ImageDataGenerator(\n",
    "\n",
    "    rescale=1./255,\n",
    "\n",
    "    rotation_range=15,\n",
    "\n",
    "    width_shift_range=0.1,\n",
    "\n",
    "    height_shift_range=0.1,\n",
    "\n",
    "    shear_range=0.2,\n",
    "\n",
    "    zoom_range=0.2,\n",
    "\n",
    "    horizontal_flip=True,\n",
    "\n",
    "    fill_mode='nearest'\n",
    "\n",
    ")\n",
    "\n",
    "​\n",
    "\n",
    "# Normalization for validation set (no augmentation)\n",
    "\n",
    "datagen_val = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "​\n",
    "\n",
    "​\n",
    "\n",
    "​\n",
    "\n",
    "train_ds = datagen_train.flow_from_directory(\"Jaffetrainvalidation/train\",\n",
    "\n",
    "                                             target_size=(256, 256),\n",
    "\n",
    "                                             color_mode=\"rgb\",\n",
    "\n",
    "                                             batch_size=batch_size,\n",
    "\n",
    "                                             class_mode='categorical',\n",
    "\n",
    "                                             shuffle=True)\n",
    "\n",
    "​\n",
    "\n",
    "test_ds = datagen_val.flow_from_directory(\"Jaffetrainvalidation/validation\",\n",
    "\n",
    "                                         target_size=(256, 256),\n",
    "\n",
    "                                         color_mode=\"rgb\",\n",
    "\n",
    "                                         batch_size=batch_size,\n",
    "\n",
    "                                         class_mode='categorical',\n",
    "\n",
    "                                         shuffle=False)\n",
    "\n",
    "print('Train and Validation sets have been created.')\n",
    "\n",
    "​\n",
    "\n",
    "​\n",
    "\n",
    "#model vgg19\n",
    "\n",
    "​\n",
    "\n",
    "from keras.applications.vgg19 import VGG19\n",
    "\n",
    "conv_base = VGG19(weights='imagenet', include_top=False, input_shape=(256, 256, 3))\n",
    "\n",
    "conv_base.summary()\n",
    "\n",
    "​\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(conv_base)\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(256,activation = 'relu'))\n",
    "\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dropout(0.30))\n",
    "\n",
    "# Second fully connected layer  \n",
    "\n",
    "model.add(Dense(512,activation = 'relu'))\n",
    "\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dropout(0.30))\n",
    "\n",
    "​\n",
    "\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "​\n",
    "\n",
    "# model.summary()\n",
    "\n",
    "# Visualize the model.\n",
    "\n",
    "#plot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True)\n",
    "\n",
    "​\n",
    "\n",
    "conv_base.trainable = False\n",
    "\n",
    "​\n",
    "\n",
    "model.compile(optimizer = Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "​\n",
    "\n",
    "print('CNN model has been created you can proceed to train you data with this model.')\n",
    "\n",
    "​\n",
    "\n",
    "​\n",
    "\n",
    "​\n",
    "\n",
    "​\n",
    "\n",
    "# Training the model\n",
    "\n",
    "epochs = 500\n",
    "\n",
    "​\n",
    "\n",
    "history = model.fit(x=train_ds,\n",
    "\n",
    "                    epochs=epochs,\n",
    "\n",
    "                    validation_data=test_ds)\n",
    "\n",
    "​\n",
    "\n",
    "# Print training history\n",
    "\n",
    "print(history.history)\n",
    "\n",
    "​\n",
    "\n",
    "# Plotting training history\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "​\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "\n",
    "plt.title('Loss')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "​\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "\n",
    "plt.title('Accuracy')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "Found 168 images belonging to 7 classes.\n",
    "Found 45 images belonging to 7 classes.\n",
    "Train and Validation sets have been created.\n",
    "Model: \"vgg19\"\n",
    "_________________________________________________________________\n",
    " Layer (type)                Output Shape              Param #   \n",
    "=================================================================\n",
    " input_6 (InputLayer)        [(None, 256, 256, 3)]     0         \n",
    "                                                                 \n",
    " block1_conv1 (Conv2D)       (None, 256, 256, 64)      1792      \n",
    "                                                                 \n",
    " block1_conv2 (Conv2D)       (None, 256, 256, 64)      36928     \n",
    "                                                                 \n",
    " block1_pool (MaxPooling2D)  (None, 128, 128, 64)      0         \n",
    "                                                                 \n",
    " block2_conv1 (Conv2D)       (None, 128, 128, 128)     73856     \n",
    "                                                                 \n",
    " block2_conv2 (Conv2D)       (None, 128, 128, 128)     147584    \n",
    "                                                                 \n",
    " block2_pool (MaxPooling2D)  (None, 64, 64, 128)       0         \n",
    "                                                                 \n",
    " block3_conv1 (Conv2D)       (None, 64, 64, 256)       295168    \n",
    "                                                                 \n",
    " block3_conv2 (Conv2D)       (None, 64, 64, 256)       590080    \n",
    "                                                                 \n",
    " block3_conv3 (Conv2D)       (None, 64, 64, 256)       590080    \n",
    "                                                                 \n",
    " block3_conv4 (Conv2D)       (None, 64, 64, 256)       590080    \n",
    "                                                                 \n",
    " block3_pool (MaxPooling2D)  (None, 32, 32, 256)       0         \n",
    "                                                                 \n",
    " block4_conv1 (Conv2D)       (None, 32, 32, 512)       1180160   \n",
    "                                                                 \n",
    " block4_conv2 (Conv2D)       (None, 32, 32, 512)       2359808   \n",
    "                                                                 \n",
    " block4_conv3 (Conv2D)       (None, 32, 32, 512)       2359808   \n",
    "                                                                 \n",
    " block4_conv4 (Conv2D)       (None, 32, 32, 512)       2359808   \n",
    "                                                                 \n",
    " block4_pool (MaxPooling2D)  (None, 16, 16, 512)       0         \n",
    "                                                                 \n",
    " block5_conv1 (Conv2D)       (None, 16, 16, 512)       2359808   \n",
    "                                                                 \n",
    " block5_conv2 (Conv2D)       (None, 16, 16, 512)       2359808   \n",
    "                                                                 \n",
    " block5_conv3 (Conv2D)       (None, 16, 16, 512)       2359808   \n",
    "                                                                 \n",
    " block5_conv4 (Conv2D)       (None, 16, 16, 512)       2359808   \n",
    "                                                                 \n",
    " block5_pool (MaxPooling2D)  (None, 8, 8, 512)         0         \n",
    "                                                                 \n",
    "=================================================================\n",
    "Total params: 20024384 (76.39 MB)\n",
    "Trainable params: 20024384 (76.39 MB)\n",
    "Non-trainable params: 0 (0.00 Byte)\n",
    "_________________________________________________________________\n",
    "CNN model has been created you can proceed to train you data with this model.\n",
    "Epoch 1/500\n",
    "3/3 [==============================] - 3s 517ms/step - loss: 2.9123 - accuracy: 0.1667 - val_loss: 2.5050 - val_accuracy: 0.2222\n",
    "Epoch 2/500\n",
    "3/3 [==============================] - 2s 447ms/step - loss: 2.4497 - accuracy: 0.2440 - val_loss: 4.1579 - val_accuracy: 0.1333\n",
    "Epoch 3/500\n",
    "3/3 [==============================] - 2s 459ms/step - loss: 1.9564 - accuracy: 0.3631 - val_loss: 7.8598 - val_accuracy: 0.1333\n",
    "Epoch 4/500\n",
    "3/3 [==============================] - 2s 452ms/step - loss: 2.0495 - accuracy: 0.3274 - val_loss: 10.0362 - val_accuracy: 0.1333\n",
    "Epoch 5/500\n",
    "3/3 [==============================] - 2s 469ms/step - loss: 2.0347 - accuracy: 0.2857 - val_loss: 10.2716 - val_accuracy: 0.1333\n",
    "Epoch 6/500\n",
    "3/3 [==============================] - 2s 610ms/step - loss: 1.9789 - accuracy: 0.3869 - val_loss: 9.2821 - val_accuracy: 0.1333\n",
    "Epoch 7/500\n",
    "3/3 [==============================] - 2s 467ms/step - loss: 1.6616 - accuracy: 0.3810 - val_loss: 8.0563 - val_accuracy: 0.1333\n",
    "Epoch 8/500\n",
    "3/3 [==============================] - 2s 604ms/step - loss: 1.6250 - accuracy: 0.4107 - val_loss: 7.9008 - val_accuracy: 0.1333\n",
    "Epoch 9/500\n",
    "3/3 [==============================] - 2s 609ms/step - loss: 1.6805 - accuracy: 0.4107 - val_loss: 8.0525 - val_accuracy: 0.1333\n",
    "Epoch 10/500\n",
    "3/3 [==============================] - 2s 448ms/step - loss: 1.4593 - accuracy: 0.4345 - val_loss: 9.2967 - val_accuracy: 0.1333\n",
    "Epoch 11/500\n",
    "3/3 [==============================] - 2s 608ms/step - loss: 1.3190 - accuracy: 0.5298 - val_loss: 11.1518 - val_accuracy: 0.1333\n",
    "Epoch 12/500\n",
    "3/3 [==============================] - 2s 471ms/step - loss: 1.3253 - accuracy: 0.5119 - val_loss: 11.8461 - val_accuracy: 0.1333\n",
    "Epoch 13/500\n",
    "3/3 [==============================] - 2s 449ms/step - loss: 1.3211 - accuracy: 0.5238 - val_loss: 11.7947 - val_accuracy: 0.1333\n",
    "Epoch 14/500\n",
    "3/3 [==============================] - 2s 448ms/step - loss: 1.2956 - accuracy: 0.5476 - val_loss: 10.4120 - val_accuracy: 0.1333\n",
    "Epoch 15/500\n",
    "3/3 [==============================] - 2s 448ms/step - loss: 1.2571 - accuracy: 0.5417 - val_loss: 8.2604 - val_accuracy: 0.1333\n",
    "Epoch 16/500\n",
    "3/3 [==============================] - 2s 468ms/step - loss: 1.2157 - accuracy: 0.5536 - val_loss: 7.6878 - val_accuracy: 0.2444\n",
    "Epoch 17/500\n",
    "3/3 [==============================] - 2s 626ms/step - loss: 1.0587 - accuracy: 0.6369 - val_loss: 7.3096 - val_accuracy: 0.1556\n",
    "Epoch 18/500\n",
    "3/3 [==============================] - 2s 593ms/step - loss: 1.1419 - accuracy: 0.5833 - val_loss: 7.2053 - val_accuracy: 0.1333\n",
    "Epoch 19/500\n",
    "3/3 [==============================] - 2s 470ms/step - loss: 1.0931 - accuracy: 0.5714 - val_loss: 7.4606 - val_accuracy: 0.1333\n",
    "Epoch 20/500\n",
    "3/3 [==============================] - 2s 464ms/step - loss: 1.0463 - accuracy: 0.5893 - val_loss: 8.1774 - val_accuracy: 0.1333\n",
    "Epoch 21/500\n",
    "3/3 [==============================] - 2s 444ms/step - loss: 1.0855 - accuracy: 0.5893 - val_loss: 8.3135 - val_accuracy: 0.1333\n",
    "Epoch 22/500\n",
    "3/3 [==============================] - 2s 593ms/step - loss: 0.9332 - accuracy: 0.6667 - val_loss: 8.5862 - val_accuracy: 0.1333\n",
    "Epoch 23/500\n",
    "3/3 [==============================] - 2s 596ms/step - loss: 1.1321 - accuracy: 0.6190 - val_loss: 8.8192 - val_accuracy: 0.1333\n",
    "Epoch 24/500\n",
    "3/3 [==============================] - 2s 472ms/step - loss: 0.8467 - accuracy: 0.6548 - val_loss: 7.8017 - val_accuracy: 0.1333\n",
    "Epoch 25/500\n",
    "3/3 [==============================] - 2s 471ms/step - loss: 0.8057 - accuracy: 0.7024 - val_loss: 7.0348 - val_accuracy: 0.1333\n",
    "Epoch 26/500\n",
    "3/3 [==============================] - 2s 593ms/step - loss: 0.8897 - accuracy: 0.6845 - val_loss: 6.9685 - val_accuracy: 0.1333\n",
    "Epoch 27/500\n",
    "3/3 [==============================] - 2s 449ms/step - loss: 0.7256 - accuracy: 0.7500 - val_loss: 7.2629 - val_accuracy: 0.1333\n",
    "Epoch 28/500\n",
    "3/3 [==============================] - 2s 599ms/step - loss: 0.6603 - accuracy: 0.7560 - val_loss: 8.3395 - val_accuracy: 0.1333\n",
    "Epoch 29/500\n",
    "3/3 [==============================] - 2s 468ms/step - loss: 0.7245 - accuracy: 0.7440 - val_loss: 8.6592 - val_accuracy: 0.1333\n",
    "Epoch 30/500\n",
    "3/3 [==============================] - 2s 467ms/step - loss: 0.6883 - accuracy: 0.7738 - val_loss: 8.1642 - val_accuracy: 0.1333\n",
    "Epoch 31/500\n",
    "3/3 [==============================] - 2s 586ms/step - loss: 0.6482 - accuracy: 0.7321 - val_loss: 7.3121 - val_accuracy: 0.1333\n",
    "Epoch 32/500\n",
    "3/3 [==============================] - 2s 592ms/step - loss: 0.5804 - accuracy: 0.7917 - val_loss: 7.0608 - val_accuracy: 0.1333\n",
    "Epoch 33/500\n",
    "\n",
    "3/3 [==============================] - 2s 448ms/step - loss: 0.5058 - accuracy: 0.8095 - val_loss: 6.9918 - val_accuracy: 0.1333\n",
    "Epoch 34/500\n",
    "3/3 [==============================] - 2s 470ms/step - loss: 0.6740 - accuracy: 0.7560 - val_loss: 6.9234 - val_accuracy: 0.1333\n",
    "Epoch 35/500\n",
    "3/3 [==============================] - 2s 446ms/step - loss: 0.5456 - accuracy: 0.7500 - val_loss: 6.3513 - val_accuracy: 0.1333\n",
    "Epoch 36/500\n",
    "3/3 [==============================] - 2s 588ms/step - loss: 0.6228 - accuracy: 0.7619 - val_loss: 5.5063 - val_accuracy: 0.1556\n",
    "Epoch 37/500\n",
    "3/3 [==============================] - 2s 591ms/step - loss: 0.5465 - accuracy: 0.7917 - val_loss: 4.7080 - val_accuracy: 0.1778\n",
    "Epoch 38/500\n",
    "3/3 [==============================] - 2s 478ms/step - loss: 0.5521 - accuracy: 0.8155 - val_loss: 5.2359 - val_accuracy: 0.1778\n",
    "Epoch 39/500\n",
    "3/3 [==============================] - 2s 467ms/step - loss: 0.4483 - accuracy: 0.8333 - val_loss: 5.1701 - val_accuracy: 0.1778\n",
    "Epoch 40/500\n",
    "3/3 [==============================] - 2s 451ms/step - loss: 0.4542 - accuracy: 0.8452 - val_loss: 4.6004 - val_accuracy: 0.2000\n",
    "Epoch 41/500\n",
    "3/3 [==============================] - 2s 596ms/step - loss: 0.5510 - accuracy: 0.7857 - val_loss: 3.9757 - val_accuracy: 0.2444\n",
    "Epoch 42/500\n",
    "3/3 [==============================] - 2s 472ms/step - loss: 0.4186 - accuracy: 0.8452 - val_loss: 3.6378 - val_accuracy: 0.2667\n",
    "Epoch 43/500\n",
    "3/3 [==============================] - 2s 471ms/step - loss: 0.5944 - accuracy: 0.7917 - val_loss: 3.4448 - val_accuracy: 0.2667\n",
    "Epoch 44/500\n",
    "3/3 [==============================] - 2s 600ms/step - loss: 0.4970 - accuracy: 0.7976 - val_loss: 3.2331 - val_accuracy: 0.2889\n",
    "Epoch 45/500\n",
    "3/3 [==============================] - 2s 593ms/step - loss: 0.5129 - accuracy: 0.8274 - val_loss: 3.6433 - val_accuracy: 0.3111\n",
    "Epoch 46/500\n",
    "3/3 [==============================] - 2s 473ms/step - loss: 0.5126 - accuracy: 0.7976 - val_loss: 3.9032 - val_accuracy: 0.2889\n",
    "Epoch 47/500\n",
    "3/3 [==============================] - 2s 451ms/step - loss: 0.4344 - accuracy: 0.8690 - val_loss: 4.2914 - val_accuracy: 0.2444\n",
    "Epoch 48/500\n",
    "3/3 [==============================] - 2s 466ms/step - loss: 0.4576 - accuracy: 0.8095 - val_loss: 3.7788 - val_accuracy: 0.2444\n",
    "Epoch 49/500\n",
    "3/3 [==============================] - 2s 469ms/step - loss: 0.3708 - accuracy: 0.8631 - val_loss: 2.7402 - val_accuracy: 0.3556\n",
    "Epoch 50/500\n",
    "3/3 [==============================] - 2s 447ms/step - loss: 0.4665 - accuracy: 0.8274 - val_loss: 2.3801 - val_accuracy: 0.3333\n",
    "Epoch 51/500\n",
    "3/3 [==============================] - 2s 471ms/step - loss: 0.4295 - accuracy: 0.8571 - val_loss: 2.3067 - val_accuracy: 0.3111\n",
    "Epoch 52/500\n",
    "3/3 [==============================] - 2s 466ms/step - loss: 0.4091 - accuracy: 0.8155 - val_loss: 2.2536 - val_accuracy: 0.3556\n",
    "Epoch 53/500\n",
    "3/3 [==============================] - 2s 449ms/step - loss: 0.3610 - accuracy: 0.8571 - val_loss: 2.3222 - val_accuracy: 0.3556\n",
    "Epoch 54/500\n",
    "3/3 [==============================] - 2s 593ms/step - loss: 0.3361 - accuracy: 0.8690 - val_loss: 2.2616 - val_accuracy: 0.4000\n",
    "Epoch 55/500\n",
    "3/3 [==============================] - 2s 445ms/step - loss: 0.4435 - accuracy: 0.8512 - val_loss: 2.2012 - val_accuracy: 0.4444\n",
    "Epoch 56/500\n",
    "3/3 [==============================] - 2s 451ms/step - loss: 0.3514 - accuracy: 0.8690 - val_loss: 2.1881 - val_accuracy: 0.4444\n",
    "Epoch 57/500\n",
    "3/3 [==============================] - 2s 594ms/step - loss: 0.3896 - accuracy: 0.8750 - val_loss: 2.2001 - val_accuracy: 0.4889\n",
    "Epoch 58/500\n",
    "3/3 [==============================] - 2s 594ms/step - loss: 0.5029 - accuracy: 0.8512 - val_loss: 2.1156 - val_accuracy: 0.4000\n",
    "Epoch 59/500\n",
    "3/3 [==============================] - 2s 453ms/step - loss: 0.3539 - accuracy: 0.8631 - val_loss: 1.9298 - val_accuracy: 0.3778\n",
    "Epoch 60/500\n",
    "3/3 [==============================] - 2s 448ms/step - loss: 0.3775 - accuracy: 0.8571 - val_loss: 1.6457 - val_accuracy: 0.4222\n",
    "Epoch 61/500\n",
    "3/3 [==============================] - 2s 594ms/step - loss: 0.4963 - accuracy: 0.8333 - val_loss: 1.3683 - val_accuracy: 0.6000\n",
    "Epoch 62/500\n",
    "3/3 [==============================] - 2s 445ms/step - loss: 0.3442 - accuracy: 0.8750 - val_loss: 1.2298 - val_accuracy: 0.6889\n",
    "Epoch 63/500\n",
    "3/3 [==============================] - 2s 450ms/step - loss: 0.4842 - accuracy: 0.8631 - val_loss: 1.1444 - val_accuracy: 0.7111\n",
    "Epoch 64/500\n",
    "3/3 [==============================] - 2s 586ms/step - loss: 0.3658 - accuracy: 0.8869 - val_loss: 0.9673 - val_accuracy: 0.7111\n",
    "Epoch 65/500\n",
    "3/3 [==============================] - 2s 590ms/step - loss: 0.3511 - accuracy: 0.8810 - val_loss: 0.7959 - val_accuracy: 0.7333\n",
    "Epoch 66/500\n",
    "3/3 [==============================] - 2s 468ms/step - loss: 0.3219 - accuracy: 0.8869 - val_loss: 0.7354 - val_accuracy: 0.7556\n",
    "Epoch 67/500\n",
    "3/3 [==============================] - 2s 447ms/step - loss: 0.3765 - accuracy: 0.8571 - val_loss: 0.7637 - val_accuracy: 0.7556\n",
    "Epoch 68/500\n",
    "3/3 [==============================] - 2s 470ms/step - loss: 0.2628 - accuracy: 0.9167 - val_loss: 0.7907 - val_accuracy: 0.7333\n",
    "Epoch 69/500\n",
    "3/3 [==============================] - 2s 471ms/step - loss: 0.3309 - accuracy: 0.8750 - val_loss: 0.7676 - val_accuracy: 0.7556\n",
    "Epoch 70/500\n",
    "3/3 [==============================] - 2s 590ms/step - loss: 0.2798 - accuracy: 0.9107 - val_loss: 0.7576 - val_accuracy: 0.7556\n",
    "Epoch 71/500\n",
    "3/3 [==============================] - 2s 594ms/step - loss: 0.2489 - accuracy: 0.8810 - val_loss: 0.8013 - val_accuracy: 0.7556\n",
    "Epoch 72/500\n",
    "3/3 [==============================] - 2s 474ms/step - loss: 0.2890 - accuracy: 0.8869 - val_loss: 0.8389 - val_accuracy: 0.7556\n",
    "Epoch 73/500\n",
    "3/3 [==============================] - 2s 597ms/step - loss: 0.3253 - accuracy: 0.8750 - val_loss: 0.8647 - val_accuracy: 0.7333\n",
    "Epoch 74/500\n",
    "3/3 [==============================] - 2s 454ms/step - loss: 0.2474 - accuracy: 0.8929 - val_loss: 0.8972 - val_accuracy: 0.7333\n",
    "Epoch 75/500\n",
    "3/3 [==============================] - 2s 585ms/step - loss: 0.2585 - accuracy: 0.8988 - val_loss: 0.8659 - val_accuracy: 0.7333\n",
    "Epoch 76/500\n",
    "3/3 [==============================] - 2s 448ms/step - loss: 0.2245 - accuracy: 0.9345 - val_loss: 0.8224 - val_accuracy: 0.7778\n",
    "Epoch 77/500\n",
    "3/3 [==============================] - 2s 450ms/step - loss: 0.2127 - accuracy: 0.9048 - val_loss: 0.7438 - val_accuracy: 0.8000\n",
    "Epoch 78/500\n",
    "3/3 [==============================] - 2s 451ms/step - loss: 0.3036 - accuracy: 0.8929 - val_loss: 0.6659 - val_accuracy: 0.8222\n",
    "Epoch 79/500\n",
    "3/3 [==============================] - 2s 453ms/step - loss: 0.3265 - accuracy: 0.8929 - val_loss: 0.5998 - val_accuracy: 0.8444\n",
    "Epoch 80/500\n",
    "3/3 [==============================] - 2s 466ms/step - loss: 0.3790 - accuracy: 0.8750 - val_loss: 0.5668 - val_accuracy: 0.8889\n",
    "Epoch 81/500\n",
    "3/3 [==============================] - 2s 449ms/step - loss: 0.2705 - accuracy: 0.8929 - val_loss: 0.5809 - val_accuracy: 0.8889\n",
    "Epoch 82/500\n",
    "3/3 [==============================] - 2s 449ms/step - loss: 0.2568 - accuracy: 0.9048 - val_loss: 0.6143 - val_accuracy: 0.8444\n",
    "Epoch 83/500\n",
    "3/3 [==============================] - 2s 472ms/step - loss: 0.3376 - accuracy: 0.8869 - val_loss: 0.6830 - val_accuracy: 0.8222\n",
    "Epoch 84/500\n",
    "3/3 [==============================] - 2s 461ms/step - loss: 0.3076 - accuracy: 0.8810 - val_loss: 0.7656 - val_accuracy: 0.8222\n",
    "Epoch 85/500\n",
    "3/3 [==============================] - 2s 453ms/step - loss: 0.3145 - accuracy: 0.8929 - val_loss: 0.7893 - val_accuracy: 0.8000\n",
    "Epoch 86/500\n",
    "3/3 [==============================] - 2s 590ms/step - loss: 0.2645 - accuracy: 0.9167 - val_loss: 0.8277 - val_accuracy: 0.7778\n",
    "Epoch 87/500\n",
    "3/3 [==============================] - 2s 592ms/step - loss: 0.2912 - accuracy: 0.8988 - val_loss: 0.8112 - val_accuracy: 0.8222\n",
    "Epoch 88/500\n",
    "3/3 [==============================] - 2s 462ms/step - loss: 0.1534 - accuracy: 0.9464 - val_loss: 0.8068 - val_accuracy: 0.8222\n",
    "Epoch 89/500\n",
    "3/3 [==============================] - 2s 446ms/step - loss: 0.2407 - accuracy: 0.9226 - val_loss: 0.8503 - val_accuracy: 0.8222\n",
    "Epoch 90/500\n",
    "3/3 [==============================] - 2s 465ms/step - loss: 0.2936 - accuracy: 0.9226 - val_loss: 0.8857 - val_accuracy: 0.8444\n",
    "\n",
    "Epoch 91/500\n",
    "3/3 [==============================] - 2s 452ms/step - loss: 0.2152 - accuracy: 0.9405 - val_loss: 0.8547 - val_accuracy: 0.8444\n",
    "Epoch 92/500\n",
    "3/3 [==============================] - 2s 586ms/step - loss: 0.2791 - accuracy: 0.8988 - val_loss: 0.8156 - val_accuracy: 0.8889\n",
    "Epoch 93/500\n",
    "3/3 [==============================] - 2s 471ms/step - loss: 0.1989 - accuracy: 0.9167 - val_loss: 0.7797 - val_accuracy: 0.8889\n",
    "Epoch 94/500\n",
    "3/3 [==============================] - 2s 472ms/step - loss: 0.2900 - accuracy: 0.8988 - val_loss: 0.8056 - val_accuracy: 0.8889\n",
    "Epoch 95/500\n",
    "3/3 [==============================] - 2s 592ms/step - loss: 0.2896 - accuracy: 0.9048 - val_loss: 0.8430 - val_accuracy: 0.8889\n",
    "Epoch 96/500\n",
    "3/3 [==============================] - 2s 596ms/step - loss: 0.3497 - accuracy: 0.8929 - val_loss: 0.8873 - val_accuracy: 0.8667\n",
    "Epoch 97/500\n",
    "3/3 [==============================] - 2s 594ms/step - loss: 0.2861 - accuracy: 0.9107 - val_loss: 0.8664 - val_accuracy: 0.8889\n",
    "Epoch 98/500\n",
    "3/3 [==============================] - 2s 596ms/step - loss: 0.2880 - accuracy: 0.8810 - val_loss: 0.8196 - val_accuracy: 0.8667\n",
    "Epoch 99/500\n",
    "3/3 [==============================] - 2s 446ms/step - loss: 0.1979 - accuracy: 0.9345 - val_loss: 0.7865 - val_accuracy: 0.8667\n",
    "Epoch 100/500\n",
    "3/3 [==============================] - 2s 450ms/step - loss: 0.3124 - accuracy: 0.8631 - val_loss: 0.8259 - val_accuracy: 0.8222\n",
    "Epoch 101/500\n",
    "3/3 [==============================] - 2s 595ms/step - loss: 0.2458 - accuracy: 0.8988 - val_loss: 0.9479 - val_accuracy: 0.7778\n",
    "Epoch 102/500\n",
    "3/3 [==============================] - 2s 479ms/step - loss: 0.3156 - accuracy: 0.8869 - val_loss: 1.0900 - val_accuracy: 0.7333\n",
    "Epoch 103/500\n",
    "3/3 [==============================] - 2s 591ms/step - loss: 0.2533 - accuracy: 0.9107 - val_loss: 1.1755 - val_accuracy: 0.7333\n",
    "Epoch 104/500\n",
    "3/3 [==============================] - 2s 596ms/step - loss: 0.2766 - accuracy: 0.8869 - val_loss: 1.2136 - val_accuracy: 0.8444\n",
    "Epoch 105/500\n",
    "3/3 [==============================] - 2s 450ms/step - loss: 0.3392 - accuracy: 0.8869 - val_loss: 1.2080 - val_accuracy: 0.8222\n",
    "Epoch 106/500\n",
    "3/3 [==============================] - 2s 591ms/step - loss: 0.2516 - accuracy: 0.9167 - val_loss: 1.1288 - val_accuracy: 0.8667\n",
    "Epoch 107/500\n",
    "3/3 [==============================] - 2s 591ms/step - loss: 0.2208 - accuracy: 0.9167 - val_loss: 1.1043 - val_accuracy: 0.8444\n",
    "Epoch 108/500\n",
    "3/3 [==============================] - 2s 593ms/step - loss: 0.1700 - accuracy: 0.9226 - val_loss: 1.0869 - val_accuracy: 0.8444\n",
    "Epoch 109/500\n",
    "3/3 [==============================] - 2s 594ms/step - loss: 0.2062 - accuracy: 0.9286 - val_loss: 1.0597 - val_accuracy: 0.8222\n",
    "Epoch 110/500\n",
    "3/3 [==============================] - 2s 599ms/step - loss: 0.2106 - accuracy: 0.9286 - val_loss: 1.2212 - val_accuracy: 0.8000\n",
    "Epoch 111/500\n",
    "3/3 [==============================] - 2s 607ms/step - loss: 0.2115 - accuracy: 0.9226 - val_loss: 1.3817 - val_accuracy: 0.7556\n",
    "Epoch 112/500\n",
    "3/3 [==============================] - 2s 479ms/step - loss: 0.1568 - accuracy: 0.9286 - val_loss: 1.4551 - val_accuracy: 0.7333\n",
    "Epoch 113/500\n",
    "3/3 [==============================] - 2s 448ms/step - loss: 0.2029 - accuracy: 0.9226 - val_loss: 1.5051 - val_accuracy: 0.7333\n",
    "Epoch 114/500\n",
    "3/3 [==============================] - 2s 465ms/step - loss: 0.2084 - accuracy: 0.9226 - val_loss: 1.4018 - val_accuracy: 0.7556\n",
    "Epoch 115/500\n",
    "3/3 [==============================] - 2s 588ms/step - loss: 0.1865 - accuracy: 0.9524 - val_loss: 1.3422 - val_accuracy: 0.7778\n",
    "Epoch 116/500\n",
    "3/3 [==============================] - 2s 469ms/step - loss: 0.2675 - accuracy: 0.8869 - val_loss: 1.2955 - val_accuracy: 0.7778\n",
    "Epoch 117/500\n",
    "3/3 [==============================] - 2s 587ms/step - loss: 0.2348 - accuracy: 0.9048 - val_loss: 1.2751 - val_accuracy: 0.8444\n",
    "Epoch 118/500\n",
    "3/3 [==============================] - 2s 445ms/step - loss: 0.2134 - accuracy: 0.9286 - val_loss: 1.3725 - val_accuracy: 0.8000\n",
    "Epoch 119/500\n",
    "3/3 [==============================] - 2s 469ms/step - loss: 0.1535 - accuracy: 0.9524 - val_loss: 1.4746 - val_accuracy: 0.7333\n",
    "Epoch 120/500\n",
    "3/3 [==============================] - 2s 454ms/step - loss: 0.1750 - accuracy: 0.9345 - val_loss: 1.5714 - val_accuracy: 0.7111\n",
    "Epoch 121/500\n",
    "3/3 [==============================] - 2s 474ms/step - loss: 0.2763 - accuracy: 0.9226 - val_loss: 1.6366 - val_accuracy: 0.7111\n",
    "Epoch 122/500\n",
    "3/3 [==============================] - 2s 593ms/step - loss: 0.1681 - accuracy: 0.9405 - val_loss: 1.6521 - val_accuracy: 0.7111\n",
    "Epoch 123/500\n",
    "3/3 [==============================] - 2s 596ms/step - loss: 0.2544 - accuracy: 0.9048 - val_loss: 1.6250 - val_accuracy: 0.7333\n",
    "Epoch 124/500\n",
    "3/3 [==============================] - 2s 451ms/step - loss: 0.2005 - accuracy: 0.9405 - val_loss: 1.6346 - val_accuracy: 0.6889\n",
    "Epoch 125/500\n",
    "3/3 [==============================] - 2s 451ms/step - loss: 0.1836 - accuracy: 0.9464 - val_loss: 1.5453 - val_accuracy: 0.7333\n",
    "Epoch 126/500\n",
    "3/3 [==============================] - 2s 472ms/step - loss: 0.1944 - accuracy: 0.9107 - val_loss: 1.4678 - val_accuracy: 0.7333\n",
    "Epoch 127/500\n",
    "3/3 [==============================] - 2s 455ms/step - loss: 0.3012 - accuracy: 0.8988 - val_loss: 1.3933 - val_accuracy: 0.7556\n",
    "Epoch 128/500\n",
    "3/3 [==============================] - 2s 596ms/step - loss: 0.1521 - accuracy: 0.9464 - val_loss: 1.4158 - val_accuracy: 0.7556\n",
    "Epoch 129/500\n",
    "3/3 [==============================] - 2s 463ms/step - loss: 0.1888 - accuracy: 0.9345 - val_loss: 1.3363 - val_accuracy: 0.8000\n",
    "Epoch 130/500\n",
    "3/3 [==============================] - 2s 466ms/step - loss: 0.1693 - accuracy: 0.9345 - val_loss: 1.3436 - val_accuracy: 0.8444\n",
    "Epoch 131/500\n",
    "3/3 [==============================] - 2s 466ms/step - loss: 0.1961 - accuracy: 0.9226 - val_loss: 1.3637 - val_accuracy: 0.7778\n",
    "Epoch 132/500\n",
    "3/3 [==============================] - 2s 472ms/step - loss: 0.1965 - accuracy: 0.9226 - val_loss: 1.3304 - val_accuracy: 0.7778\n",
    "Epoch 133/500\n",
    "3/3 [==============================] - 2s 586ms/step - loss: 0.2662 - accuracy: 0.9167 - val_loss: 1.2142 - val_accuracy: 0.8000\n",
    "Epoch 134/500\n",
    "3/3 [==============================] - 2s 445ms/step - loss: 0.0963 - accuracy: 0.9464 - val_loss: 1.2025 - val_accuracy: 0.8000\n",
    "Epoch 135/500\n",
    "3/3 [==============================] - 2s 450ms/step - loss: 0.2065 - accuracy: 0.9226 - val_loss: 1.2501 - val_accuracy: 0.8000\n",
    "Epoch 136/500\n",
    "3/3 [==============================] - 2s 454ms/step - loss: 0.1508 - accuracy: 0.9464 - val_loss: 1.2826 - val_accuracy: 0.8222\n",
    "Epoch 137/500\n",
    "3/3 [==============================] - 2s 449ms/step - loss: 0.1464 - accuracy: 0.9583 - val_loss: 1.2840 - val_accuracy: 0.8222\n",
    "Epoch 138/500\n",
    "3/3 [==============================] - 2s 594ms/step - loss: 0.1745 - accuracy: 0.9464 - val_loss: 1.2831 - val_accuracy: 0.8000\n",
    "Epoch 139/500\n",
    "3/3 [==============================] - 2s 463ms/step - loss: 0.2007 - accuracy: 0.9107 - val_loss: 1.2073 - val_accuracy: 0.8222\n",
    "Epoch 140/500\n",
    "3/3 [==============================] - 2s 449ms/step - loss: 0.1906 - accuracy: 0.9405 - val_loss: 1.1420 - val_accuracy: 0.8667\n",
    "Epoch 141/500\n",
    "3/3 [==============================] - 2s 475ms/step - loss: 0.1782 - accuracy: 0.9464 - val_loss: 1.1384 - val_accuracy: 0.8667\n",
    "Epoch 142/500\n",
    "3/3 [==============================] - 2s 467ms/step - loss: 0.2369 - accuracy: 0.9107 - val_loss: 1.2005 - val_accuracy: 0.8667\n",
    "Epoch 143/500\n",
    "3/3 [==============================] - 2s 592ms/step - loss: 0.1568 - accuracy: 0.9405 - val_loss: 1.3284 - val_accuracy: 0.8222\n",
    "Epoch 144/500\n",
    "3/3 [==============================] - 2s 471ms/step - loss: 0.1689 - accuracy: 0.9524 - val_loss: 1.3843 - val_accuracy: 0.7778\n",
    "Epoch 145/500\n",
    "3/3 [==============================] - 2s 460ms/step - loss: 0.1362 - accuracy: 0.9405 - val_loss: 1.4216 - val_accuracy: 0.7556\n",
    "Epoch 146/500\n",
    "3/3 [==============================] - 2s 592ms/step - loss: 0.0847 - accuracy: 0.9762 - val_loss: 1.4641 - val_accuracy: 0.7778\n",
    "Epoch 147/500\n",
    "3/3 [==============================] - 2s 596ms/step - loss: 0.1333 - accuracy: 0.9583 - val_loss: 1.5188 - val_accuracy: 0.7778\n",
    "Epoch 148/500\n",
    "\n",
    "3/3 [==============================] - 2s 447ms/step - loss: 0.2321 - accuracy: 0.9286 - val_loss: 1.5636 - val_accuracy: 0.7778\n",
    "Epoch 149/500\n",
    "3/3 [==============================] - 2s 593ms/step - loss: 0.1975 - accuracy: 0.9405 - val_loss: 1.5912 - val_accuracy: 0.8000\n",
    "Epoch 150/500\n",
    "3/3 [==============================] - 2s 468ms/step - loss: 0.1794 - accuracy: 0.9286 - val_loss: 1.5815 - val_accuracy: 0.8000\n",
    "Epoch 151/500\n",
    "3/3 [==============================] - 2s 450ms/step - loss: 0.1943 - accuracy: 0.9286 - val_loss: 1.4486 - val_accuracy: 0.8000\n",
    "Epoch 152/500\n",
    "3/3 [==============================] - 2s 474ms/step - loss: 0.2000 - accuracy: 0.9286 - val_loss: 1.3041 - val_accuracy: 0.8444\n",
    "Epoch 153/500\n",
    "3/3 [==============================] - 2s 586ms/step - loss: 0.2501 - accuracy: 0.9107 - val_loss: 1.3298 - val_accuracy: 0.8667\n",
    "Epoch 154/500\n",
    "3/3 [==============================] - 2s 449ms/step - loss: 0.1421 - accuracy: 0.9524 - val_loss: 1.4715 - val_accuracy: 0.8000\n",
    "Epoch 155/500\n",
    "3/3 [==============================] - 2s 594ms/step - loss: 0.1374 - accuracy: 0.9524 - val_loss: 1.7230 - val_accuracy: 0.7778\n",
    "Epoch 156/500\n",
    "3/3 [==============================] - 2s 588ms/step - loss: 0.1471 - accuracy: 0.9345 - val_loss: 1.8683 - val_accuracy: 0.7333\n",
    "Epoch 157/500\n",
    "3/3 [==============================] - 2s 450ms/step - loss: 0.1570 - accuracy: 0.9524 - val_loss: 1.9173 - val_accuracy: 0.7333\n",
    "Epoch 158/500\n",
    "3/3 [==============================] - 2s 450ms/step - loss: 0.1293 - accuracy: 0.9345 - val_loss: 1.8600 - val_accuracy: 0.7333\n",
    "Epoch 159/500\n",
    "3/3 [==============================] - 2s 464ms/step - loss: 0.1159 - accuracy: 0.9643 - val_loss: 1.7093 - val_accuracy: 0.7778\n",
    "Epoch 160/500\n",
    "3/3 [==============================] - 2s 593ms/step - loss: 0.1154 - accuracy: 0.9643 - val_loss: 1.5023 - val_accuracy: 0.7778\n",
    "Epoch 161/500\n",
    "3/3 [==============================] - 2s 450ms/step - loss: 0.1715 - accuracy: 0.9583 - val_loss: 1.3180 - val_accuracy: 0.8444\n",
    "Epoch 162/500\n",
    "3/3 [==============================] - 2s 469ms/step - loss: 0.1529 - accuracy: 0.9583 - val_loss: 1.1864 - val_accuracy: 0.8444\n",
    "Epoch 163/500\n",
    "3/3 [==============================] - 2s 584ms/step - loss: 0.1689 - accuracy: 0.9345 - val_loss: 1.0991 - val_accuracy: 0.8667\n",
    "Epoch 164/500\n",
    "3/3 [==============================] - 2s 451ms/step - loss: 0.1056 - accuracy: 0.9702 - val_loss: 1.0883 - val_accuracy: 0.8667\n",
    "Epoch 165/500\n",
    "3/3 [==============================] - 2s 445ms/step - loss: 0.1256 - accuracy: 0.9524 - val_loss: 1.0970 - val_accuracy: 0.8444\n",
    "Epoch 166/500\n",
    "3/3 [==============================] - 2s 447ms/step - loss: 0.1581 - accuracy: 0.9702 - val_loss: 1.0460 - val_accuracy: 0.8667\n",
    "Epoch 167/500\n",
    "3/3 [==============================] - 2s 470ms/step - loss: 0.1361 - accuracy: 0.9524 - val_loss: 1.0109 - val_accuracy: 0.8667\n",
    "Epoch 168/500\n",
    "3/3 [==============================] - 2s 471ms/step - loss: 0.1023 - accuracy: 0.9643 - val_loss: 0.9495 - val_accuracy: 0.8667\n",
    "Epoch 169/500\n",
    "3/3 [==============================] - 2s 449ms/step - loss: 0.2286 - accuracy: 0.9405 - val_loss: 0.9471 - val_accuracy: 0.8667\n",
    "Epoch 170/500\n",
    "3/3 [==============================] - 2s 472ms/step - loss: 0.1496 - accuracy: 0.9345 - val_loss: 1.0518 - val_accuracy: 0.8667\n",
    "Epoch 171/500\n",
    "3/3 [==============================] - 2s 464ms/step - loss: 0.2136 - accuracy: 0.9464 - val_loss: 1.2011 - val_accuracy: 0.8444\n",
    "Epoch 172/500\n",
    "3/3 [==============================] - 2s 447ms/step - loss: 0.1554 - accuracy: 0.9286 - val_loss: 1.3053 - val_accuracy: 0.8667\n",
    "Epoch 173/500\n",
    "3/3 [==============================] - 2s 464ms/step - loss: 0.1208 - accuracy: 0.9524 - val_loss: 1.3396 - val_accuracy: 0.8444\n",
    "Epoch 174/500\n",
    "3/3 [==============================] - 2s 473ms/step - loss: 0.1220 - accuracy: 0.9524 - val_loss: 1.2521 - val_accuracy: 0.8444\n",
    "Epoch 175/500\n",
    "3/3 [==============================] - 2s 585ms/step - loss: 0.1299 - accuracy: 0.9464 - val_loss: 1.1413 - val_accuracy: 0.8667\n",
    "Epoch 176/500\n",
    "3/3 [==============================] - 2s 470ms/step - loss: 0.1671 - accuracy: 0.9405 - val_loss: 1.1010 - val_accuracy: 0.8667\n",
    "Epoch 177/500\n",
    "3/3 [==============================] - 2s 452ms/step - loss: 0.0787 - accuracy: 0.9643 - val_loss: 1.0957 - val_accuracy: 0.8667\n",
    "Epoch 178/500\n",
    "3/3 [==============================] - 2s 449ms/step - loss: 0.1974 - accuracy: 0.9583 - val_loss: 1.0280 - val_accuracy: 0.8667\n",
    "Epoch 179/500\n",
    "3/3 [==============================] - 2s 596ms/step - loss: 0.1471 - accuracy: 0.9464 - val_loss: 0.9828 - val_accuracy: 0.8667\n",
    "Epoch 180/500\n",
    "3/3 [==============================] - 2s 590ms/step - loss: 0.1179 - accuracy: 0.9524 - val_loss: 0.9316 - val_accuracy: 0.8889\n",
    "Epoch 181/500\n",
    "3/3 [==============================] - 2s 593ms/step - loss: 0.2033 - accuracy: 0.9286 - val_loss: 0.8583 - val_accuracy: 0.8889\n",
    "Epoch 182/500\n",
    "3/3 [==============================] - 2s 597ms/step - loss: 0.1723 - accuracy: 0.9464 - val_loss: 0.9655 - val_accuracy: 0.8444\n",
    "Epoch 183/500\n",
    "3/3 [==============================] - 2s 464ms/step - loss: 0.1395 - accuracy: 0.9702 - val_loss: 1.1741 - val_accuracy: 0.8444\n",
    "Epoch 184/500\n",
    "3/3 [==============================] - 2s 601ms/step - loss: 0.1501 - accuracy: 0.9464 - val_loss: 1.4828 - val_accuracy: 0.8222\n",
    "Epoch 185/500\n",
    "3/3 [==============================] - 2s 599ms/step - loss: 0.1102 - accuracy: 0.9583 - val_loss: 1.7192 - val_accuracy: 0.8000\n",
    "Epoch 186/500\n",
    "3/3 [==============================] - 2s 588ms/step - loss: 0.1189 - accuracy: 0.9583 - val_loss: 1.8592 - val_accuracy: 0.8000\n",
    "Epoch 187/500\n",
    "3/3 [==============================] - 2s 594ms/step - loss: 0.1920 - accuracy: 0.9167 - val_loss: 1.8827 - val_accuracy: 0.8000\n",
    "Epoch 188/500\n",
    "3/3 [==============================] - 2s 451ms/step - loss: 0.0876 - accuracy: 0.9821 - val_loss: 1.8168 - val_accuracy: 0.8000\n",
    "Epoch 189/500\n",
    "3/3 [==============================] - 2s 452ms/step - loss: 0.3118 - accuracy: 0.8929 - val_loss: 1.7082 - val_accuracy: 0.8000\n",
    "Epoch 190/500\n",
    "3/3 [==============================] - 2s 453ms/step - loss: 0.1184 - accuracy: 0.9702 - val_loss: 1.6029 - val_accuracy: 0.8222\n",
    "Epoch 191/500\n",
    "3/3 [==============================] - 2s 475ms/step - loss: 0.1461 - accuracy: 0.9405 - val_loss: 1.5248 - val_accuracy: 0.8444\n",
    "Epoch 192/500\n",
    "3/3 [==============================] - 2s 452ms/step - loss: 0.1538 - accuracy: 0.9345 - val_loss: 1.4676 - val_accuracy: 0.8444\n",
    "Epoch 193/500\n",
    "3/3 [==============================] - 2s 448ms/step - loss: 0.1256 - accuracy: 0.9524 - val_loss: 1.3020 - val_accuracy: 0.8444\n",
    "Epoch 194/500\n",
    "3/3 [==============================] - 2s 451ms/step - loss: 0.1527 - accuracy: 0.9524 - val_loss: 1.1820 - val_accuracy: 0.8667\n",
    "Epoch 195/500\n",
    "3/3 [==============================] - 2s 597ms/step - loss: 0.1467 - accuracy: 0.9464 - val_loss: 1.1021 - val_accuracy: 0.8667\n",
    "Epoch 196/500\n",
    "3/3 [==============================] - 2s 448ms/step - loss: 0.2208 - accuracy: 0.9405 - val_loss: 1.1124 - val_accuracy: 0.8667\n",
    "Epoch 197/500\n",
    "3/3 [==============================] - 2s 457ms/step - loss: 0.1074 - accuracy: 0.9702 - val_loss: 1.1534 - val_accuracy: 0.8667\n",
    "Epoch 198/500\n",
    "3/3 [==============================] - 2s 588ms/step - loss: 0.1661 - accuracy: 0.9524 - val_loss: 1.2019 - val_accuracy: 0.8667\n",
    "Epoch 199/500\n",
    "3/3 [==============================] - 2s 598ms/step - loss: 0.1813 - accuracy: 0.9405 - val_loss: 1.1930 - val_accuracy: 0.8667\n",
    "Epoch 200/500\n",
    "3/3 [==============================] - 2s 464ms/step - loss: 0.1689 - accuracy: 0.9286 - val_loss: 1.1532 - val_accuracy: 0.8667\n",
    "Epoch 201/500\n",
    "3/3 [==============================] - 2s 461ms/step - loss: 0.0950 - accuracy: 0.9583 - val_loss: 1.1187 - val_accuracy: 0.8667\n",
    "Epoch 202/500\n",
    "3/3 [==============================] - 2s 445ms/step - loss: 0.2096 - accuracy: 0.9345 - val_loss: 1.1090 - val_accuracy: 0.8444\n",
    "Epoch 203/500\n",
    "3/3 [==============================] - 2s 589ms/step - loss: 0.1210 - accuracy: 0.9702 - val_loss: 1.1682 - val_accuracy: 0.8222\n",
    "Epoch 204/500\n",
    "3/3 [==============================] - 2s 466ms/step - loss: 0.1742 - accuracy: 0.9345 - val_loss: 1.2362 - val_accuracy: 0.8444\n",
    "Epoch 205/500\n",
    "\n",
    "3/3 [==============================] - 2s 473ms/step - loss: 0.1519 - accuracy: 0.9464 - val_loss: 1.3328 - val_accuracy: 0.8222\n",
    "Epoch 206/500\n",
    "3/3 [==============================] - 2s 447ms/step - loss: 0.1873 - accuracy: 0.9286 - val_loss: 1.3993 - val_accuracy: 0.8444\n",
    "Epoch 207/500\n",
    "3/3 [==============================] - 2s 451ms/step - loss: 0.2013 - accuracy: 0.9107 - val_loss: 1.4986 - val_accuracy: 0.8222\n",
    "Epoch 208/500\n",
    "3/3 [==============================] - 2s 591ms/step - loss: 0.0542 - accuracy: 0.9881 - val_loss: 1.6404 - val_accuracy: 0.8000\n",
    "Epoch 209/500\n",
    "3/3 [==============================] - 2s 453ms/step - loss: 0.1293 - accuracy: 0.9524 - val_loss: 1.5641 - val_accuracy: 0.8000\n",
    "Epoch 210/500\n",
    "3/3 [==============================] - 2s 593ms/step - loss: 0.1098 - accuracy: 0.9524 - val_loss: 1.4120 - val_accuracy: 0.8000\n",
    "Epoch 211/500\n",
    "3/3 [==============================] - 2s 601ms/step - loss: 0.1466 - accuracy: 0.9345 - val_loss: 1.3137 - val_accuracy: 0.8222\n",
    "Epoch 212/500\n",
    "3/3 [==============================] - 2s 472ms/step - loss: 0.1313 - accuracy: 0.9464 - val_loss: 1.2535 - val_accuracy: 0.8444\n",
    "Epoch 213/500\n",
    "3/3 [==============================] - 2s 473ms/step - loss: 0.1535 - accuracy: 0.9702 - val_loss: 1.1967 - val_accuracy: 0.8667\n",
    "Epoch 214/500\n",
    "3/3 [==============================] - 2s 592ms/step - loss: 0.0894 - accuracy: 0.9702 - val_loss: 1.1351 - val_accuracy: 0.8667\n",
    "Epoch 215/500\n",
    "3/3 [==============================] - 2s 594ms/step - loss: 0.0866 - accuracy: 0.9524 - val_loss: 1.0954 - val_accuracy: 0.8667\n",
    "Epoch 216/500\n",
    "3/3 [==============================] - 2s 447ms/step - loss: 0.1205 - accuracy: 0.9702 - val_loss: 1.0748 - val_accuracy: 0.8444\n",
    "Epoch 217/500\n",
    "3/3 [==============================] - 2s 471ms/step - loss: 0.0814 - accuracy: 0.9762 - val_loss: 1.1006 - val_accuracy: 0.8667\n",
    "Epoch 218/500\n",
    "3/3 [==============================] - 2s 589ms/step - loss: 0.0622 - accuracy: 0.9762 - val_loss: 1.1069 - val_accuracy: 0.8667\n",
    "Epoch 219/500\n",
    "3/3 [==============================] - 2s 473ms/step - loss: 0.0716 - accuracy: 0.9643 - val_loss: 1.1067 - val_accuracy: 0.8889\n",
    "Epoch 220/500\n",
    "3/3 [==============================] - 2s 462ms/step - loss: 0.0902 - accuracy: 0.9583 - val_loss: 1.0631 - val_accuracy: 0.8889\n",
    "Epoch 221/500\n",
    "3/3 [==============================] - 2s 445ms/step - loss: 0.1073 - accuracy: 0.9583 - val_loss: 1.0057 - val_accuracy: 0.8889\n",
    "Epoch 222/500\n",
    "3/3 [==============================] - 2s 592ms/step - loss: 0.0655 - accuracy: 0.9762 - val_loss: 0.9587 - val_accuracy: 0.9111\n",
    "Epoch 223/500\n",
    "3/3 [==============================] - 2s 471ms/step - loss: 0.0969 - accuracy: 0.9643 - val_loss: 0.9134 - val_accuracy: 0.9111\n",
    "Epoch 224/500\n",
    "3/3 [==============================] - 2s 593ms/step - loss: 0.0772 - accuracy: 0.9702 - val_loss: 0.9028 - val_accuracy: 0.9111\n",
    "Epoch 225/500\n",
    "3/3 [==============================] - 2s 597ms/step - loss: 0.0763 - accuracy: 0.9762 - val_loss: 0.9250 - val_accuracy: 0.8889\n",
    "Epoch 226/500\n",
    "3/3 [==============================] - 2s 465ms/step - loss: 0.1732 - accuracy: 0.9405 - val_loss: 0.9469 - val_accuracy: 0.8889\n",
    "Epoch 227/500\n",
    "3/3 [==============================] - 2s 444ms/step - loss: 0.0873 - accuracy: 0.9702 - val_loss: 0.9181 - val_accuracy: 0.8889\n",
    "Epoch 228/500\n",
    "3/3 [==============================] - 2s 448ms/step - loss: 0.0833 - accuracy: 0.9702 - val_loss: 0.9445 - val_accuracy: 0.8889\n",
    "Epoch 229/500\n",
    "3/3 [==============================] - 2s 595ms/step - loss: 0.1440 - accuracy: 0.9464 - val_loss: 0.9800 - val_accuracy: 0.8889\n",
    "Epoch 230/500\n",
    "3/3 [==============================] - 2s 451ms/step - loss: 0.1701 - accuracy: 0.9464 - val_loss: 1.0488 - val_accuracy: 0.8444\n",
    "Epoch 231/500\n",
    "3/3 [==============================] - 2s 450ms/step - loss: 0.1561 - accuracy: 0.9524 - val_loss: 1.0804 - val_accuracy: 0.8667\n",
    "Epoch 232/500\n",
    "3/3 [==============================] - 2s 469ms/step - loss: 0.1168 - accuracy: 0.9583 - val_loss: 1.0378 - val_accuracy: 0.8667\n",
    "Epoch 233/500\n",
    "3/3 [==============================] - 2s 594ms/step - loss: 0.0955 - accuracy: 0.9762 - val_loss: 1.0250 - val_accuracy: 0.8444\n",
    "Epoch 234/500\n",
    "3/3 [==============================] - 2s 461ms/step - loss: 0.0942 - accuracy: 0.9702 - val_loss: 0.9902 - val_accuracy: 0.8444\n",
    "Epoch 235/500\n",
    "3/3 [==============================] - 2s 472ms/step - loss: 0.0802 - accuracy: 0.9762 - val_loss: 0.9575 - val_accuracy: 0.8444\n",
    "Epoch 236/500\n",
    "3/3 [==============================] - 2s 471ms/step - loss: 0.1069 - accuracy: 0.9524 - val_loss: 0.9055 - val_accuracy: 0.8444\n",
    "Epoch 237/500\n",
    "3/3 [==============================] - 2s 447ms/step - loss: 0.1028 - accuracy: 0.9762 - val_loss: 0.8771 - val_accuracy: 0.8444\n",
    "Epoch 238/500\n",
    "3/3 [==============================] - 2s 450ms/step - loss: 0.0757 - accuracy: 0.9762 - val_loss: 0.8655 - val_accuracy: 0.8667\n",
    "Epoch 239/500\n",
    "3/3 [==============================] - 2s 599ms/step - loss: 0.0937 - accuracy: 0.9762 - val_loss: 0.8479 - val_accuracy: 0.8667\n",
    "Epoch 240/500\n",
    "3/3 [==============================] - 2s 446ms/step - loss: 0.0998 - accuracy: 0.9702 - val_loss: 0.8320 - val_accuracy: 0.8667\n",
    "Epoch 241/500\n",
    "3/3 [==============================] - 2s 475ms/step - loss: 0.1373 - accuracy: 0.9702 - val_loss: 0.8938 - val_accuracy: 0.8444\n",
    "Epoch 242/500\n",
    "3/3 [==============================] - 2s 449ms/step - loss: 0.1040 - accuracy: 0.9643 - val_loss: 0.9962 - val_accuracy: 0.8222\n",
    "Epoch 243/500\n",
    "3/3 [==============================] - 2s 599ms/step - loss: 0.1165 - accuracy: 0.9762 - val_loss: 0.9512 - val_accuracy: 0.8222\n",
    "Epoch 244/500\n",
    "3/3 [==============================] - 2s 448ms/step - loss: 0.0500 - accuracy: 0.9821 - val_loss: 0.9035 - val_accuracy: 0.8444\n",
    "Epoch 245/500\n",
    "3/3 [==============================] - 2s 593ms/step - loss: 0.1562 - accuracy: 0.9405 - val_loss: 0.9149 - val_accuracy: 0.8444\n",
    "Epoch 246/500\n",
    "3/3 [==============================] - 2s 456ms/step - loss: 0.1650 - accuracy: 0.9345 - val_loss: 0.9775 - val_accuracy: 0.8667\n",
    "Epoch 247/500\n",
    "3/3 [==============================] - 2s 599ms/step - loss: 0.0885 - accuracy: 0.9643 - val_loss: 0.9852 - val_accuracy: 0.8667\n",
    "Epoch 248/500\n",
    "3/3 [==============================] - 2s 454ms/step - loss: 0.0607 - accuracy: 0.9702 - val_loss: 0.9907 - val_accuracy: 0.8667\n",
    "Epoch 249/500\n",
    "3/3 [==============================] - 2s 457ms/step - loss: 0.0788 - accuracy: 0.9702 - val_loss: 0.9920 - val_accuracy: 0.8667\n",
    "Epoch 250/500\n",
    "3/3 [==============================] - 2s 447ms/step - loss: 0.1296 - accuracy: 0.9405 - val_loss: 0.9708 - val_accuracy: 0.8667\n",
    "Epoch 251/500\n",
    "3/3 [==============================] - 2s 453ms/step - loss: 0.0991 - accuracy: 0.9702 - val_loss: 0.9515 - val_accuracy: 0.8667\n",
    "Epoch 252/500\n",
    "3/3 [==============================] - 2s 468ms/step - loss: 0.1055 - accuracy: 0.9583 - val_loss: 0.9569 - val_accuracy: 0.8667\n",
    "Epoch 253/500\n",
    "3/3 [==============================] - 2s 476ms/step - loss: 0.0670 - accuracy: 0.9702 - val_loss: 0.9900 - val_accuracy: 0.8667\n",
    "Epoch 254/500\n",
    "3/3 [==============================] - 2s 455ms/step - loss: 0.1212 - accuracy: 0.9524 - val_loss: 1.0451 - val_accuracy: 0.8889\n",
    "Epoch 255/500\n",
    "3/3 [==============================] - 2s 469ms/step - loss: 0.1294 - accuracy: 0.9524 - val_loss: 1.0927 - val_accuracy: 0.8889\n",
    "Epoch 256/500\n",
    "3/3 [==============================] - 2s 465ms/step - loss: 0.0850 - accuracy: 0.9643 - val_loss: 1.1086 - val_accuracy: 0.8889\n",
    "Epoch 257/500\n",
    "3/3 [==============================] - 2s 451ms/step - loss: 0.0888 - accuracy: 0.9583 - val_loss: 1.0824 - val_accuracy: 0.8889\n",
    "Epoch 258/500\n",
    "3/3 [==============================] - 2s 600ms/step - loss: 0.0573 - accuracy: 0.9881 - val_loss: 1.1042 - val_accuracy: 0.8667\n",
    "Epoch 259/500\n",
    "3/3 [==============================] - 2s 466ms/step - loss: 0.1769 - accuracy: 0.9583 - val_loss: 1.1024 - val_accuracy: 0.8667\n",
    "Epoch 260/500\n",
    "3/3 [==============================] - 2s 591ms/step - loss: 0.1345 - accuracy: 0.9583 - val_loss: 1.0389 - val_accuracy: 0.8667\n",
    "Epoch 261/500\n",
    "3/3 [==============================] - 2s 595ms/step - loss: 0.0770 - accuracy: 0.9762 - val_loss: 1.0264 - val_accuracy: 0.8667\n",
    "Epoch 262/500\n",
    "\n",
    "3/3 [==============================] - 2s 448ms/step - loss: 0.0991 - accuracy: 0.9643 - val_loss: 1.0837 - val_accuracy: 0.8889\n",
    "Epoch 263/500\n",
    "3/3 [==============================] - 2s 451ms/step - loss: 0.1301 - accuracy: 0.9583 - val_loss: 1.2081 - val_accuracy: 0.8667\n",
    "Epoch 264/500\n",
    "3/3 [==============================] - 2s 447ms/step - loss: 0.0797 - accuracy: 0.9702 - val_loss: 1.3122 - val_accuracy: 0.8222\n",
    "Epoch 265/500\n",
    "3/3 [==============================] - 2s 594ms/step - loss: 0.0670 - accuracy: 0.9821 - val_loss: 1.3918 - val_accuracy: 0.8222\n",
    "Epoch 266/500\n",
    "3/3 [==============================] - 2s 604ms/step - loss: 0.1450 - accuracy: 0.9643 - val_loss: 1.4346 - val_accuracy: 0.8222\n",
    "Epoch 267/500\n",
    "3/3 [==============================] - 2s 469ms/step - loss: 0.0771 - accuracy: 0.9762 - val_loss: 1.4203 - val_accuracy: 0.8444\n",
    "Epoch 268/500\n",
    "3/3 [==============================] - 2s 584ms/step - loss: 0.0635 - accuracy: 0.9762 - val_loss: 1.3463 - val_accuracy: 0.8444\n",
    "Epoch 269/500\n",
    "3/3 [==============================] - 2s 444ms/step - loss: 0.0379 - accuracy: 0.9940 - val_loss: 1.3079 - val_accuracy: 0.8667\n",
    "Epoch 270/500\n",
    "3/3 [==============================] - 2s 589ms/step - loss: 0.0967 - accuracy: 0.9702 - val_loss: 1.2917 - val_accuracy: 0.8444\n",
    "Epoch 271/500\n",
    "3/3 [==============================] - 2s 590ms/step - loss: 0.1114 - accuracy: 0.9524 - val_loss: 1.2644 - val_accuracy: 0.8667\n",
    "Epoch 272/500\n",
    "3/3 [==============================] - 2s 451ms/step - loss: 0.1000 - accuracy: 0.9643 - val_loss: 1.2013 - val_accuracy: 0.8667\n",
    "Epoch 273/500\n",
    "3/3 [==============================] - 2s 466ms/step - loss: 0.1116 - accuracy: 0.9464 - val_loss: 1.1357 - val_accuracy: 0.8667\n",
    "Epoch 274/500\n",
    "3/3 [==============================] - 2s 595ms/step - loss: 0.1571 - accuracy: 0.9345 - val_loss: 1.0922 - val_accuracy: 0.8444\n",
    "Epoch 275/500\n",
    "3/3 [==============================] - 2s 598ms/step - loss: 0.0904 - accuracy: 0.9702 - val_loss: 1.0875 - val_accuracy: 0.8222\n",
    "Epoch 276/500\n",
    "3/3 [==============================] - 2s 473ms/step - loss: 0.1000 - accuracy: 0.9762 - val_loss: 1.1694 - val_accuracy: 0.8222\n",
    "Epoch 277/500\n",
    "3/3 [==============================] - 2s 466ms/step - loss: 0.0816 - accuracy: 0.9643 - val_loss: 1.2417 - val_accuracy: 0.8222\n",
    "Epoch 278/500\n",
    "3/3 [==============================] - 2s 470ms/step - loss: 0.0860 - accuracy: 0.9643 - val_loss: 1.2863 - val_accuracy: 0.8000\n",
    "Epoch 279/500\n",
    "3/3 [==============================] - 2s 455ms/step - loss: 0.0669 - accuracy: 0.9702 - val_loss: 1.2882 - val_accuracy: 0.8000\n",
    "Epoch 280/500\n",
    "3/3 [==============================] - 2s 594ms/step - loss: 0.0606 - accuracy: 0.9821 - val_loss: 1.2175 - val_accuracy: 0.8222\n",
    "Epoch 281/500\n",
    "3/3 [==============================] - 2s 453ms/step - loss: 0.1346 - accuracy: 0.9643 - val_loss: 1.1606 - val_accuracy: 0.8444\n",
    "Epoch 282/500\n",
    "3/3 [==============================] - 2s 589ms/step - loss: 0.1330 - accuracy: 0.9524 - val_loss: 1.0774 - val_accuracy: 0.8667\n",
    "Epoch 283/500\n",
    "3/3 [==============================] - 2s 589ms/step - loss: 0.1296 - accuracy: 0.9524 - val_loss: 0.9864 - val_accuracy: 0.8667\n",
    "Epoch 284/500\n",
    "3/3 [==============================] - 2s 447ms/step - loss: 0.0834 - accuracy: 0.9762 - val_loss: 0.8805 - val_accuracy: 0.8667\n",
    "Epoch 285/500\n",
    "3/3 [==============================] - 2s 450ms/step - loss: 0.0848 - accuracy: 0.9702 - val_loss: 0.8257 - val_accuracy: 0.8667\n",
    "Epoch 286/500\n",
    "3/3 [==============================] - 2s 586ms/step - loss: 0.0891 - accuracy: 0.9702 - val_loss: 0.8328 - val_accuracy: 0.8667\n",
    "Epoch 287/500\n",
    "3/3 [==============================] - 2s 593ms/step - loss: 0.0306 - accuracy: 0.9940 - val_loss: 0.8907 - val_accuracy: 0.8667\n",
    "Epoch 288/500\n",
    "3/3 [==============================] - 2s 592ms/step - loss: 0.0506 - accuracy: 0.9762 - val_loss: 0.9458 - val_accuracy: 0.8667\n",
    "Epoch 289/500\n",
    "3/3 [==============================] - 2s 462ms/step - loss: 0.1118 - accuracy: 0.9583 - val_loss: 0.9509 - val_accuracy: 0.8667\n",
    "Epoch 290/500\n",
    "3/3 [==============================] - 2s 465ms/step - loss: 0.0971 - accuracy: 0.9762 - val_loss: 0.9449 - val_accuracy: 0.8667\n",
    "Epoch 291/500\n",
    "3/3 [==============================] - 2s 469ms/step - loss: 0.0498 - accuracy: 0.9702 - val_loss: 0.9579 - val_accuracy: 0.8889\n",
    "Epoch 292/500\n",
    "3/3 [==============================] - 2s 469ms/step - loss: 0.0610 - accuracy: 0.9821 - val_loss: 0.9862 - val_accuracy: 0.8889\n",
    "Epoch 293/500\n",
    "3/3 [==============================] - 2s 465ms/step - loss: 0.0631 - accuracy: 0.9762 - val_loss: 1.0049 - val_accuracy: 0.8889\n",
    "Epoch 294/500\n",
    "3/3 [==============================] - 2s 589ms/step - loss: 0.0779 - accuracy: 0.9821 - val_loss: 1.0101 - val_accuracy: 0.8889\n",
    "Epoch 295/500\n",
    "3/3 [==============================] - 2s 446ms/step - loss: 0.1308 - accuracy: 0.9643 - val_loss: 0.9920 - val_accuracy: 0.8889\n",
    "Epoch 296/500\n",
    "3/3 [==============================] - 2s 593ms/step - loss: 0.0649 - accuracy: 0.9762 - val_loss: 0.9440 - val_accuracy: 0.8889\n",
    "Epoch 297/500\n",
    "3/3 [==============================] - 2s 468ms/step - loss: 0.1069 - accuracy: 0.9583 - val_loss: 0.9664 - val_accuracy: 0.8889\n",
    "Epoch 298/500\n",
    "3/3 [==============================] - 2s 592ms/step - loss: 0.0599 - accuracy: 0.9881 - val_loss: 1.0366 - val_accuracy: 0.8889\n",
    "Epoch 299/500\n",
    "3/3 [==============================] - 2s 451ms/step - loss: 0.0792 - accuracy: 0.9762 - val_loss: 1.0685 - val_accuracy: 0.8889\n",
    "Epoch 300/500\n",
    "3/3 [==============================] - 2s 452ms/step - loss: 0.1189 - accuracy: 0.9583 - val_loss: 1.0009 - val_accuracy: 0.8889\n",
    "Epoch 301/500\n",
    "3/3 [==============================] - 2s 466ms/step - loss: 0.1630 - accuracy: 0.9524 - val_loss: 0.9883 - val_accuracy: 0.8889\n",
    "Epoch 302/500\n",
    "3/3 [==============================] - 2s 474ms/step - loss: 0.1731 - accuracy: 0.9583 - val_loss: 1.0408 - val_accuracy: 0.8889\n",
    "Epoch 303/500\n",
    "3/3 [==============================] - 2s 588ms/step - loss: 0.1022 - accuracy: 0.9702 - val_loss: 1.0657 - val_accuracy: 0.8889"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
